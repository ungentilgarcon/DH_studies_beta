{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some imports\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser\n",
    "from difflib import SequenceMatcher\n",
    "import re,os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FOR TOPIC MODELLING\n",
    "#https://de.dariah.eu/tatom/topic_model_python.html\n",
    "import numpy as np  # a conventional alias\n",
    "\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "  cleanr = re.compile('<.*?>')\n",
    "  cleantext = re.sub(cleanr, '', raw_html)\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def language_detection_with_pyenchant(string_to_read):\n",
    "    #https://stackoverflow.com/questions/3788870/how-to-check-if-a-word-is-an-english-word-with-python\n",
    "    import enchant\n",
    "    lg_ang=0\n",
    "    us = enchant.Dict(\"en_US\")\n",
    "    #print \"US LOADED\"\n",
    "    fr = enchant.Dict(\"fr_FR\")\n",
    "    #print \"FR LOADED\"\n",
    "    lg_fr=0\n",
    "    lg_ang=0\n",
    "    #print \"string_to_read\",string_to_read\n",
    "    for word in string_to_read.split():\n",
    "        #print fr.check(word)\n",
    "        #print word\n",
    "        if fr.check(word) == True:\n",
    "            lg_fr+=1\n",
    "        if us.check(word) == True:\n",
    "            lg_ang+=1\n",
    "    #print \"THERE I AM\"\n",
    "    if lg_fr >= lg_ang :\n",
    "        return \"french\"\n",
    "    else:\n",
    "        if lg_ang > lg_fr :\n",
    "        \n",
    "            return \"english\"\n",
    "        else: \n",
    "            return \"NEITHER ENGLISH NOR FRENCH\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_analysis_with_mallet(string_to_read):\n",
    "    lng = language_detection_with_pyenchant(string_to_read)\n",
    "    if lng== \"french\" :\n",
    "        lng= [\"a\",\"à\",\"â\",\"abord\",\"afin\",\"ah\",\"ai\",\"aie\",\"ainsi\",\"allaient\",\"allo\",\"allô\",\"allons\",\"après\",\"assez\",\"attendu\",\"au\",\"aucun\",\"aucune\",\"aujourd\",\"aujourd'hui\",\"auquel\",\"aura\",\"auront\",\"aussi\",\"autre\",\"autres\",\"aux\",\"auxquelles\",\"auxquels\",\"avaient\",\"avais\",\"avait\",\"avant\",\"avec\",\"avoir\",\"ayant\",\"b\",\"bah\",\"beaucoup\",\"bien\",\"bigre\",\"boum\",\"bravo\",\"brrr\",\"c\",\"ça\",\"car\",\"ce\",\"ceci\",\"cela\",\"celle\",\"celle-ci\",\"celle-là\",\"celles\",\"celles-ci\",\"celles-là\",\"celui\",\"celui-ci\",\"celui-là\",\"cent\",\"cependant\",\"certain\",\"certaine\",\"certaines\",\"certains\",\"certes\",\"ces\",\"cet\",\"cette\",\"ceux\",\"ceux-ci\",\"ceuxlà\",\"chacun\",\"chaque\",\"cher\",\"chère\",\"chères\",\"chers\",\"chez\",\"chiche\",\"chut\",\"ci\",\"cinq\",\"cinquantaine\",\"cinquante\",\"cinquantième\",\"cinquième\",\"clac\",\"clic\",\"combien\",\"comme\",\"comment\",\"compris\",\"concernant\",\"contre\",\"couic\",\"crac\",\"d\",\"da\",\"dans\",\"de\",\"debout\",\"dedans\",\"dehors\",\"delà\",\"depuis\",\"derrière\",\"des\",\"dès\",\"désormais\",\"desquelles\",\"desquels\",\"dessous\",\"dessus\",\"deux\",\"deuxième\",\"deuxièmement\",\"devant\",\"devers\",\"devra\",\"différent\",\"différente\",\"différentes\",\"différents\",\"dire\",\"divers\",\"diverse\",\"diverses\",\"dix\",\"dix-huit\",\"dixième\",\"dix-neuf\",\"dixsept\",\"doit\",\"doivent\",\"donc\",\"dont\",\"douze\",\"douzième\",\"dring\",\"du\",\"duquel\",\"durant\",\"e\",\"effet\",\"eh\",\"elle\",\"elle-même\",\"elles\",\"ellesmêmes\",\"en\",\"encore\",\"entre\",\"envers\",\"environ\",\"es\",\"ès\",\"est\",\"et\",\"etant\",\"étaient\",\"étais\",\"était\",\"étant\",\"etc\",\"été\",\"etre\",\"être\",\"eu\",\"euh\",\"eux\",\"eux-mêmes\",\"excepté\",\"f\",\"façon\",\"fais\",\"faisaient\",\"faisant\",\"fait\",\"feront\",\"fi\",\"flac\",\"floc\",\"font\",\"g\",\"gens\",\"h\",\"ha\",\"hé\",\"hein\",\"hélas\",\"hem\",\"hep\",\"hi\",\"ho\",\"holà\",\"hop\",\"hormis\",\"hors\",\"hou\",\"houp\",\"hue\",\"hui\",\"huit\",\"huitième\",\"hum\",\"hurrah\",\"i\",\"il\",\"ils\",\"importe\",\"j\",\"je\",\"jusqu\",\"jusque\",\"k\",\"l\",\"la\",\"là\",\"laquelle\",\"las\",\"le\",\"lequel\",\"les\",\"lès\",\"lesquelles\",\"lesquels\",\"leur\",\"leurs\",\"longtemps\",\"lorsque\",\"lui\",\"lui-même\",\"m\",\"ma\",\"maint\",\"mais\",\"malgré\",\"me\",\"même\",\"mêmes\",\"merci\",\"mes\",\"mien\",\"mienne\",\"miennes\",\"miens\",\"mille\",\"mince\",\"moi\",\"moi-même\",\"moins\",\"mon\",\"moyennant\",\"n\",\"na\",\"ne\",\"néanmoins\",\"neuf\",\"neuvième\",\"ni\",\"nombreuses\",\"nombreux\",\"non\",\"nos\",\"notre\",\"nôtre\",\"nôtres\",\"nous\",\"nous-mêmes\",\"nul\",\"o\",\"o|\",\"ô\",\"oh\",\"ohé\",\"olé\",\"ollé\",\"on\",\"ont\",\"onze\",\"onzième\",\"ore\",\"ou\",\"où\",\"ouf\",\"ouias\",\"oust\",\"ouste\",\"outre\",\"p\",\"paf\",\"pan\",\"par\",\"parmi\",\"partant\",\"particulier\",\"particulière\",\"particulièrement\",\"pas\",\"passé\",\"pendant\",\"personne\",\"peu\",\"peut\",\"peuvent\",\"peux\",\"pff\",\"pfft\",\"pfut\",\"pif\",\"plein\",\"plouf\",\"plus\",\"plusieurs\",\"plutôt\",\"pouah\",\"pour\",\"pourquoi\",\"premier\",\"première\",\"premièrement\",\"près\",\"proche\",\"psitt\",\"puisque\",\"q\",\"qu\",\"quand\",\"quant\",\"quanta\",\"quant-à-soi\",\"quarante\",\"quatorze\",\"quatre\",\"quatre- vingt\",\"quatrième\",\"quatrièmement\",\"que\",\"quel\",\"quelconque\",\"quelle\",\"quelles\",\"quelque\",\"quelques\",\"quelqu'un\",\"quels\",\"qui\",\"quiconque\",\"quinze\",\"quoi\",\"quoique\",\"r\",\"revoici\",\"revoilà\",\"rien\",\"s\",\"sa\",\"sacrebleu\",\"sans\",\"sapristi\",\"sauf\",\"se\",\"seize\",\"selon\",\"sept\",\"septième\",\"sera\",\"seront\",\"ses\",\"si\",\"sien\",\"sienne\",\"siennes\",\"siens\",\"sinon\",\"six\",\"sixième\",\"soi\",\"soi-même\",\"soit\",\"soixante\",\"son\",\"sont\",\"sous\",\"stop\",\"suis\",\"suivant\",\"sur\",\"surtout\",\"t\",\"ta\",\"tac\",\"tant\",\"te\",\"té\",\"tel\",\"telle\",\"tellement\",\"telles\",\"tels\",\"tenant\",\"tes\",\"tic\",\"tien\",\"tienne\",\"tiennes\",\"tiens\",\"toc\",\"toi\",\"toi-même\",\"ton\",\"touchant\",\"toujours\",\"tous\",\"tout\",\"toute\",\"toutes\",\"treize\",\"trente\",\"très\",\"trois\",\"troisième\",\"troisièmement\",\"trop\",\"tsoin\",\"tsouin\",\"tu\",\"u\",\"un\",\"une\",\"unes\",\"uns\",\"v\",\"va\",\"vais\",\"vas\",\"vé\",\"vers\",\"via\",\"vif\",\"vifs\",\"vingt\",\"vivat\",\"vive\",\"vives\",\"vlan\",\"voici\",\"voilà\",\"vont\",\"vos\",\"votre\",\"vôtre\",\"vôtres\",\"vous\",\"vous-mêmes\",\"vu\",\"w\",\"x\",\"y\",\"z\",\"zut\"]\n",
    "    \n",
    "    vectorizer = text.CountVectorizer(input=string_to_read, stop_words=lng, min_df=3)\n",
    "    dtm = vectorizer.fit_transform(string_to_read.split()).toarray()\n",
    "    vocab = np.array(vectorizer.get_feature_names())\n",
    "    dtm.shape\n",
    "    len(vocab)\n",
    "    num_topics = 20\n",
    "    num_top_words = 20\n",
    "    clf = decomposition.NMF(n_components=num_topics, random_state=1)\n",
    "    doctopic = clf.fit_transform(dtm)\n",
    "    topic_words = []\n",
    "    for topic in clf.components_:\n",
    "        word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "        topic_words.append([vocab[i] for i in word_idx])\n",
    "    doctopic = doctopic / np.sum(doctopic, axis=1, keepdims=True)\n",
    "    return [topic_words,doctopic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#so far french support\n",
    "def find_geo_topics(string_to_read, lng):\n",
    "    geo=0\n",
    "    geo_words=[]\n",
    "    #print string_to_read\n",
    "    if lng== \"french\":\n",
    "        for word in (string_to_read.split()):\n",
    "        #print word\n",
    "        #LUSSAULT ;) https://www.espacestemps.net/articles/entrees-par-categories/\n",
    "            # if (word in [\"Théorie de l’espace\",\"Accessibilité\",\"Acteur spatial\",\"Action spatiale\",\"Agencement\",\"Agglomération\",\"Ailleurs\",\"Aire\",\"Aire culturelle\",\"Alignement\",\"Ambiance architecturale et urbaine\",\"Aménagement du territoire\",\"Anamorphose\",\"Anthropisation\",\"Archipel mégalopolitain mondial \",\"Armature urbaine\",\"Attraction\",\"Autocorrélation spatiale\",\"Banlieue\",\"Campagne\",\"Capital spatial\",\"Carte\",\"Carte mentale\",\"Centralité\",\"Centre/Périphérie\",\"Centre urbain\",\"Chorème\",\"Chorotype\",\"Circulation\",\"Citadinité\",\"Communication territoriale\",\"Commutateur\",\"Compromis territorial\",\"Concentration\",\"Configuration spatiale\",\"Confins\",\"Connexité\",\"Contact\",\"Contiguïté\",\"Continent\",\"Continuité\",\"Coprésence\",\"Corps\",\"Cospatialité\",\"Cyberespace\",\"Décentralisation\",\"Découpage\",\"Découverte\",\"Défrichement\",\"Densité\",\"Désert\",\"Déterritorialisation\",\"Développement local\",\"Diaspora\",\"Différenciation spatiale -Diffusion\",\"Discontinuité\",\"Dispositif spatial légitime\",\"Distance\",\"Distribution rang/taille\",\"Distribution spatiale\",\"District industriel\",\"Diversité\",\"Dynamique spatiale\",\"Écart\",\"Échelle\",\"Économie-monde\",\"Écoumène\",\"Edge City\",\"Emblème territorial\",\"Emboîtement\",\"Empire\",\"Enclavement -Ensemble géographique\",\"Espace\",\"Espace public \",\"Espace vécu\",\"État\",\"État local\",\"Étendue\",\"Fédéralisme\",\"Finage\",\"Firme transnationale\",\"Fleuve\",\"Flux\",\"Foncier\",\"Forêt\",\"Fractale\",\"Friche\",\"Front\",\"Front pionnier\",\"Frontière\",\"Générique \",\"Gentrification\",\"Géoéconomie\",\"Géogramme\",\"Géographicité\",\"Géographie\",\"Géon\",\"Géopolitique\",\"Géosystème\",\"Géotype\",\"Ghetto\",\"Glacis\",\"Gouvernement urbain\",\"Gradient\",\"Graphe\",\"Graphique\",\"Gravitaire \",\"Guerre\",\"Habitat\",\"Habitat non-réglementaire\",\"Habiter\",\"Haut lieu\",\"Heimat\",\"Hétérotopie\",\"Hinterland\",\"Horizont\",\"Hors-sol\",\"Hub\",\"Identité spatiale\",\"Île\",\"Image\",\"Imaginaire géographique\",\"Immanence/Transcendance \",\"Infra-urbain\",\"Interaction spatiale\",\"Interface\",\"Interspatialité\",\"Irrigation\",\"Isolat\",\"Isotropie\",\"Jardin\",\"Justice spatiale\",\"Lieu\",\"Lieux centraux \",\"Limite\",\"Littoral\",\"Local\",\"Localisation\",\"Logistique\",\"Maillage\",\"Maison\",\"Marchandise\",\"Médiance\",\"Méditerranée\",\"Mer\",\"Métaphore spatiale\",\"Métrique\",\"Métropole/Mégalopole\",\"Métropolisation\",\"Migration\",\"Milieu\",\"Milieu innovateur\",\"Minorité territoriale\",\"Mobilité\",\"Monde\",\"Mondialisation\",\"Montagne\",\"Nation\",\"Network\",\"Nœud\",\"Norme\",\"Oasis\",\"Objet géographique\",\"Parc à thème\",\"Parc naturel\",\"Parcours\",\"Partie du monde\",\"Pavillonnaire \",\"Pays\",\"Paysage\",\"Périurbain\",\"Peuplement\",\"Polarisation\",\"Population \",\"Position\",\"Pratique spatiale\",\"Projet urbain\",\"Prospective territoriale\",\"Proxémie\",\"Reconstruction\",\"Reconversion\",\"Rénovation/Restauration/Réhabilitation\",\"Représentation de l’espace\",\"Réseau\",\"Réseau technique\",\"Réseau urbain\",\"Rhizome\",\"Rue\",\"Rural\",\"Schéma d’aménagement\",\"Ségrégation\",\"Seuil\",\"Site\",\"Situation géographique\",\"Société-Monde\",\"Sol\",\"Spatialité\",\"Stratégie spatiale\",\"Substance\",\"Système d’Information Géographique \",\"Système productif local \",\"Système spatial\",\"Technopôle/Technopole\",\"Télé-communication\",\"Télétravail\",\"Terre\",\"Territoire\",\"Territorial \",\"Territorialité\",\"Terroir\",\"Topogenèse\",\"Topographie\",\"Topologie\",\"Toponymie\",\"Tourisme\",\"Transition démographique\",\"Transports\",\"Ubiquité\",\"Urbain\",\"Urbain \",\"Urbanisation\",\"Urbanité\",\"Valeur spatiale\",\"Vaterland\",\"Végétation\",\"Village\",\"Ville\",\"Ville mondiale\",\"Ville nouvelle\",\"Violence\",\"Visibilité \",\"Voisinage\",\"Zonage\",\"Zone climatique\"]) or  \"géogr\" in word or \"géomat\" in word :\n",
    "            if  \"géogr\" in word or \"géomat\" in word or \"spatial\" in word or \"urbanis\" in word:  #or \"carto\" in word:  \n",
    "                geo+=1\n",
    "                geo_words.append(word)\n",
    "    if lng== \"english\":\n",
    "        for word in (string_to_read.split()):\n",
    "        #print word\n",
    "        #LUSSAULT ;) https://www.espacestemps.net/articles/entrees-par-categories/\n",
    "             if  \"geogr\" in word or \"geomat\" in word or \"spatial\" in word or \"urbanis\" in word: # or \"carto\" in word:\n",
    "                geo+=1\n",
    "                geo_words.append(word)\n",
    "    print \"SUJETS\\n\",geo_words,\"\\n points : \",geo\n",
    "    return [geo, geo_words]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_analysis_with_manual_detection(string_to_read):\n",
    "    lng = language_detection_with_pyenchant(string_to_read)\n",
    "    if lng== \"french\" :\n",
    "        geopoints= find_geo_topics(string_to_read, \"french\")\n",
    "        #print string_to_read\n",
    "    else:\n",
    "        if lng== \"english\" :\n",
    "            geopoints= find_geo_topics(string_to_read, \"english\")\n",
    "        \n",
    "        else:\n",
    "            print \"LANGUAGE NOT YET SUPPORTED\"\n",
    "            geopoints=['',''] \n",
    "    #    lng= [\"a\",\"à\",\"â\",\"abord\",\"afin\",\"ah\",\"ai\",\"aie\",\"ainsi\",\"allaient\",\"allo\",\"allô\",\"allons\",\"après\",\"assez\",\"attendu\",\"au\",\"aucun\",\"aucune\",\"aujourd\",\"aujourd'hui\",\"auquel\",\"aura\",\"auront\",\"aussi\",\"autre\",\"autres\",\"aux\",\"auxquelles\",\"auxquels\",\"avaient\",\"avais\",\"avait\",\"avant\",\"avec\",\"avoir\",\"ayant\",\"b\",\"bah\",\"beaucoup\",\"bien\",\"bigre\",\"boum\",\"bravo\",\"brrr\",\"c\",\"ça\",\"car\",\"ce\",\"ceci\",\"cela\",\"celle\",\"celle-ci\",\"celle-là\",\"celles\",\"celles-ci\",\"celles-là\",\"celui\",\"celui-ci\",\"celui-là\",\"cent\",\"cependant\",\"certain\",\"certaine\",\"certaines\",\"certains\",\"certes\",\"ces\",\"cet\",\"cette\",\"ceux\",\"ceux-ci\",\"ceuxlà\",\"chacun\",\"chaque\",\"cher\",\"chère\",\"chères\",\"chers\",\"chez\",\"chiche\",\"chut\",\"ci\",\"cinq\",\"cinquantaine\",\"cinquante\",\"cinquantième\",\"cinquième\",\"clac\",\"clic\",\"combien\",\"comme\",\"comment\",\"compris\",\"concernant\",\"contre\",\"couic\",\"crac\",\"d\",\"da\",\"dans\",\"de\",\"debout\",\"dedans\",\"dehors\",\"delà\",\"depuis\",\"derrière\",\"des\",\"dès\",\"désormais\",\"desquelles\",\"desquels\",\"dessous\",\"dessus\",\"deux\",\"deuxième\",\"deuxièmement\",\"devant\",\"devers\",\"devra\",\"différent\",\"différente\",\"différentes\",\"différents\",\"dire\",\"divers\",\"diverse\",\"diverses\",\"dix\",\"dix-huit\",\"dixième\",\"dix-neuf\",\"dixsept\",\"doit\",\"doivent\",\"donc\",\"dont\",\"douze\",\"douzième\",\"dring\",\"du\",\"duquel\",\"durant\",\"e\",\"effet\",\"eh\",\"elle\",\"elle-même\",\"elles\",\"ellesmêmes\",\"en\",\"encore\",\"entre\",\"envers\",\"environ\",\"es\",\"ès\",\"est\",\"et\",\"etant\",\"étaient\",\"étais\",\"était\",\"étant\",\"etc\",\"été\",\"etre\",\"être\",\"eu\",\"euh\",\"eux\",\"eux-mêmes\",\"excepté\",\"f\",\"façon\",\"fais\",\"faisaient\",\"faisant\",\"fait\",\"feront\",\"fi\",\"flac\",\"floc\",\"font\",\"g\",\"gens\",\"h\",\"ha\",\"hé\",\"hein\",\"hélas\",\"hem\",\"hep\",\"hi\",\"ho\",\"holà\",\"hop\",\"hormis\",\"hors\",\"hou\",\"houp\",\"hue\",\"hui\",\"huit\",\"huitième\",\"hum\",\"hurrah\",\"i\",\"il\",\"ils\",\"importe\",\"j\",\"je\",\"jusqu\",\"jusque\",\"k\",\"l\",\"la\",\"là\",\"laquelle\",\"las\",\"le\",\"lequel\",\"les\",\"lès\",\"lesquelles\",\"lesquels\",\"leur\",\"leurs\",\"longtemps\",\"lorsque\",\"lui\",\"lui-même\",\"m\",\"ma\",\"maint\",\"mais\",\"malgré\",\"me\",\"même\",\"mêmes\",\"merci\",\"mes\",\"mien\",\"mienne\",\"miennes\",\"miens\",\"mille\",\"mince\",\"moi\",\"moi-même\",\"moins\",\"mon\",\"moyennant\",\"n\",\"na\",\"ne\",\"néanmoins\",\"neuf\",\"neuvième\",\"ni\",\"nombreuses\",\"nombreux\",\"non\",\"nos\",\"notre\",\"nôtre\",\"nôtres\",\"nous\",\"nous-mêmes\",\"nul\",\"o\",\"o|\",\"ô\",\"oh\",\"ohé\",\"olé\",\"ollé\",\"on\",\"ont\",\"onze\",\"onzième\",\"ore\",\"ou\",\"où\",\"ouf\",\"ouias\",\"oust\",\"ouste\",\"outre\",\"p\",\"paf\",\"pan\",\"par\",\"parmi\",\"partant\",\"particulier\",\"particulière\",\"particulièrement\",\"pas\",\"passé\",\"pendant\",\"personne\",\"peu\",\"peut\",\"peuvent\",\"peux\",\"pff\",\"pfft\",\"pfut\",\"pif\",\"plein\",\"plouf\",\"plus\",\"plusieurs\",\"plutôt\",\"pouah\",\"pour\",\"pourquoi\",\"premier\",\"première\",\"premièrement\",\"près\",\"proche\",\"psitt\",\"puisque\",\"q\",\"qu\",\"quand\",\"quant\",\"quanta\",\"quant-à-soi\",\"quarante\",\"quatorze\",\"quatre\",\"quatre- vingt\",\"quatrième\",\"quatrièmement\",\"que\",\"quel\",\"quelconque\",\"quelle\",\"quelles\",\"quelque\",\"quelques\",\"quelqu'un\",\"quels\",\"qui\",\"quiconque\",\"quinze\",\"quoi\",\"quoique\",\"r\",\"revoici\",\"revoilà\",\"rien\",\"s\",\"sa\",\"sacrebleu\",\"sans\",\"sapristi\",\"sauf\",\"se\",\"seize\",\"selon\",\"sept\",\"septième\",\"sera\",\"seront\",\"ses\",\"si\",\"sien\",\"sienne\",\"siennes\",\"siens\",\"sinon\",\"six\",\"sixième\",\"soi\",\"soi-même\",\"soit\",\"soixante\",\"son\",\"sont\",\"sous\",\"stop\",\"suis\",\"suivant\",\"sur\",\"surtout\",\"t\",\"ta\",\"tac\",\"tant\",\"te\",\"té\",\"tel\",\"telle\",\"tellement\",\"telles\",\"tels\",\"tenant\",\"tes\",\"tic\",\"tien\",\"tienne\",\"tiennes\",\"tiens\",\"toc\",\"toi\",\"toi-même\",\"ton\",\"touchant\",\"toujours\",\"tous\",\"tout\",\"toute\",\"toutes\",\"treize\",\"trente\",\"très\",\"trois\",\"troisième\",\"troisièmement\",\"trop\",\"tsoin\",\"tsouin\",\"tu\",\"u\",\"un\",\"une\",\"unes\",\"uns\",\"v\",\"va\",\"vais\",\"vas\",\"vé\",\"vers\",\"via\",\"vif\",\"vifs\",\"vingt\",\"vivat\",\"vive\",\"vives\",\"vlan\",\"voici\",\"voilà\",\"vont\",\"vos\",\"votre\",\"vôtre\",\"vôtres\",\"vous\",\"vous-mêmes\",\"vu\",\"w\",\"x\",\"y\",\"z\",\"zut\"]\n",
    "    \n",
    "    return geopoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_html_pages_in_dirs_and_extract_mails_dict(pages_dir):\n",
    "    dict_mails={}\n",
    "    #rootdir=\"./DHFR_sample/\"\n",
    "    count_Cc=0\n",
    "    count_message=0\n",
    "    for subdir, dirs, files in os.walk(pages_dir):\n",
    "#ON VA LIRE TOUS LES FICHIERS DU REP ROOTDIR\n",
    "        \n",
    "        for file in files:\n",
    "    #V1     if file.endswith('.html') == True  and file != (\"index.html\") and not re.match(\"mail\\d+.html\",file) : \n",
    "           if re.match(\"msg\\d+.html\",file) :\n",
    "          \n",
    "            # les index.html sont des recap, on pourrait les compter pour s'assurer du nombre de messages\n",
    "                \n",
    "                #!print \"\\n\",os.path.join(subdir, file), \"\\n\"\n",
    "                champs_mail={}\n",
    "                filename=os.path.join(subdir, file)\n",
    "                champs_mail[\"Geo_Topic\"]=read_html_and_testtextextract(filename)\n",
    "                \n",
    "                \n",
    "                soup=BeautifulSoup(open(filename, 'r').read(),'lxml')#, 'html.parser') \n",
    "#RECUPERER LE SUJET DU MESSAGE\n",
    "                \n",
    "                i=0\n",
    "                for zone_cible in soup.findAll('ul'):\n",
    "                    i+=1\n",
    "                    sujet_messg = zone_cible.find(string=re.compile(\"\\[DH\\]\"))\n",
    "                    zone_de_metadonnees = None\n",
    "                    special_auteur=0\n",
    "##ON ISOLE LE CHAMPS AUTEUR\n",
    "                    if '<li><strong>From</strong>:' in str(zone_cible) and '<li><strong>To</strong>:' in str(zone_cible) :\n",
    "                        #print \"FROM/TO DETECTED, i= \",i\n",
    "                        count_message+=1\n",
    "                        #print  zone_cible\n",
    "                        nom_auteur= re.sub(r'<ul>\\n<li><strong>From</strong>: ', '',str(zone_cible))\n",
    "                        sep = \"&lt;\"\n",
    "                        nom_auteur = nom_auteur.split(sep, 1)[0]\n",
    "                        \n",
    "                        \n",
    "                        mail1= re.sub(r'<ul>\\n<li><strong>From</strong>:.+\\n.+\\n.+\\n document\\.write\\(\\\"', '',str(zone_cible))\n",
    "                        mail2=re.sub(r'<ul>\\n<li><strong>From</strong>:.+\\n.+\\n.+\\n document\\.write\\(\\\".+@\\\" \\+ \\\"', '',str(zone_cible))\n",
    "                        sep1 = \"\\\" + \\\"@\\\" + \"\n",
    "                        sep2 = \"\\\")\"\n",
    "                        mail1= mail1.split(sep1, 1)[0]\n",
    "                        mail2= mail2.split(sep2, 1)[0]\n",
    "                        mail_auteur= mail1+\"@\"+mail2\n",
    "                        #print mail_auteur\n",
    "                        if len(mail_auteur)>60:\n",
    "                            try:\n",
    "                                mail= re.sub(r'<ul>\\n<li><strong>From</strong>:', '',str(zone_cible))\n",
    "                                mail=re.search('&lt;(.*)&gt;',str(mail))\n",
    "                                mail=mail.group(1)\n",
    "                                #print \"MAILBIS\",mail\n",
    "                                mail_auteur=mail\n",
    "                                #special_auteur=1\n",
    "                            except:\n",
    "                                print \"BUG\"\n",
    "                        #CHECK PROPER CAPTURE OF THE NAME    \n",
    "                        if len(nom_auteur)>50:\n",
    "                            #print zone_cible\n",
    "                            nom_auteur=mail_auteur\n",
    "                        if len(nom_auteur)<5:\n",
    "                            nom_auteur=mail_auteur\n",
    "                        if nom_auteur[0]== \"\\\"\":\n",
    "                            nom_auteur=nom_auteur[1:]\n",
    "                            nom_auteur=nom_auteur[:-2]\n",
    "                            print nom_auteur\n",
    "                        if nom_auteur.endswith(' '):\n",
    "                            nom_auteur=nom_auteur[:-1]\n",
    "                        #!print \"nom_auteur \",nom_auteur+\" mail_auteur \"+mail_auteur,\"\\n\"\n",
    "                        champs_mail[\"nom_auteur\"]=  nom_auteur\n",
    "                        champs_mail[\"mail_auteur\"]=  mail_auteur\n",
    "                        champs_mail[\"ref_physique_de_l_article\"]=  os.path.join(subdir, file)\n",
    "                        champs_mail[\"sujet_du_message\"] = sujet_messg\n",
    "##ON ISOLE LE CHAMP DESTINATAIRES\n",
    "                        #if special_auteur == 1:\n",
    "                            #print \"ZCspecAut\",zone_cible\n",
    "                        #else: \n",
    "                        \n",
    "                        destinataires=re.sub(r'<ul>\\n<li><strong>From</strong>:.+\\n.+\\n.+\\n.+\\n.+\\n.+\\n.+\\n', '',str(zone_cible))\n",
    "                        \n",
    "                        sep3=\";</li>\"\n",
    "                        destinataires= destinataires.split(sep3, 1)[0]\n",
    "                        #print destinataires\n",
    "                        dest=re.findall(\"<script type=.*\\n.+\\n.+\\n.+\\n.+/script>\", destinataires)\n",
    "                        #print dest\n",
    "                        liste_destinataires=[]\n",
    "                        for destinataire in dest:\n",
    "                            #print destinataire,\"\\n___________________\\n\"\n",
    "                            mail1= re.sub(r'<script type=\\\"text/javascript\\\">\\n <!-- \\n document\\.write\\(\\\"', '',str(destinataire))\n",
    "                            mail2=re.sub(r'<script type=\\\"text/javascript\\\">\\n <!-- \\n document\\.write\\(\\\".+@\\\" \\+ \\\"', '',str(destinataire))\n",
    "                            sep1 = \"\\\" + \\\"@\\\" + \"\n",
    "                            sep2 = \"\\\")\"\n",
    "                            mail1= mail1.split(sep1, 1)[0]\n",
    "                            mail2= mail2.split(sep2, 1)[0]\n",
    "                            mail_destinataire= mail1+\"@\"+mail2\n",
    "                            #print mail_destinataire,\"\\n___________\\n\"\n",
    "                            if mail_destinataire not in liste_destinataires :\n",
    "                                liste_destinataires.append(mail_destinataire)\n",
    "                        if liste_destinataires==[]:\n",
    "                            #print zone_cible\n",
    "                            liste_destinataires=['dh@groupes.renater.fr']\n",
    "                        #!print \"destinataires: \",liste_destinataires,\"\\n\"\n",
    "                        champs_mail[\"liste_mails_dests\"] = liste_destinataires\n",
    "                       \n",
    "                        if '<li><strong>Cc</strong>:' in str(zone_cible):\n",
    "##ON ISOLE LE CHAMP CC\n",
    "                            #print \"CC DETECTED, i= \",i\n",
    "                            #print zone_cible\n",
    "                            count_Cc+=1\n",
    "                            CCfield= re.search('<li><strong>Cc</strong>:(.*)<li><strong>Subject</strong>',str(zone_cible),flags=re.DOTALL)\n",
    "                            CC= CCfield.group(1)\n",
    "                            CC=str(CC)\n",
    "                            CC=re.sub(r'^.+\\n','',str(CC),flags=0)\n",
    "                            CCs=re.findall(\"<script type=.*\\n.+\\n.+\\n.+\\n.+/script>\", CC)\n",
    "                            #sep1 = \"\\\" + \\\"@\\\" + \"\n",
    "                            #mail1= mail1.split(sep1, 1)[0]\n",
    "                            #print \"CC FIELd\\n\"#,CCs\n",
    "                            liste_CCs=[]\n",
    "                            for CC in CCs:\n",
    "                                #print CC,\"\\n___________________\\n\"\n",
    "                                mail1= re.sub(r'<script type=\\\"text/javascript\\\">\\n <!-- \\n document\\.write\\(\\\"', '',str(CC))\n",
    "                                mail2=re.sub(r'<script type=\\\"text/javascript\\\">\\n <!-- \\n document\\.write\\(\\\".+@\\\" \\+ \\\"', '',str(CC))\n",
    "                                sep1 = \"\\\" + \\\"@\\\" + \"\n",
    "                                sep2 = \"\\\")\"\n",
    "                                mail1= mail1.split(sep1, 1)[0]\n",
    "                                mail2= mail2.split(sep2, 1)[0]\n",
    "                                mail_CC= mail1+\"@\"+mail2\n",
    "                                #print mail_CC,\"\\n___________\\n\"\n",
    "                                if mail_CC not in liste_CCs :\n",
    "                                    liste_CCs.append(mail_CC)\n",
    "                            #!print \"CCs: \",liste_CCs\n",
    "                            champs_mail[\"liste_CCs\"] = liste_CCs\n",
    "##ON ISOLE LE CHAMP DATE                       \n",
    "                        #print \"ZONECIBLE\",zone_cible\n",
    "                        champs_datef=re.search('<li><strong>Date</strong>:(.+?)</li>',str(zone_cible),flags=re.DOTALL)\n",
    "                        #print \"CHAMPSDATEF\",champs_datef\n",
    "                        try :\n",
    "                            champs_date=champs_datef.group(1)\n",
    "                        except :\n",
    "                            #print \"ZCparent\", zone_cible.parent\n",
    "                            champs_date=re.search('<!--X-Date: (.*) -->',str(soup))\n",
    "                            champs_date=champs_date.group(1)\n",
    "                            #print champs_date\n",
    "                            \n",
    "                        #print champs_date\n",
    "                        try : \n",
    "                            champs_date=parser.parse(champs_date)\n",
    "                        except ValueError :\n",
    "                            sep=\" (\"\n",
    "                            champs_date=champs_date.split(sep, 1)[0]\n",
    "                            \n",
    "                        #!print  champs_date\n",
    "                        champs_mail[\"date\"] = champs_date\n",
    "                        \n",
    "                        \n",
    "                        dict_mails[os.path.join(subdir, file)]= champs_mail\n",
    "                        break\n",
    "##LE DICTIONNAIRE DES MAILS SE NOMME dict_mails\n",
    "    #print dict_mails\n",
    "    print count_message,\" messages\"\n",
    "    print count_Cc, \" messages avec Cc\"\n",
    "    ratiocount = float(count_Cc)/float(count_message)\n",
    "    print \"Taux_de_mails_avec_CC:\",ratiocount,\"\\n\"\n",
    "    return dict_mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus_and_extract_topo_network(corpus):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus_and_extract_topo_Geo_network(corpus):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus_and_extract_mails(corpus):\n",
    "    liste_mails=[]\n",
    "    for mesg in corpus:\n",
    "        #print corpus[mesg]\n",
    "        if corpus[mesg]['mail_auteur'] not in liste_mails:\n",
    "            liste_mails.append(corpus[mesg]['mail_auteur'])\n",
    "        for i,key in enumerate(corpus[mesg]['liste_mails_dests']):\n",
    "            #print key\n",
    "            if key not in liste_mails:\n",
    "                liste_mails.append(key)\n",
    "        if 'liste_CCs' in corpus[mesg] :\n",
    "            #print \"TRUVE CC\"\n",
    "            for i,key in enumerate(corpus[mesg]['liste_CCs']):\n",
    "                #print key\n",
    "                if key not in liste_mails:\n",
    "                    liste_mails.append(key)\n",
    "    return liste_mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus_and_extract_localisations(corpus):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus_and_extract_names(corpus):\n",
    "    liste_noms=[]\n",
    "    duplicate_list_noms=[]\n",
    "    for mesg in corpus:\n",
    "        try: \n",
    "            if corpus[mesg]['nom_auteur'][0]== \"\\\"\":\n",
    "                print corpus[mesg]['nom_auteur']\n",
    "                \n",
    "        except IndexError:\n",
    "            print \"Bug\", corpus[mesg]['nom_auteur']\n",
    "            print corpus[mesg]\n",
    "            \n",
    "        if [corpus[mesg]['nom_auteur'],corpus[mesg]['mail_auteur']] not in liste_noms:\n",
    "            for i,key in enumerate(liste_noms):\n",
    "                if SequenceMatcher(None, corpus[mesg]['nom_auteur'], key[0]).ratio() >=0.9:\n",
    "                    print \"POSSIBLE DUPLICATE |\", corpus[mesg]['nom_auteur'],\"|   |\",key[0],\"|\"\n",
    "                    print corpus[mesg]['mail_auteur'], key[1]\n",
    "                    if [[corpus[mesg]['nom_auteur'],corpus[mesg]['mail_auteur']],[key[0],key[1]]] not in duplicate_list_noms:\n",
    "                        \n",
    "                        duplicate_list_noms.append([[corpus[mesg]['nom_auteur'],corpus[mesg]['mail_auteur']],[key[0],key[1]]])\n",
    "                    \n",
    "                \n",
    "            liste_noms.append([corpus[mesg]['nom_auteur'],corpus[mesg]['mail_auteur']])\n",
    "        \n",
    "    return [liste_noms,duplicate_list_noms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_noms_and_dups=read_corpus_and_extract_names(corpus_mails_DH)\n",
    "print len(liste_noms_and_dups[0]),len(liste_noms_and_dups[1])\n",
    "print liste_noms_and_dups[0]\n",
    "print \"________________________\\n\"\n",
    "print liste_noms_and_dups[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pierre Mounier': [['pierre.mounier@openedition.org', datetime.datetime(2013, 7, 5, 10, 9, 24, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 5, 11, 11, tzinfo=tzoffset(None, 3600))], ['pierre.mounier@ehess.fr', datetime.datetime(2013, 7, 5, 12, 13, 22, tzinfo=tzoffset(None, 7200)), datetime.datetime(2013, 7, 5, 12, 13, 22, tzinfo=tzoffset(None, 7200))]], 'Thierry Poibeau': [['thierry.poibeau@ens.fr', datetime.datetime(2018, 3, 5, 7, 56, 29, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 26, 9, 17, tzinfo=tzoffset(None, 7200))]], 'delegue.general@adbu.fr': [['delegue.general@adbu.fr', datetime.datetime(2017, 6, 13, 15, 36, 29, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 13, 15, 36, 29, tzinfo=tzoffset(None, 7200))]], 'Dominique Stutzmann': [['dominique.stutzmann@irht.cnrs.fr', datetime.datetime(2012, 7, 2, 8, 25, 19, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 27, 14, 18, 48, tzinfo=tzoffset(None, 7200))]], 'Martin Grandjean': [['martin.grandjean@unil.ch', datetime.datetime(2016, 7, 6, 16, 1, 43, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 19, 14, 6, 2, tzinfo=tzoffset(None, 3600))]], 'Nicolas Larrousse': [['Nicolas.Larrousse@huma-num.fr', datetime.datetime(2017, 6, 6, 17, 11, 37, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 9, 15, 47, 7, tzinfo=tzoffset(None, 7200))]], 'HANNOUN Judith': [['judith.hannoun@univ-amu.fr', datetime.datetime(2018, 3, 2, 15, 19, 10, tzinfo=tzutc()), datetime.datetime(2018, 3, 2, 15, 19, 10, tzinfo=tzutc())]], 'Nancy Ottaviano': [['ottaviano.nancy@gmail.com', datetime.datetime(2014, 7, 9, 22, 34, 26, tzinfo=tzoffset(None, 7200)), datetime.datetime(2014, 7, 9, 22, 34, 26, tzinfo=tzoffset(None, 7200))]], 'YAMINA BENSAADOUNE': [['yamina.bensaadoune@univ-rouen.fr', datetime.datetime(2015, 7, 15, 17, 7, 53, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 15, 17, 7, 53, tzinfo=tzoffset(None, 7200))]], 'Paquien-S\\xc3\\xa9guy Fran\\xc3\\xa7oise\\t': [['francoise.paquienseguy@sciencespo-lyon.fr', datetime.datetime(2017, 6, 9, 8, 5, 48, tzinfo=tzutc()), datetime.datetime(2017, 6, 16, 18, 12, 58, tzinfo=tzutc())]], 'Cecile Rodrigues': [['cecile.rodrigues@cnrs.fr', datetime.datetime(2018, 3, 9, 14, 10, 27, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 9, 14, 10, 27, tzinfo=tzoffset(None, 3600))]], 'Michel Bernard': [['michel.bernard@univ-paris3.fr', datetime.datetime(2014, 7, 10, 9, 9, 21, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 9, 17, 52, 54, tzinfo=tzoffset(None, 3600))]], 'Fran\\xc3\\xa7ois Bavaud': [['francois.bavaud@unil.ch', datetime.datetime(2018, 3, 16, 8, 5, 59, tzinfo=tzutc()), datetime.datetime(2018, 3, 16, 8, 5, 59, tzinfo=tzutc())]], 'beauguitte laurent': [['beauguittelaurent@hotmail.com', datetime.datetime(2017, 6, 28, 8, 20, 32, tzinfo=tzutc()), datetime.datetime(2017, 6, 28, 8, 20, 32, tzinfo=tzutc())]], 'Audrey Baneyx - Sciences Po': [['audrey.baneyx@sciencespo.fr', datetime.datetime(2015, 7, 27, 14, 16, 20, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 27, 14, 16, 20, tzinfo=tzoffset(None, 7200))]], 'Jouni Tuominen': [['jouni.tuominen@helsinki.fi', datetime.datetime(2017, 6, 1, 20, 0, 43, tzinfo=tzoffset(None, 10800)), datetime.datetime(2017, 6, 1, 20, 0, 43, tzinfo=tzoffset(None, 10800))], ['jouni.tuominen@aalto.fi', datetime.datetime(2018, 3, 13, 14, 20, 26, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 13, 14, 20, 26, tzinfo=tzoffset(None, 7200))]], 'Noiret, Serge': [['Serge.Noiret@EUI.eu', datetime.datetime(2011, 7, 18, 2, 6, 6, tzinfo=tzoffset(None, -25200)), datetime.datetime(2015, 7, 3, 10, 17, 19, tzinfo=tzutc())]], 'Seth van Hooland': [['svhoolan@ulb.ac.be', datetime.datetime(2018, 3, 2, 11, 1, 59, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 29, 15, 57, 2, tzinfo=tzoffset(None, 7200))]], 'Florence Andreacola': [['florence.andreacola@univ-grenoble-alpes.fr', datetime.datetime(2018, 3, 2, 11, 55, 28, tzinfo=tzlocal()), datetime.datetime(2018, 3, 2, 11, 55, 28, tzinfo=tzlocal())]], 'Olfa Lamloum': [['olfa.lamloum@gmail.com', datetime.datetime(2018, 3, 26, 14, 32, 33, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 26, 14, 32, 33, tzinfo=tzoffset(None, 3600))]], 'jerome valluy': [['jerome.valluy@univ-paris1.fr', datetime.datetime(2017, 6, 13, 19, 7, 46, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 13, 18, 57, 33, tzinfo=tzoffset(None, 3600))]], 'Caroline Muller': [['caroline.muller@univ-reims.fr', datetime.datetime(2016, 7, 11, 20, 40, 8, tzinfo=tzoffset(None, 7200)), datetime.datetime(2016, 7, 11, 20, 40, 8, tzinfo=tzoffset(None, 7200))]], 'R\\xc3\\xa9mi Jimenes': [['remi.jimenes@gmail.com', datetime.datetime(2017, 6, 21, 12, 12, 42, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 21, 12, 12, 42, tzinfo=tzoffset(None, 7200))]], 'stefanev': [['stefanev@club-internet.fr', datetime.datetime(2012, 7, 26, 18, 39, 41, tzinfo=tzlocal()), datetime.datetime(2012, 7, 26, 18, 39, 41, tzinfo=tzlocal())]], 'Lisette Calderan': [['lisette.calderan@inria.fr', datetime.datetime(2014, 7, 9, 11, 48, 10, tzinfo=tzoffset(None, 7200)), datetime.datetime(2014, 7, 9, 11, 48, 10, tzinfo=tzoffset(None, 7200))]], 'Gilles Pansu': [['gpansu@gmail.com', datetime.datetime(2017, 6, 9, 16, 32, 46, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 9, 16, 32, 46, tzinfo=tzoffset(None, 7200))]], 'H\\xc3\\xa9l\\xc3\\xa8ne de Foucaud': [['hdefouca@u-paris10.fr', datetime.datetime(2015, 7, 8, 16, 44, 49, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 8, 16, 44, 49, tzinfo=tzoffset(None, 7200))]], 'Laetitia Bontemps': [['laetitia.bontemps@univ-tours.fr', datetime.datetime(2011, 7, 19, 15, 4, 42, tzinfo=tzoffset(u'BST', 3600)), datetime.datetime(2011, 7, 19, 15, 4, 42, tzinfo=tzoffset(u'BST', 3600))]], 'Aur\\xc3\\xa9lie Olivesi': [['aurelie.olivesi@gmail.com', datetime.datetime(2015, 7, 16, 0, 22, 43, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 16, 0, 22, 43, tzinfo=tzoffset(None, 7200))]], 'Manuel Zacklad': [['manuel.zacklad@cnam.fr', datetime.datetime(2015, 7, 12, 18, 26, 27, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 12, 18, 26, 27, tzinfo=tzoffset(None, 7200))]], 'St\\xc3\\xa9phane Vial': [['vial.stephane@gmail.com', datetime.datetime(2017, 6, 11, 14, 29, 59, tzinfo=tzutc()), datetime.datetime(2018, 3, 6, 10, 58, 35, tzinfo=tzutc())]], 'Cynthia Pedroja': [['cynthia.pedroja@meshs.fr', datetime.datetime(2014, 7, 17, 14, 42, 17, tzinfo=tzoffset(None, 7200)), datetime.datetime(2014, 7, 17, 14, 42, 17, tzinfo=tzoffset(None, 7200))]], 'elisabeth.belmas': [['elisabeth.belmas@wanadoo.fr', datetime.datetime(2014, 7, 29, 12, 4, 36, tzinfo=tzoffset(None, 7200)), datetime.datetime(2014, 7, 29, 12, 4, 36, tzinfo=tzoffset(None, 7200))]], 'Richard Walter': [['richard.walter@ens.fr', datetime.datetime(2016, 7, 12, 16, 3, 9, tzinfo=tzoffset(None, 7200)), datetime.datetime(2016, 7, 12, 16, 3, 9, tzinfo=tzoffset(None, 7200))]], 'Olivier Le Deuff': [['oledeuff@gmail.com', datetime.datetime(2013, 7, 12, 15, 54, 35, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 6, 11, 48, 1, tzinfo=tzoffset(None, 3600))]], 'Christine Michel': [['Christine.michel@insa-lyon.fr', datetime.datetime(2016, 7, 28, 19, 24, 3, tzinfo=tzoffset(None, 7200)), datetime.datetime(2016, 7, 28, 19, 24, 3, tzinfo=tzoffset(None, 7200))]], 'Laurence Rageot': [['laurence.rageot@univ-tours.fr', datetime.datetime(2015, 7, 8, 15, 10, 16, tzinfo=tzlocal()), datetime.datetime(2018, 3, 12, 13, 45, 47, tzinfo=tzlocal())]], 'CHADIER Christine': [['christine.chadier@univ-lyon3.fr', datetime.datetime(2015, 7, 30, 9, 52, 1, tzinfo=tzutc()), datetime.datetime(2015, 7, 30, 9, 59, 31, tzinfo=tzutc())]], 'Emilien RUIZ': [['emilien.ruiz@ehess.fr', datetime.datetime(2012, 7, 2, 14, 47, 31, tzinfo=tzoffset(None, 7200)), datetime.datetime(2012, 7, 2, 14, 47, 31, tzinfo=tzoffset(None, 7200))]], 'Guillaume Blum': [['Guillaume.Blum@design.ulaval.ca', datetime.datetime(2017, 6, 21, 17, 16, 28, tzinfo=tzutc()), datetime.datetime(2017, 6, 21, 17, 16, 28, tzinfo=tzutc())]], 'Wandl-Vogt, Eveline': [['Eveline.Wandl-Vogt@oeaw.ac.at', datetime.datetime(2017, 6, 25, 22, 53, 30, tzinfo=tzutc()), datetime.datetime(2017, 6, 26, 20, 54, 38, tzinfo=tzutc())]], 'Antonio Casilli': [['antonio.casilli@googlemail.com', datetime.datetime(2014, 7, 7, 11, 23, 8, tzinfo=tzoffset(None, 7200)), datetime.datetime(2014, 7, 7, 11, 23, 8, tzinfo=tzoffset(None, 7200))], ['antonio.casilli@ehess.fr', datetime.datetime(2018, 3, 12, 8, 56, 36, tzinfo=tzlocal()), datetime.datetime(2018, 3, 13, 7, 55, 52, tzinfo=tzlocal())]], 'G\\xc3\\xa9rald KEMBELLEC': [['gerald.kembellec@lecnam.net', datetime.datetime(2016, 7, 6, 18, 55, 47, tzinfo=tzoffset(None, 7200)), datetime.datetime(2016, 7, 6, 18, 55, 47, tzinfo=tzoffset(None, 7200))]], 'Jeanne Herzog': [['jea.herzog@gmail.com', datetime.datetime(2017, 6, 10, 7, 37, 15, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 10, 7, 37, 15, tzinfo=tzoffset(None, 7200))]], 'Claire Clivaz': [['claire.clivaz@sib.swiss', datetime.datetime(2016, 7, 20, 16, 54, 26, tzinfo=tzoffset(None, 7200)), datetime.datetime(2016, 7, 29, 13, 21, 16, tzinfo=tzoffset(None, 7200))], ['claire.clivaz@unil.ch', datetime.datetime(2013, 7, 9, 15, 28, 26, tzinfo=tzoffset(None, 7200)), datetime.datetime(2014, 7, 12, 12, 31, 28, tzinfo=tzoffset(None, 7200))]], 'Fatiha IDMHAND': [['fatihaidmhand@yahoo.es', datetime.datetime(2016, 7, 19, 9, 59, 52, tzinfo=tzutc()), datetime.datetime(2016, 7, 19, 9, 59, 52, tzinfo=tzutc())]], 'Yosra Ghliss': [['yosra.ghliss17@gmail.com', datetime.datetime(2017, 6, 6, 17, 17, 31, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 6, 17, 17, 31, tzinfo=tzoffset(None, 7200))]], 'Equipe projet IGLouvre': [['projet.iglouvre@gmail.com', datetime.datetime(2014, 7, 17, 12, 39, 42, tzinfo=tzoffset(None, 7200)), datetime.datetime(2014, 7, 17, 12, 39, 42, tzinfo=tzoffset(None, 7200))]], 'Marta Severo': [['martaseve@gmail.com', datetime.datetime(2015, 7, 24, 15, 52, 17, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 24, 15, 52, 17, tzinfo=tzoffset(None, 7200))]], 'Julia Bonaccorsi': [['julia.bonaccorsi@univ-lyon2.fr', datetime.datetime(2016, 7, 1, 12, 39, 37, tzinfo=tzoffset(None, 7200)), datetime.datetime(2016, 7, 1, 12, 39, 37, tzinfo=tzoffset(None, 7200))]], 'Annael LE POULLENNEC': [['annael.le-poullennec@psl.eu', datetime.datetime(2018, 3, 1, 14, 3, 55, tzinfo=tzutc()), datetime.datetime(2018, 3, 1, 14, 3, 55, tzinfo=tzutc())]], 'emmanuel guez': [['emmanuelguez@yahoo.fr', datetime.datetime(2014, 7, 22, 17, 24, 51, tzinfo=tzoffset(None, 7200)), datetime.datetime(2014, 7, 22, 17, 24, 51, tzinfo=tzoffset(None, 7200))]], 'Francoise Blum': [['Francoise.Blum@univ-paris1.fr', datetime.datetime(2017, 6, 26, 17, 4, 56, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 26, 17, 4, 56, tzinfo=tzoffset(None, 7200))]], 'Elena Gonz\\xc3\\xa1lez-Blanco': [['elenagonzalezblanco@yahoo.es', datetime.datetime(2015, 7, 7, 23, 20, 18, tzinfo=tzutc()), datetime.datetime(2016, 7, 18, 0, 53, 35, tzinfo=tzutc())]], 'Caroline Cance': [['caroline.cance@univ-orleans.fr', datetime.datetime(2018, 3, 21, 9, 24, 41, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 21, 9, 24, 41, tzinfo=tzoffset(None, 3600))]], 'Viera Rebolledo-Dhuin': [['viera.rebolledodhuin@free.fr', datetime.datetime(2014, 7, 16, 20, 28, 39, tzinfo=tzoffset(None, 7200)), datetime.datetime(2014, 7, 16, 20, 28, 39, tzinfo=tzoffset(None, 7200))]], 'isabelle.thiebau@univ-lille2.fr': [['isabelle.thiebau@univ-lille2.fr', datetime.datetime(2016, 7, 6, 18, 52, 24, tzinfo=tzoffset(None, 7200)), datetime.datetime(2016, 7, 20, 0, 0, 32, tzinfo=tzoffset(None, 7200))]], 'Alexandre Moatti': [['alexandre.moatti@mines.org', datetime.datetime(2013, 7, 24, 19, 28, 45, tzinfo=tzoffset(None, 7200)), datetime.datetime(2013, 7, 24, 19, 28, 45, tzinfo=tzoffset(None, 7200))]], 'Serge Heiden': [['slh@ens-lyon.fr', datetime.datetime(2012, 7, 26, 21, 21, 27, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 15, 11, 33, 42, tzinfo=tzoffset(None, 3600))]], 'Epron Beno\\xc3\\xaet': [['benoit.epron@enssib.fr', datetime.datetime(2017, 6, 9, 11, 29, 3, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 9, 11, 29, 3, tzinfo=tzoffset(None, 7200))]], 'Jorge Fins': [['jorge.fins@univ-tours.fr', datetime.datetime(2017, 6, 8, 13, 51, 2, tzinfo=tzlocal()), datetime.datetime(2017, 6, 19, 10, 6, 25, tzinfo=tzlocal())]], 'Rapha\\xc3\\xablle Bour': [['raphaelle.bour@irit.fr', datetime.datetime(2018, 3, 21, 12, 43, 49, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 21, 12, 43, 49, tzinfo=tzoffset(None, 3600))]], 'CAROLINE ROSSI': [['caroline.rossi@univ-grenoble-alpes.fr', datetime.datetime(2017, 6, 16, 15, 27, 11, tzinfo=tzlocal()), datetime.datetime(2017, 6, 16, 15, 27, 11, tzinfo=tzlocal())]], 'G\\xc3\\xa9rald Kembellec': [['gerald.kembellec@cnam.fr', datetime.datetime(2015, 7, 13, 12, 46, 15, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 26, 19, 12, 16, tzinfo=tzoffset(None, 7200))]], 'Mehdi Khamassi': [['mehdi.khamassi@upmc.fr', datetime.datetime(2016, 7, 3, 7, 31, 28, tzinfo=tzoffset(None, 7200)), datetime.datetime(2016, 7, 3, 7, 31, 28, tzinfo=tzoffset(None, 7200))]], 'Carmen Brando': [['carmen.brando@gmail.com', datetime.datetime(2017, 6, 7, 14, 12, 9, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 7, 14, 12, 9, tzinfo=tzoffset(None, 7200))]], 'Sarah Cadorel': [['sarah.cadorel@sciencespo.fr', datetime.datetime(2016, 7, 13, 15, 27, 26, tzinfo=tzoffset(None, 7200)), datetime.datetime(2016, 7, 13, 15, 27, 26, tzinfo=tzoffset(None, 7200))]], 'listes@ahicf.com': [['listes@ahicf.com', datetime.datetime(2013, 7, 18, 10, 20, 44, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 28, 16, 43, 54, tzinfo=tzoffset(None, 7200))]], 'recherche.coordination@bnf.fr': [['recherche.coordination@bnf.fr', datetime.datetime(2018, 3, 12, 8, 39, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 12, 8, 39, tzinfo=tzoffset(None, 3600))]], 'Cristofoli Pascal': [['pascal.cristofoli@ehess.fr', datetime.datetime(2018, 3, 12, 12, 39, 6, tzinfo=tzlocal()), datetime.datetime(2018, 3, 12, 12, 39, 6, tzinfo=tzlocal())]], 'Johann Holland': [['johann.holland@campus-condorcet.fr', datetime.datetime(2014, 7, 6, 14, 32, 27, tzinfo=tzoffset(None, 7200)), datetime.datetime(2016, 7, 19, 10, 46, 36, tzinfo=tzoffset(None, 7200))]], 'C\\xc3\\xa9cile BOULAIRE': [['cecile.boulaire@univ-tours.fr', datetime.datetime(2014, 7, 9, 20, 12, 7, tzinfo=tzoffset(None, 7200)), datetime.datetime(2014, 7, 9, 20, 12, 7, tzinfo=tzoffset(None, 7200))]], 'Am\\xc3\\xa9lie VAIRELLES': [['amelie.vairelles@sciencespo.fr', datetime.datetime(2018, 3, 23, 17, 23, 21, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 23, 17, 23, 21, tzinfo=tzoffset(None, 3600))]], 'IRIHS': [['irihs@univ-rouen.fr', datetime.datetime(2018, 3, 15, 9, 12, 21, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 15, 9, 12, 21, tzinfo=tzoffset(None, 3600))]], 'Myriam TAZI': [['myriam.tazi@sciencespo.fr', datetime.datetime(2017, 6, 8, 12, 11, 54, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 12, 8, 42, 16, tzinfo=tzoffset(None, 7200))]], 'Elena Spadini': [['spadinielena@gmail.com', datetime.datetime(2015, 7, 16, 12, 16, 27, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 16, 12, 16, 27, tzinfo=tzoffset(None, 7200))]], 'Joel Marchand': [['joel.marchand@huma-num.fr', datetime.datetime(2018, 3, 26, 9, 0, 20, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 26, 9, 0, 20, tzinfo=tzoffset(None, 7200))]], 'Laurent Romary': [['laurent.romary@inria.fr', datetime.datetime(2014, 7, 30, 15, 35, 56, tzinfo=tzoffset(None, 7200)), datetime.datetime(2016, 7, 6, 19, 1, 56, tzinfo=tzoffset(None, 7200))]], 'Elodie Faath': [['elodie.faath@openedition.org', datetime.datetime(2018, 3, 28, 15, 32, 23, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 28, 15, 32, 23, tzinfo=tzoffset(None, 7200))]], 'Fr\\xc3\\xa9d\\xc3\\xa9ric Clavert': [['Frederic.Clavert@unil.ch', datetime.datetime(2016, 7, 7, 13, 11, 56, tzinfo=tzutc()), datetime.datetime(2016, 7, 7, 13, 11, 56, tzinfo=tzutc())], ['frederic@clavert.net', datetime.datetime(2012, 7, 10, 10, 39, 11, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 2, 15, 41, 35, tzinfo=tzoffset(None, 7200))]], 'Aurelie OLIVESI': [['aurelie.olivesi@wanadoo.fr', datetime.datetime(2014, 7, 9, 22, 2, 2, tzinfo=tzlocal()), datetime.datetime(2014, 7, 9, 22, 2, 2, tzinfo=tzlocal())]], 'Antoine Blanchard': [['antoine.blanchard@gmail.com', datetime.datetime(2013, 7, 18, 10, 47, 22, tzinfo=tzoffset(None, 7200)), datetime.datetime(2013, 7, 18, 10, 47, 22, tzinfo=tzoffset(None, 7200))]], 'Javier Espejo Sur\\xc3\\xb3s': [['espejosuros.javier@gmail.com', datetime.datetime(2013, 7, 12, 22, 12, 37, tzinfo=tzoffset(None, 7200)), datetime.datetime(2013, 7, 12, 22, 12, 37, tzinfo=tzoffset(None, 7200))]], 'B\\xc3\\xa9atrice Markhoff': [['beatrice.markhoff@univ-tours.fr', datetime.datetime(2017, 6, 13, 14, 27, 18, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 4, 11, 39, 36, tzinfo=tzoffset(None, 3600))]], 'Sofia Papastamkou': [['stamkou@free.fr', datetime.datetime(2015, 7, 13, 17, 8, 22, tzinfo=tzlocal()), datetime.datetime(2017, 6, 10, 9, 8, 54, tzinfo=tzlocal())]], 'Graham Ranger': [['graham.ranger@univ-avignon.fr', datetime.datetime(2018, 3, 15, 20, 45, 58, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 15, 20, 45, 58, tzinfo=tzoffset(None, 3600))]], 'Anne Gresillon': [['gresillon@cmb.hu-berlin.de', datetime.datetime(2018, 3, 7, 15, 44, 1, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 7, 15, 44, 1, tzinfo=tzoffset(None, 3600))]], 'camille monnier': [['camillemonnier33@hotmail.com', datetime.datetime(2017, 6, 12, 9, 19, 59, tzinfo=tzutc()), datetime.datetime(2017, 6, 12, 9, 19, 59, tzinfo=tzutc())]], 'Nicolas Th\\xc3\\xa9ly': [['nicolasthelyrennes2@gmail.com', datetime.datetime(2017, 6, 12, 14, 18, 46, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 12, 14, 18, 46, tzinfo=tzoffset(None, 7200))]], 'ekergosien': [['eric.kergosien@univ-lille3.fr', datetime.datetime(2017, 6, 17, 13, 52, 49, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 12, 15, 18, 57, tzinfo=tzoffset(None, 3600))]], 'Elina Leblanc': [['elinaleblanc3007@gmail.com', datetime.datetime(2016, 7, 26, 16, 20, 6, tzinfo=tzoffset(None, 7200)), datetime.datetime(2016, 7, 26, 16, 20, 6, tzinfo=tzoffset(None, 7200))]], 'Georges-Xavier Blary': [['georges-xavier.blary@unilim.fr', datetime.datetime(2018, 3, 12, 11, 0, 23, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 12, 11, 0, 23, tzinfo=tzoffset(None, 3600))]], 'Eric Guichard': [['Eric.Guichard@enssib.fr', datetime.datetime(2013, 7, 10, 15, 56, 42, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 19, 19, 7, 45, tzinfo=tzoffset(None, 7200))]], 'Marjorie Burghart': [['marjorie.burghart@ehess.fr', datetime.datetime(2013, 7, 5, 1, 35, 31, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 9, 13, 52, 33, tzinfo=tzlocal())]], 'florence.clavaud@free.fr': [['florence.clavaud@free.fr', datetime.datetime(2018, 3, 2, 15, 36, 5, tzinfo=tzlocal()), datetime.datetime(2018, 3, 2, 15, 36, 5, tzinfo=tzlocal())]], 'Enrica Salvatori': [['e.salvatori@mediev.unipi.it', datetime.datetime(2014, 7, 11, 10, 0, 21, tzinfo=tzoffset(None, 7200)), datetime.datetime(2014, 7, 11, 10, 0, 21, tzinfo=tzoffset(None, 7200))]], 'Alexandre Hocquet': [['alexandre.hocquet@univ-lorraine.fr', datetime.datetime(2017, 6, 27, 14, 19, 29, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 27, 14, 19, 29, tzinfo=tzoffset(None, 7200))]], 'Elena Pierazzo': [['pierazzo@gmail.com', datetime.datetime(2018, 3, 20, 12, 33, 7, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 20, 12, 33, 7, tzinfo=tzoffset(None, 3600))]], 'J\\xc3\\xa9r\\xc3\\xb4me Darmont': [['jerome.darmont@univ-lyon2.fr', datetime.datetime(2018, 3, 9, 11, 17, 31, tzinfo=tzlocal()), datetime.datetime(2018, 3, 9, 11, 17, 31, tzinfo=tzlocal())]], 'Isabelle Thiebau': [['isabelle.thiebau@univ-lille2.fr', datetime.datetime(2016, 7, 6, 18, 52, 24, tzinfo=tzoffset(None, 7200)), datetime.datetime(2016, 7, 20, 0, 0, 32, tzinfo=tzoffset(None, 7200))]], 'Marin Dacos': [['marin.dacos@openedition.org', datetime.datetime(2012, 7, 16, 15, 51, 56, tzinfo=tzoffset(None, 7200)), datetime.datetime(2013, 7, 8, 0, 16, 37, tzinfo=tzoffset(None, 7200))], ['marin.dacos@revues.org', datetime.datetime(2010, 7, 5, 9, 45, 15, tzinfo=tzoffset(None, 3600)), datetime.datetime(2011, 7, 16, 14, 58, 47, tzinfo=tzoffset(None, 7200))]], 'Burghart Marjorie': [['marjorie.burghart@ehess.fr', datetime.datetime(2013, 7, 5, 1, 35, 31, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 9, 13, 52, 33, tzinfo=tzlocal())]], 'Enrico Natale': [['enrico.natale@infoclio.ch', datetime.datetime(2012, 7, 10, 14, 8, 20, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 8, 15, 8, 24, tzinfo=tzoffset(None, 7200))]], 'CAVALLO Delphine': [['delphine.cavallo@univ-amu.fr', datetime.datetime(2018, 3, 13, 10, 19, 25, tzinfo=tzutc()), datetime.datetime(2018, 3, 29, 10, 23, 54, tzinfo=tzutc())]], 'St\\xc3\\xa9phane Pouyllau': [['stephane.pouyllau@cnrs.fr', datetime.datetime(2013, 7, 5, 10, 28, 47, tzinfo=tzoffset(None, 7200)), datetime.datetime(2013, 7, 18, 11, 56, 40, tzinfo=tzoffset(None, 7200))], ['stephane.pouyllau@huma-num.fr', datetime.datetime(2013, 7, 5, 10, 46, 22, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 12, 14, 59, 27, tzinfo=tzoffset(None, 3600))]], 'Clarisse Bardiot': [['clarisse_bardiot@me.com', datetime.datetime(2015, 7, 9, 14, 46, 14, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 9, 14, 46, 14, tzinfo=tzoffset(None, 7200))], ['clarisse_bardiot@mac.com', datetime.datetime(2017, 6, 7, 13, 19, 2, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 10, 9, 19, 4, tzinfo=tzoffset(None, 7200))]], 'lamasse': [['stephane.lamasse@univ-paris1.fr', datetime.datetime(2018, 3, 9, 19, 46, 27, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 12, 19, 23, 47, tzinfo=tzoffset(None, 3600))]], 'Mareike Koenig': [['MKoenig@dhi-paris.fr', datetime.datetime(2013, 7, 5, 14, 36, 19, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 14, 14, 57, 17, tzinfo=tzoffset(None, 7200))]], 'Soudan Cecile': [['cecile.soudan@ehess.fr', datetime.datetime(2014, 7, 6, 15, 51, 49, tzinfo=tzlocal()), datetime.datetime(2014, 7, 6, 15, 51, 49, tzinfo=tzlocal())]], 'St\\xc3\\xa9phane Loret': [['stephane.loret@univ-nantes.fr', datetime.datetime(2015, 7, 10, 10, 55, 45, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 8, 15, 40, 3, tzinfo=tzoffset(None, 7200))]], 'Alexandre Monnin': [['aamonnz@gmail.com', datetime.datetime(2012, 7, 17, 7, 17, 9, tzinfo=tzoffset(None, 7200)), datetime.datetime(2014, 7, 10, 5, 14, 24, tzinfo=tzoffset(None, 7200))]], 'marc jahjah': [['jahjah.marc@gmail.com', datetime.datetime(2018, 3, 12, 14, 46, 23, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 12, 14, 46, 23, tzinfo=tzoffset(None, 3600))]], 'jf.omhover@histographe.com': [['jf.omhover@histographe.com', datetime.datetime(2012, 7, 24, 17, 45, 29, tzinfo=tzoffset(None, 7200)), datetime.datetime(2012, 7, 24, 17, 45, 29, tzinfo=tzoffset(None, 7200))]], 'aude.da-cruz-lima': [['aude.da-cruz-lima@mae.u-paris10.fr', datetime.datetime(2016, 7, 28, 9, 38, 17, tzinfo=tzoffset(None, 7200)), datetime.datetime(2016, 7, 28, 9, 38, 17, tzinfo=tzoffset(None, 7200))]], 'Sandrine Cl\\xc3\\xa9risse': [['sandrine.clerisse@cnrs.fr', datetime.datetime(2018, 3, 21, 10, 40, 36, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 21, 10, 40, 36, tzinfo=tzoffset(None, 3600))], ['sclerisse@parisnanterre.fr', datetime.datetime(2018, 3, 21, 10, 33, 6, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 21, 10, 33, 6, tzinfo=tzoffset(None, 3600))]], 'Formation continue ENC': [['formation.continue@enc-sorbonne.fr', datetime.datetime(2016, 7, 5, 15, 46, 59, tzinfo=tzoffset(None, 7200)), datetime.datetime(2016, 7, 5, 15, 46, 59, tzinfo=tzoffset(None, 7200))]], 'Jean-Baptiste CAMPS': [['jbcamps@hotmail.com', datetime.datetime(2017, 6, 2, 12, 43, 25, tzinfo=tzutc()), datetime.datetime(2017, 6, 2, 12, 43, 25, tzinfo=tzutc())]], 'Claire Lemercier': [['claire.lemercier@sciencespo.fr', datetime.datetime(2018, 3, 14, 10, 3, 2, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 14, 10, 3, 2, tzinfo=tzoffset(None, 3600))]], 'Emmanuelle Duwez': [['emmanuelle.duwez@sciencespo.fr', datetime.datetime(2015, 7, 10, 18, 33, 17, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 10, 18, 33, 17, tzinfo=tzoffset(None, 7200))]], 'BRUNET Mich\\xc3\\xa8le': [['iglouvremb@gmail.com', datetime.datetime(2014, 7, 17, 14, 17, 4, tzinfo=tzoffset(None, 7200)), datetime.datetime(2014, 7, 17, 14, 17, 4, tzinfo=tzoffset(None, 7200))]], 'Laube Sylvain': [['sylvain.laube@univ-brest.fr', datetime.datetime(2014, 7, 25, 22, 53, 36, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 21, 16, 16, 6, tzinfo=tzoffset(None, 3600))]], 'Vanessa Juloux': [['vanessa.juloux@ephe.sorbonne.fr', datetime.datetime(2017, 6, 1, 18, 59, 10, tzinfo=tzutc()), datetime.datetime(2017, 6, 1, 18, 59, 10, tzinfo=tzutc())]], 'Ghislain SILLAUME': [['ghislain.sillaume@cvce.eu', datetime.datetime(2013, 7, 18, 17, 36, 42, tzinfo=tzoffset(None, 7200)), datetime.datetime(2013, 7, 18, 17, 36, 42, tzinfo=tzoffset(None, 7200))]], 'Olivier secardin': [['secardinolivier@yahoo.fr', datetime.datetime(2017, 6, 18, 21, 48, 44, tzinfo=tzutc()), datetime.datetime(2017, 6, 18, 21, 48, 44, tzinfo=tzutc())]], 'Nicolas Legrand': [['nicolas.legrand@obspm.fr', datetime.datetime(2013, 7, 5, 14, 59, 58, tzinfo=tzoffset(None, 7200)), datetime.datetime(2013, 7, 15, 10, 46, 43, tzinfo=tzoffset(None, 7200))]], 'Michael Sinatra': [['michaelesinatra@gmail.com', datetime.datetime(2014, 7, 9, 4, 1, 53, tzinfo=tzoffset(None, -14400)), datetime.datetime(2014, 7, 9, 4, 1, 53, tzinfo=tzoffset(None, -14400))]], 'Paul Girard': [['paul.girard@sciencespo.fr', datetime.datetime(2013, 7, 15, 9, 34, 39, tzinfo=tzoffset(None, 7200)), datetime.datetime(2013, 7, 15, 9, 34, 39, tzinfo=tzoffset(None, 7200))]], 'Francesco Beretta': [['francesco.beretta@ish-lyon.cnrs.fr', datetime.datetime(2018, 3, 12, 9, 52, 19, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 12, 10, 55, 52, tzinfo=tzoffset(None, 3600))]], 'Adeline Joffres': [['adeline.joffres@huma-num.fr', datetime.datetime(2018, 3, 26, 11, 36, 23, tzinfo=tzlocal()), datetime.datetime(2018, 3, 26, 12, 4, 54, tzinfo=tzlocal())]], 'dbernhard@unistra.fr': [['dbernhard@unistra.fr', datetime.datetime(2018, 3, 26, 12, 32, 35, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 26, 12, 32, 35, tzinfo=tzoffset(None, 7200))]], 'Muriel Foulonneau': [['muriel.foulonneau@gmail.com', datetime.datetime(2010, 7, 16, 10, 57, 19, tzinfo=tzoffset(None, 7200)), datetime.datetime(2010, 7, 16, 10, 57, 19, tzinfo=tzoffset(None, 7200))]], 'marta materni': [['marta.materni@gmail.com', datetime.datetime(2017, 6, 9, 10, 25, 58, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 9, 10, 25, 58, tzinfo=tzoffset(None, 7200))]], 'marie-noelle.polino@ahicf.com': [['marie-noelle.polino@ahicf.com', datetime.datetime(2015, 7, 2, 14, 10, 25, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 2, 14, 10, 25, tzinfo=tzoffset(None, 7200))]], 'antoine Courtin': [['antoine.courtin@mac.com', datetime.datetime(2013, 7, 12, 20, 19, 16, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 9, 22, 38, 49, tzinfo=tzoffset(None, 7200))]], 'RAPHAELLE BRANGIER': [['raphaelle.krummeich@univ-rouen.fr', datetime.datetime(2018, 3, 19, 18, 2, 18, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 19, 18, 2, 18, tzinfo=tzoffset(None, 3600))]], 'marionlame@gmail.com': [['marionlame@gmail.com', datetime.datetime(2011, 7, 18, 11, 1, tzinfo=tzoffset(None, 7200)), datetime.datetime(2011, 7, 18, 11, 1, tzinfo=tzoffset(None, 7200))]], 'arno.zucker@gmail.com': [['arno.zucker@gmail.com', datetime.datetime(2015, 7, 5, 9, 38, 43, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 5, 9, 38, 43, tzinfo=tzoffset(None, 7200))]], 'Daniel Stoekl': [['Daniel.Stoekl@ephe.sorbonne.fr', datetime.datetime(2017, 6, 16, 6, 45, 25, tzinfo=tzutc()), datetime.datetime(2017, 6, 16, 6, 45, 25, tzinfo=tzutc())]], 'Amel Fraisse': [['amel.fraisse@univ-lille3.fr', datetime.datetime(2018, 3, 6, 14, 57, 59, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 6, 14, 57, 59, tzinfo=tzoffset(None, 3600))]], 'Mathieu Andro': [['Mathieu.Andro@versailles.inra.fr', datetime.datetime(2015, 7, 9, 12, 5, 28, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 9, 12, 5, 28, tzinfo=tzoffset(None, 7200))]], 'cl\\xc3\\xa9mence jacquot': [['clemence.jacquot@gmail.com', datetime.datetime(2018, 3, 12, 15, 15, 16, tzinfo=tzoffset(None, 3600)), datetime.datetime(2018, 3, 12, 15, 15, 16, tzinfo=tzoffset(None, 3600))]], 'Anne-Laure Brisac-Chra\\xc3\\xafbi': [['anne-laure.brisac@inha.fr', ' Fri, 13 Jul 2012 09:56:18 +0100', ' Fri, 13 Jul 2012 09:56:18 +0100']], 'dumouchel suzanne': [['dumouchelsuzanne@yahoo.fr', datetime.datetime(2017, 6, 16, 7, 39, 48, tzinfo=tzutc()), datetime.datetime(2018, 3, 29, 6, 51, 33, tzinfo=tzutc())]], 'Annaig Mahe': [['mahe.annaig@wanadoo.fr', datetime.datetime(2013, 7, 26, 15, 37, 16, tzinfo=tzoffset(None, 7200)), datetime.datetime(2013, 7, 26, 15, 37, 16, tzinfo=tzoffset(None, 7200))]], 'Florent Laroche': [['florent.laroche@ec-nantes.fr', datetime.datetime(2017, 6, 26, 19, 5, 41, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 26, 19, 5, 41, tzinfo=tzoffset(None, 7200))]], 'Anne Baillot': [['anne.baillot@gmail.com', datetime.datetime(2015, 7, 26, 16, 15, 52, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 20, 13, 59, 42, tzinfo=tzoffset(None, 3600))]], 'Fran\\xc3\\xa7ois Th\\xc3\\xa9ron': [['francois.theron@uvsq.fr', datetime.datetime(2017, 6, 2, 12, 36, 14, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 2, 12, 36, 14, tzinfo=tzoffset(None, 7200))]], 'Cadiou Colette': [['colette.cadiou@irstea.fr', datetime.datetime(2016, 7, 20, 11, 48, 26, tzinfo=tzlocal()), datetime.datetime(2016, 7, 20, 11, 48, 26, tzinfo=tzlocal())]], 'Th\\xc3\\xa9ly Nicolas': [['nicolas.thely@univ-rennes2.fr', datetime.datetime(2013, 7, 2, 14, 15, 54, tzinfo=tzoffset(None, 7200)), datetime.datetime(2013, 7, 2, 14, 15, 54, tzinfo=tzoffset(None, 7200))]], 'Rails &amp; Histoire': [['listes@ahicf.com', datetime.datetime(2013, 7, 18, 10, 20, 44, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 28, 16, 43, 54, tzinfo=tzoffset(None, 7200))]], 'Sandrine Breuil': [['sandrine.breuil@univ-tours.fr', datetime.datetime(2013, 7, 9, 8, 45, 6, tzinfo=tzoffset(u'BST', 3600)), datetime.datetime(2013, 7, 9, 8, 45, 6, tzinfo=tzoffset(u'BST', 3600))]], 'Sylvain Laurens': [['laurens@ehess.fr', datetime.datetime(2015, 7, 9, 11, 45, 25, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 9, 11, 45, 25, tzinfo=tzoffset(None, 7200))]], 'Maud Ingarao': [['maud.ingarao@ens-lyon.fr', datetime.datetime(2011, 7, 7, 15, 59, 17, tzinfo=tzoffset(None, 7200)), datetime.datetime(2011, 7, 7, 15, 59, 17, tzinfo=tzoffset(None, 7200))]], 'Pablo Ruiz': [['pabloruizfabo@gmail.com', datetime.datetime(2017, 6, 14, 10, 17, 22, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 14, 10, 17, 22, tzinfo=tzoffset(None, 7200))]], 'Val\\xc3\\xa9rie Beaugiraud': [['valerie.beaugiraud@ens-lyon.fr', datetime.datetime(2015, 7, 9, 11, 42, 23, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 9, 11, 42, 23, tzinfo=tzoffset(None, 7200))]], 'Marie-Eglantine Lescasse': [['marie.e.lescasse@gmail.com', datetime.datetime(2018, 3, 28, 17, 9, 57, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 28, 17, 9, 57, tzinfo=tzoffset(None, 7200))]], 'Marie-laure Massot': [['marie-laure.massot@ens.fr', datetime.datetime(2017, 6, 26, 16, 30, 12, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 15, 14, 40, 57, tzinfo=tzoffset(None, 3600))]], 'Nadine Dardenne': [['nadine.dardenne@cnrs.fr', datetime.datetime(2017, 6, 19, 17, 21, 6, tzinfo=tzoffset(None, 7200)), datetime.datetime(2017, 6, 19, 17, 21, 6, tzinfo=tzoffset(None, 7200))]], 'Pierre-Edouard Barrault': [['pe.barrault@gmail.com', datetime.datetime(2015, 7, 10, 19, 43, 36, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 10, 19, 43, 36, tzinfo=tzoffset(None, 7200))]], 'Aur\\xc3\\xa9lien Berra': [['aurelien.berra@gmail.com', datetime.datetime(2013, 7, 5, 14, 14, 20, tzinfo=tzoffset(None, 7200)), datetime.datetime(2018, 3, 23, 9, 48, 3, tzinfo=tzoffset(None, 3600))]], 'CASANOVA Nathalie': [['ncasanova@mmsh.univ-aix.fr', datetime.datetime(2016, 7, 22, 14, 58, 33, tzinfo=tzutc()), datetime.datetime(2016, 7, 22, 14, 58, 33, tzinfo=tzutc())]], 'Michael Bourgatte': [['m.bourgatte@icp.fr', datetime.datetime(2016, 7, 19, 14, 37, 11, tzinfo=tzoffset(None, 7200)), datetime.datetime(2016, 7, 19, 14, 37, 11, tzinfo=tzoffset(None, 7200))]], 'Epler, Jakob': [['jakob.epler@dariah.eu', datetime.datetime(2018, 3, 14, 14, 36, 16, tzinfo=tzutc()), datetime.datetime(2018, 3, 14, 14, 36, 16, tzinfo=tzutc())]], 'Ren\\xc3\\xa9 Audet': [['Rene.Audet@lit.ulaval.ca', datetime.datetime(2018, 3, 21, 14, 11, 14, tzinfo=tzutc()), datetime.datetime(2018, 3, 21, 14, 11, 14, tzinfo=tzutc())]], 'Fr\\xc3\\xa9d\\xc3\\xa9ric CLAVERT': [['frederic.clavert@uni.lu', datetime.datetime(2018, 3, 12, 11, 53, 38, tzinfo=tzutc()), datetime.datetime(2018, 3, 27, 11, 23, 8, tzinfo=tzutc())]], 'Emmanuelle Morlock': [['emmanuelle.morlock@gmail.com', datetime.datetime(2015, 7, 11, 19, 24, 29, tzinfo=tzoffset(None, 7200)), datetime.datetime(2015, 7, 11, 19, 24, 29, tzinfo=tzoffset(None, 7200))]], 'inatheque@ina.fr': [['inatheque@ina.fr', datetime.datetime(2018, 3, 15, 17, 28, 55), datetime.datetime(2018, 3, 15, 17, 28, 55)]]}\n"
     ]
    }
   ],
   "source": [
    "def read_corpus_and_extract_names_2(corpus,liste_noms_and_dups):\n",
    "    liste_noms=liste_noms_and_dups[0]\n",
    "    dups=liste_noms_and_dups[1]\n",
    "    dict_noms={}\n",
    "    tmp_dups_dict={}\n",
    "    tmp_dups_list=[]\n",
    "    for dup in dups:\n",
    "        dupA=dup[0]\n",
    "        dupAnom=dup[0][0]\n",
    "        dupAmail=dup[0][1]\n",
    "        dupAdatemin=None\n",
    "        dupAdatemax=None\n",
    "        for mesg in corpus:\n",
    "            #TEST MIN DATE\n",
    "            if corpus[mesg]['mail_auteur']==dupAmail:\n",
    "                if dupAdatemin is not None:\n",
    "                    if corpus[mesg]['date']<dupAdatemin:\n",
    "                        #print corpus[mesg]['date']\n",
    "                        dupAdatemin=corpus[mesg]['date']\n",
    "                else: dupAdatemin=corpus[mesg]['date']\n",
    "             #TEST MAX DATE       \n",
    "            if corpus[mesg]['mail_auteur']==dupAmail:\n",
    "                if dupAdatemax is not None:\n",
    "                    if corpus[mesg]['date']>dupAdatemax:\n",
    "                        #print corpus[mesg]['date']\n",
    "                        dupAdatemax=corpus[mesg]['date']\n",
    "                else: dupAdatemax=corpus[mesg]['date']   \n",
    "        \n",
    "        \n",
    "        dupB=dup[1]\n",
    "        dupBnom=dup[1][0]\n",
    "        dupBmail=dup[1][1]\n",
    "        dupBdatemin=None\n",
    "        dupBdatemax=None\n",
    "        \n",
    "        #TO TEST LATER WITH LIST_OF_NAMES_IN_ORDER_NOT_TO_ADD_DUPS_MULTIPLE_TIMES\n",
    "        if dupAnom not in tmp_dups_list:\n",
    "            tmp_dups_list.append(dupAnom)\n",
    "        if dupBnom not in tmp_dups_list:\n",
    "            tmp_dups_list.append(dupBnom)\n",
    "        for mesg in corpus:\n",
    "            #TEST MIN DATE\n",
    "            if corpus[mesg]['mail_auteur']==dupBmail:\n",
    "                if dupBdatemin is not None:\n",
    "                    if corpus[mesg]['date']<dupBdatemin:\n",
    "                        #print corpus[mesg]['date']\n",
    "                        dupBdatemin=corpus[mesg]['date']\n",
    "                else: dupBdatemin=corpus[mesg]['date']\n",
    "             #TEST MAX DATE       \n",
    "            if corpus[mesg]['mail_auteur']==dupBmail:\n",
    "                if dupBdatemax is not None:\n",
    "                    if corpus[mesg]['date']>dupBdatemax:\n",
    "                        #print corpus[mesg]['date']\n",
    "                        dupBdatemax=corpus[mesg]['date']\n",
    "                else: dupBdatemax=corpus[mesg]['date'] \n",
    "        #print dupAmail,\" \",dupAdatemin,\"/\",dupAdatemax,\"\\n\",dupBmail,\" \",dupBdatemin,\"/\",dupBdatemax,\"\\n\"\n",
    "        if bool(tmp_dups_dict)== False:\n",
    "            tmp_dups_dict[dupAnom]=[[dupAmail,dupAdatemin,dupAdatemax],[dupBmail,dupBdatemin,dupBdatemax]]\n",
    "\n",
    "            init=1\n",
    "            \n",
    "        if dupAnom not in tmp_dups_dict and init !=1:\n",
    "            #CHECK PRINT\n",
    "            #print dupAnom,\"NOT IN\\n\",tmp_dups_dict\n",
    "            \n",
    "            tmp_dups_dict_tmp= dict.copy(tmp_dups_dict)\n",
    "            for key in tmp_dups_dict_tmp:\n",
    "                #print key\n",
    "                \n",
    "                if SequenceMatcher(None, dupAnom, key).ratio() <0.9:\n",
    "                    tmp_dups_dict[dupAnom]=[[dupAmail,dupAdatemin,dupAdatemax],[dupBmail,dupBdatemin,dupBdatemax]]\n",
    "                else: \n",
    "                    #print \"HEERE\"\n",
    "                    tmp_dups_dict[key].append([[dupAmail,dupAdatemin,dupAdatemax],[dupBmail,dupBdatemin,dupBdatemax]])\n",
    "        else:\n",
    "            \n",
    "            if init !=1:\n",
    "                #print \"HERE\"\n",
    "                tmp_dups_dict[dupAnom].append([[dupAmail,dupAdatemin,dupAdatemax],[dupBmail,dupBdatemin,dupBdatemax]])\n",
    "        init=0\n",
    "    \n",
    "    #print     tmp_dups_dict\n",
    "    tmp_nom_dict={}\n",
    "    \n",
    "    for nom in liste_noms:\n",
    "        \n",
    "        if nom[0] not in tmp_dups_list:\n",
    "            if nom[0] in tmp_nom_dict:\n",
    "                print \"BUG NOMS\"\n",
    "                break\n",
    "            else:\n",
    "                nomdatemin=None\n",
    "                nomdatemax=None\n",
    "                for mesg in corpus:\n",
    "            #TEST MIN DATE\n",
    "                    if corpus[mesg]['mail_auteur']==nom[1]:\n",
    "                        if nomdatemin is not None:\n",
    "                            if corpus[mesg]['date']<nomdatemin:\n",
    "                                #print corpus[mesg]['date']\n",
    "                                nomdatemin=corpus[mesg]['date']\n",
    "                        else: nomdatemin=corpus[mesg]['date']\n",
    "                     #TEST MAX DATE       \n",
    "                    if corpus[mesg]['mail_auteur']==nom[1]:\n",
    "                        if nomdatemax is not None:\n",
    "                            if corpus[mesg]['date']>nomdatemax:\n",
    "                                #print corpus[mesg]['date']\n",
    "                                nomdatemax=corpus[mesg]['date']\n",
    "                        else: nomdatemax=corpus[mesg]['date']\n",
    "                \n",
    "                tmp_nom_dict[nom[0]]=[[nom[1],nomdatemin,nomdatemax]]\n",
    "                #print nom[0],\" \",nom[1],\" \",nomdatemin,\"/\",nomdatemax,\"\\n\"\n",
    "                if bool(tmp_nom_dict)== False:\n",
    "                    tmp_nom_dict[nom[0]]=[[nom[1],nomdatemin,nomdatemax]]\n",
    "\n",
    "    #print  tmp_dups_dict,\"\\n_________________\\n\",tmp_nom_dict,'\\n____________________________\\n'    \n",
    "    dict_noms_parses_dedoub_avec_date= tmp_dups_dict.copy()\n",
    "    dict_noms_parses_dedoub_avec_date.update(tmp_nom_dict)\n",
    "    print dict_noms_parses_dedoub_avec_date\n",
    "    return dict_noms_parses_dedoub_avec_date\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "        \n",
    "    \n",
    "dict_noms_mails_debut_fin=read_corpus_and_extract_names_2(corpus_mails_DH,liste_noms_and_dups)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['openedition.org', 'ehess.fr', 'ens.fr', 'adbu.fr', 'irht.cnrs.fr', 'unil.ch', 'huma-num.fr', 'univ-amu.fr', 'gmail.com', 'univ-rouen.fr', 'sciencespo-lyon.fr', 'cnrs.fr', 'univ-paris3.fr', 'hotmail.com', 'sciencespo.fr', 'helsinki.fi', 'aalto.fi', 'EUI.eu', 'ulb.ac.be', 'univ-grenoble-alpes.fr', 'univ-paris1.fr', 'univ-reims.fr', 'club-internet.fr', 'inria.fr', 'u-paris10.fr', 'univ-tours.fr', 'cnam.fr', 'meshs.fr', 'wanadoo.fr', 'insa-lyon.fr', 'univ-lyon3.fr', 'design.ulaval.ca', 'oeaw.ac.at', 'googlemail.com', 'lecnam.net', 'sib.swiss', 'yahoo.es', 'univ-lyon2.fr', 'psl.eu', 'yahoo.fr', 'univ-orleans.fr', 'free.fr', 'univ-lille2.fr', 'mines.org', 'ens-lyon.fr', 'enssib.fr', 'irit.fr', 'upmc.fr', 'ahicf.com', 'bnf.fr', 'campus-condorcet.fr', 'clavert.net', 'univ-avignon.fr', 'cmb.hu-berlin.de', 'univ-lille3.fr', 'unilim.fr', 'mediev.unipi.it', 'univ-lorraine.fr', 'revues.org', 'infoclio.ch', 'me.com', 'mac.com', 'dhi-paris.fr', 'univ-nantes.fr', 'histographe.com', 'mae.u-paris10.fr', 'parisnanterre.fr', 'enc-sorbonne.fr', 'univ-brest.fr', 'ephe.sorbonne.fr', 'cvce.eu', 'obspm.fr', 'ish-lyon.cnrs.fr', 'unistra.fr', 'versailles.inra.fr', 'inha.fr', 'ec-nantes.fr', 'uvsq.fr', 'irstea.fr', 'univ-rennes2.fr', 'mmsh.univ-aix.fr', 'icp.fr', 'dariah.eu', 'lit.ulaval.ca', 'uni.lu', 'ina.fr']\n"
     ]
    }
   ],
   "source": [
    "def extract_locations_from_email_adresses(dict_noms):\n",
    " ###ON EXTRAIT LES DOMAINES_MAILS_ET_ON_VA_RECHERCHER_UNE_LOCALISATION_A_PARTIR_DE_CELA\n",
    "    liste_domaines=[]\n",
    "    for key in  dict_noms:\n",
    "        #print key\n",
    "        #print dict_noms[key]\n",
    "        for array in dict_noms[key]:\n",
    "            #print str(array[0].split(\"@\")[1:])\n",
    "            if str(array[0].split(\"@\")[1:][0]) not in liste_domaines:\n",
    "                liste_domaines.append(array[0].split(\"@\")[1:][0])\n",
    "    print liste_domaines\n",
    "    \n",
    "        \n",
    "    \n",
    "        \n",
    "list_localisations=extract_locations_from_email_adresses(dict_noms_mails_debut_fin)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_mails_DH=read_html_pages_in_dirs_and_extract_mails_dict(\"./DHFRsample\")\n",
    "print corpus_mails_DH\n",
    "\n",
    "\n",
    "\n",
    "liste_mails=read_corpus_and_extract_mails(corpus_mails_DH)\n",
    "print len(liste_mails)\n",
    "print liste_mails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html_pages_in_dirs_and_testtextextract(pages_dir):\n",
    "    dict_mails={}\n",
    "    #rootdir=\"./DHFR_sample/\"\n",
    "\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(pages_dir):\n",
    "#ON VA LIRE TOUS LES FICHIERS DU REP ROOTDIR\n",
    "        \n",
    "        for file in files:\n",
    "            dictT={}\n",
    "    #V1     if file.endswith('.html') == True  and file != (\"index.html\") and not re.match(\"mail\\d+.html\",file) : \n",
    "            if re.match(\"msg\\d+.html\",file):\n",
    "                filename=os.path.join(subdir, file)\n",
    "                with open(filename, 'r') as myfile:           \n",
    "                \n",
    "                    data=myfile.read()\n",
    "                    result = re.search('<!--X-Body-of-Message-->(.*)<!--X-Body-of-Message-End-->', data, flags=re.DOTALL)\n",
    "                    ZC= result.group(1)\n",
    "                    ZC=cleanhtml(ZC)\n",
    "                    dictT[\"content\"]=ZC\n",
    "                    dictT[\"geo\"]=topic_analysis_with_manual_detection(ZC)\n",
    "                dict_mails[filename]= dictT\n",
    "            \n",
    "    return dict_mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html_and_testtextextract(filename):\n",
    "                dictT={}\n",
    "                with open(filename, 'r') as myfile:\n",
    "                    data=myfile.read()\n",
    "                    result = re.search('<!--X-Body-of-Message-->(.*)<!--X-Body-of-Message-End-->', data, flags=re.DOTALL)\n",
    "                    ZC= result.group(1)\n",
    "                    ZC=cleanhtml(ZC)\n",
    "                    dictT[\"content\"]=ZC\n",
    "                    dictT[\"geo\"]=topic_analysis_with_manual_detection(ZC)\n",
    "                return dictT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_txt=read_html_pages_in_dirs_and_testtextextract(\"./DHFRsample\")\n",
    "#print corpus_txt\n",
    "#lng= language_detection_with_pyenchant(str(corpus_txt))\n",
    "#print lng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "print enchant.list_languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyse =topic_analysis_with_nltk_gensim(corpus_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Analyse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_analysis_with_nltk_gensim(string_to_read):\n",
    "    string_to_read=unicode(str(string_to_read), 'utf8')\n",
    "    lng = language_detection_with_pyenchant(string_to_read)\n",
    "   \n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    import string\n",
    "    print lng\n",
    "    stop = set(stopwords.words(lng))\n",
    "    exclude = set(string.punctuation)\n",
    "    lemma = WordNetLemmatizer()\n",
    "    def clean(doc):\n",
    "        stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "        \n",
    "        punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "        return normalized\n",
    "\n",
    "    doc_clean = clean(string_to_read).split()\n",
    "    print \"DOC CLEAN\",doc_clean\n",
    "        # Importing Gensim\n",
    "    import gensim\n",
    "    from gensim import corpora\n",
    "\n",
    "        # Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "    dictionary = corpora.Dictionary([doc_clean])\n",
    "\n",
    "        # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = dictionary.doc2bow(doc_clean)\n",
    "    Lda = gensim.models.ldamodel.LdaModel\n",
    "    print Lda\n",
    "\n",
    "        # Running and Trainign LDA model on the document term matrix.\n",
    "    ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=5)\n",
    "    return ldamodel\n",
    "       \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    #return [topic_words,doctopic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
