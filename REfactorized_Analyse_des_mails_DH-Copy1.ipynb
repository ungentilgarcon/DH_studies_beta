{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some imports\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser\n",
    "from difflib import SequenceMatcher\n",
    "import re,os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FOR TOPIC MODELLING\n",
    "#https://de.dariah.eu/tatom/topic_model_python.html\n",
    "import numpy as np  # a conventional alias\n",
    "\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "  cleanr = re.compile('<.*?>')\n",
    "  cleantext = re.sub(cleanr, '', raw_html)\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def language_detection_with_pyenchant(string_to_read):\n",
    "    #https://stackoverflow.com/questions/3788870/how-to-check-if-a-word-is-an-english-word-with-python\n",
    "    import enchant\n",
    "    lg_ang=0\n",
    "    us = enchant.Dict(\"en_US\")\n",
    "    #print \"US LOADED\"\n",
    "    fr = enchant.Dict(\"fr_FR\")\n",
    "    #print \"FR LOADED\"\n",
    "    lg_fr=0\n",
    "    lg_ang=0\n",
    "    #print \"string_to_read\",string_to_read\n",
    "    for word in string_to_read.split():\n",
    "        #print fr.check(word)\n",
    "        #print word\n",
    "        if fr.check(word) == True:\n",
    "            lg_fr+=1\n",
    "        if us.check(word) == True:\n",
    "            lg_ang+=1\n",
    "    #print \"THERE I AM\"\n",
    "    if lg_fr >= lg_ang :\n",
    "        return \"french\"\n",
    "    else:\n",
    "        if lg_ang > lg_fr :\n",
    "        \n",
    "            return \"english\"\n",
    "        else: \n",
    "            return \"NEITHER ENGLISH NOR FRENCH\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_analysis_with_mallet(string_to_read):\n",
    "    lng = language_detection_with_pyenchant(string_to_read)\n",
    "    if lng== \"french\" :\n",
    "        lng= [\"a\",\"à\",\"â\",\"abord\",\"afin\",\"ah\",\"ai\",\"aie\",\"ainsi\",\"allaient\",\"allo\",\"allô\",\"allons\",\"après\",\"assez\",\"attendu\",\"au\",\"aucun\",\"aucune\",\"aujourd\",\"aujourd'hui\",\"auquel\",\"aura\",\"auront\",\"aussi\",\"autre\",\"autres\",\"aux\",\"auxquelles\",\"auxquels\",\"avaient\",\"avais\",\"avait\",\"avant\",\"avec\",\"avoir\",\"ayant\",\"b\",\"bah\",\"beaucoup\",\"bien\",\"bigre\",\"boum\",\"bravo\",\"brrr\",\"c\",\"ça\",\"car\",\"ce\",\"ceci\",\"cela\",\"celle\",\"celle-ci\",\"celle-là\",\"celles\",\"celles-ci\",\"celles-là\",\"celui\",\"celui-ci\",\"celui-là\",\"cent\",\"cependant\",\"certain\",\"certaine\",\"certaines\",\"certains\",\"certes\",\"ces\",\"cet\",\"cette\",\"ceux\",\"ceux-ci\",\"ceuxlà\",\"chacun\",\"chaque\",\"cher\",\"chère\",\"chères\",\"chers\",\"chez\",\"chiche\",\"chut\",\"ci\",\"cinq\",\"cinquantaine\",\"cinquante\",\"cinquantième\",\"cinquième\",\"clac\",\"clic\",\"combien\",\"comme\",\"comment\",\"compris\",\"concernant\",\"contre\",\"couic\",\"crac\",\"d\",\"da\",\"dans\",\"de\",\"debout\",\"dedans\",\"dehors\",\"delà\",\"depuis\",\"derrière\",\"des\",\"dès\",\"désormais\",\"desquelles\",\"desquels\",\"dessous\",\"dessus\",\"deux\",\"deuxième\",\"deuxièmement\",\"devant\",\"devers\",\"devra\",\"différent\",\"différente\",\"différentes\",\"différents\",\"dire\",\"divers\",\"diverse\",\"diverses\",\"dix\",\"dix-huit\",\"dixième\",\"dix-neuf\",\"dixsept\",\"doit\",\"doivent\",\"donc\",\"dont\",\"douze\",\"douzième\",\"dring\",\"du\",\"duquel\",\"durant\",\"e\",\"effet\",\"eh\",\"elle\",\"elle-même\",\"elles\",\"ellesmêmes\",\"en\",\"encore\",\"entre\",\"envers\",\"environ\",\"es\",\"ès\",\"est\",\"et\",\"etant\",\"étaient\",\"étais\",\"était\",\"étant\",\"etc\",\"été\",\"etre\",\"être\",\"eu\",\"euh\",\"eux\",\"eux-mêmes\",\"excepté\",\"f\",\"façon\",\"fais\",\"faisaient\",\"faisant\",\"fait\",\"feront\",\"fi\",\"flac\",\"floc\",\"font\",\"g\",\"gens\",\"h\",\"ha\",\"hé\",\"hein\",\"hélas\",\"hem\",\"hep\",\"hi\",\"ho\",\"holà\",\"hop\",\"hormis\",\"hors\",\"hou\",\"houp\",\"hue\",\"hui\",\"huit\",\"huitième\",\"hum\",\"hurrah\",\"i\",\"il\",\"ils\",\"importe\",\"j\",\"je\",\"jusqu\",\"jusque\",\"k\",\"l\",\"la\",\"là\",\"laquelle\",\"las\",\"le\",\"lequel\",\"les\",\"lès\",\"lesquelles\",\"lesquels\",\"leur\",\"leurs\",\"longtemps\",\"lorsque\",\"lui\",\"lui-même\",\"m\",\"ma\",\"maint\",\"mais\",\"malgré\",\"me\",\"même\",\"mêmes\",\"merci\",\"mes\",\"mien\",\"mienne\",\"miennes\",\"miens\",\"mille\",\"mince\",\"moi\",\"moi-même\",\"moins\",\"mon\",\"moyennant\",\"n\",\"na\",\"ne\",\"néanmoins\",\"neuf\",\"neuvième\",\"ni\",\"nombreuses\",\"nombreux\",\"non\",\"nos\",\"notre\",\"nôtre\",\"nôtres\",\"nous\",\"nous-mêmes\",\"nul\",\"o\",\"o|\",\"ô\",\"oh\",\"ohé\",\"olé\",\"ollé\",\"on\",\"ont\",\"onze\",\"onzième\",\"ore\",\"ou\",\"où\",\"ouf\",\"ouias\",\"oust\",\"ouste\",\"outre\",\"p\",\"paf\",\"pan\",\"par\",\"parmi\",\"partant\",\"particulier\",\"particulière\",\"particulièrement\",\"pas\",\"passé\",\"pendant\",\"personne\",\"peu\",\"peut\",\"peuvent\",\"peux\",\"pff\",\"pfft\",\"pfut\",\"pif\",\"plein\",\"plouf\",\"plus\",\"plusieurs\",\"plutôt\",\"pouah\",\"pour\",\"pourquoi\",\"premier\",\"première\",\"premièrement\",\"près\",\"proche\",\"psitt\",\"puisque\",\"q\",\"qu\",\"quand\",\"quant\",\"quanta\",\"quant-à-soi\",\"quarante\",\"quatorze\",\"quatre\",\"quatre- vingt\",\"quatrième\",\"quatrièmement\",\"que\",\"quel\",\"quelconque\",\"quelle\",\"quelles\",\"quelque\",\"quelques\",\"quelqu'un\",\"quels\",\"qui\",\"quiconque\",\"quinze\",\"quoi\",\"quoique\",\"r\",\"revoici\",\"revoilà\",\"rien\",\"s\",\"sa\",\"sacrebleu\",\"sans\",\"sapristi\",\"sauf\",\"se\",\"seize\",\"selon\",\"sept\",\"septième\",\"sera\",\"seront\",\"ses\",\"si\",\"sien\",\"sienne\",\"siennes\",\"siens\",\"sinon\",\"six\",\"sixième\",\"soi\",\"soi-même\",\"soit\",\"soixante\",\"son\",\"sont\",\"sous\",\"stop\",\"suis\",\"suivant\",\"sur\",\"surtout\",\"t\",\"ta\",\"tac\",\"tant\",\"te\",\"té\",\"tel\",\"telle\",\"tellement\",\"telles\",\"tels\",\"tenant\",\"tes\",\"tic\",\"tien\",\"tienne\",\"tiennes\",\"tiens\",\"toc\",\"toi\",\"toi-même\",\"ton\",\"touchant\",\"toujours\",\"tous\",\"tout\",\"toute\",\"toutes\",\"treize\",\"trente\",\"très\",\"trois\",\"troisième\",\"troisièmement\",\"trop\",\"tsoin\",\"tsouin\",\"tu\",\"u\",\"un\",\"une\",\"unes\",\"uns\",\"v\",\"va\",\"vais\",\"vas\",\"vé\",\"vers\",\"via\",\"vif\",\"vifs\",\"vingt\",\"vivat\",\"vive\",\"vives\",\"vlan\",\"voici\",\"voilà\",\"vont\",\"vos\",\"votre\",\"vôtre\",\"vôtres\",\"vous\",\"vous-mêmes\",\"vu\",\"w\",\"x\",\"y\",\"z\",\"zut\"]\n",
    "    \n",
    "    vectorizer = text.CountVectorizer(input=string_to_read, stop_words=lng, min_df=3)\n",
    "    dtm = vectorizer.fit_transform(string_to_read.split()).toarray()\n",
    "    vocab = np.array(vectorizer.get_feature_names())\n",
    "    dtm.shape\n",
    "    len(vocab)\n",
    "    num_topics = 20\n",
    "    num_top_words = 20\n",
    "    clf = decomposition.NMF(n_components=num_topics, random_state=1)\n",
    "    doctopic = clf.fit_transform(dtm)\n",
    "    topic_words = []\n",
    "    for topic in clf.components_:\n",
    "        word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "        topic_words.append([vocab[i] for i in word_idx])\n",
    "    doctopic = doctopic / np.sum(doctopic, axis=1, keepdims=True)\n",
    "    return [topic_words,doctopic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#so far french support\n",
    "def find_geo_topics(string_to_read, lng):\n",
    "    geo=0\n",
    "    geo_words=[]\n",
    "    #print string_to_read\n",
    "    if lng== \"french\":\n",
    "        for word in (string_to_read.split()):\n",
    "        #print word\n",
    "        #LUSSAULT ;) https://www.espacestemps.net/articles/entrees-par-categories/\n",
    "            # if (word in [\"Théorie de l’espace\",\"Accessibilité\",\"Acteur spatial\",\"Action spatiale\",\"Agencement\",\"Agglomération\",\"Ailleurs\",\"Aire\",\"Aire culturelle\",\"Alignement\",\"Ambiance architecturale et urbaine\",\"Aménagement du territoire\",\"Anamorphose\",\"Anthropisation\",\"Archipel mégalopolitain mondial \",\"Armature urbaine\",\"Attraction\",\"Autocorrélation spatiale\",\"Banlieue\",\"Campagne\",\"Capital spatial\",\"Carte\",\"Carte mentale\",\"Centralité\",\"Centre/Périphérie\",\"Centre urbain\",\"Chorème\",\"Chorotype\",\"Circulation\",\"Citadinité\",\"Communication territoriale\",\"Commutateur\",\"Compromis territorial\",\"Concentration\",\"Configuration spatiale\",\"Confins\",\"Connexité\",\"Contact\",\"Contiguïté\",\"Continent\",\"Continuité\",\"Coprésence\",\"Corps\",\"Cospatialité\",\"Cyberespace\",\"Décentralisation\",\"Découpage\",\"Découverte\",\"Défrichement\",\"Densité\",\"Désert\",\"Déterritorialisation\",\"Développement local\",\"Diaspora\",\"Différenciation spatiale -Diffusion\",\"Discontinuité\",\"Dispositif spatial légitime\",\"Distance\",\"Distribution rang/taille\",\"Distribution spatiale\",\"District industriel\",\"Diversité\",\"Dynamique spatiale\",\"Écart\",\"Échelle\",\"Économie-monde\",\"Écoumène\",\"Edge City\",\"Emblème territorial\",\"Emboîtement\",\"Empire\",\"Enclavement -Ensemble géographique\",\"Espace\",\"Espace public \",\"Espace vécu\",\"État\",\"État local\",\"Étendue\",\"Fédéralisme\",\"Finage\",\"Firme transnationale\",\"Fleuve\",\"Flux\",\"Foncier\",\"Forêt\",\"Fractale\",\"Friche\",\"Front\",\"Front pionnier\",\"Frontière\",\"Générique \",\"Gentrification\",\"Géoéconomie\",\"Géogramme\",\"Géographicité\",\"Géographie\",\"Géon\",\"Géopolitique\",\"Géosystème\",\"Géotype\",\"Ghetto\",\"Glacis\",\"Gouvernement urbain\",\"Gradient\",\"Graphe\",\"Graphique\",\"Gravitaire \",\"Guerre\",\"Habitat\",\"Habitat non-réglementaire\",\"Habiter\",\"Haut lieu\",\"Heimat\",\"Hétérotopie\",\"Hinterland\",\"Horizont\",\"Hors-sol\",\"Hub\",\"Identité spatiale\",\"Île\",\"Image\",\"Imaginaire géographique\",\"Immanence/Transcendance \",\"Infra-urbain\",\"Interaction spatiale\",\"Interface\",\"Interspatialité\",\"Irrigation\",\"Isolat\",\"Isotropie\",\"Jardin\",\"Justice spatiale\",\"Lieu\",\"Lieux centraux \",\"Limite\",\"Littoral\",\"Local\",\"Localisation\",\"Logistique\",\"Maillage\",\"Maison\",\"Marchandise\",\"Médiance\",\"Méditerranée\",\"Mer\",\"Métaphore spatiale\",\"Métrique\",\"Métropole/Mégalopole\",\"Métropolisation\",\"Migration\",\"Milieu\",\"Milieu innovateur\",\"Minorité territoriale\",\"Mobilité\",\"Monde\",\"Mondialisation\",\"Montagne\",\"Nation\",\"Network\",\"Nœud\",\"Norme\",\"Oasis\",\"Objet géographique\",\"Parc à thème\",\"Parc naturel\",\"Parcours\",\"Partie du monde\",\"Pavillonnaire \",\"Pays\",\"Paysage\",\"Périurbain\",\"Peuplement\",\"Polarisation\",\"Population \",\"Position\",\"Pratique spatiale\",\"Projet urbain\",\"Prospective territoriale\",\"Proxémie\",\"Reconstruction\",\"Reconversion\",\"Rénovation/Restauration/Réhabilitation\",\"Représentation de l’espace\",\"Réseau\",\"Réseau technique\",\"Réseau urbain\",\"Rhizome\",\"Rue\",\"Rural\",\"Schéma d’aménagement\",\"Ségrégation\",\"Seuil\",\"Site\",\"Situation géographique\",\"Société-Monde\",\"Sol\",\"Spatialité\",\"Stratégie spatiale\",\"Substance\",\"Système d’Information Géographique \",\"Système productif local \",\"Système spatial\",\"Technopôle/Technopole\",\"Télé-communication\",\"Télétravail\",\"Terre\",\"Territoire\",\"Territorial \",\"Territorialité\",\"Terroir\",\"Topogenèse\",\"Topographie\",\"Topologie\",\"Toponymie\",\"Tourisme\",\"Transition démographique\",\"Transports\",\"Ubiquité\",\"Urbain\",\"Urbain \",\"Urbanisation\",\"Urbanité\",\"Valeur spatiale\",\"Vaterland\",\"Végétation\",\"Village\",\"Ville\",\"Ville mondiale\",\"Ville nouvelle\",\"Violence\",\"Visibilité \",\"Voisinage\",\"Zonage\",\"Zone climatique\"]) or  \"géogr\" in word or \"géomat\" in word :\n",
    "            if  \"géogr\" in word or \"géomat\" in word or \"spatial\" in word or \"urbanis\" in word:  #or \"carto\" in word:  \n",
    "                geo+=1\n",
    "                geo_words.append(word)\n",
    "    if lng== \"english\":\n",
    "        for word in (string_to_read.split()):\n",
    "        #print word\n",
    "        #LUSSAULT ;) https://www.espacestemps.net/articles/entrees-par-categories/\n",
    "             if  \"geogr\" in word or \"geomat\" in word or \"spatial\" in word or \"urbanis\" in word: # or \"carto\" in word:\n",
    "                geo+=1\n",
    "                geo_words.append(word)\n",
    "    print \"SUJETS\\n\",geo_words,\"\\n points : \",geo\n",
    "    return [geo, geo_words]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_analysis_with_manual_detection(string_to_read):\n",
    "    lng = language_detection_with_pyenchant(string_to_read)\n",
    "    if lng== \"french\" :\n",
    "        geopoints= find_geo_topics(string_to_read, \"french\")\n",
    "        #print string_to_read\n",
    "    else:\n",
    "        if lng== \"english\" :\n",
    "            geopoints= find_geo_topics(string_to_read, \"english\")\n",
    "        \n",
    "        else:\n",
    "            print \"LANGUAGE NOT YET SUPPORTED\"\n",
    "            geopoints=['',''] \n",
    "    #    lng= [\"a\",\"à\",\"â\",\"abord\",\"afin\",\"ah\",\"ai\",\"aie\",\"ainsi\",\"allaient\",\"allo\",\"allô\",\"allons\",\"après\",\"assez\",\"attendu\",\"au\",\"aucun\",\"aucune\",\"aujourd\",\"aujourd'hui\",\"auquel\",\"aura\",\"auront\",\"aussi\",\"autre\",\"autres\",\"aux\",\"auxquelles\",\"auxquels\",\"avaient\",\"avais\",\"avait\",\"avant\",\"avec\",\"avoir\",\"ayant\",\"b\",\"bah\",\"beaucoup\",\"bien\",\"bigre\",\"boum\",\"bravo\",\"brrr\",\"c\",\"ça\",\"car\",\"ce\",\"ceci\",\"cela\",\"celle\",\"celle-ci\",\"celle-là\",\"celles\",\"celles-ci\",\"celles-là\",\"celui\",\"celui-ci\",\"celui-là\",\"cent\",\"cependant\",\"certain\",\"certaine\",\"certaines\",\"certains\",\"certes\",\"ces\",\"cet\",\"cette\",\"ceux\",\"ceux-ci\",\"ceuxlà\",\"chacun\",\"chaque\",\"cher\",\"chère\",\"chères\",\"chers\",\"chez\",\"chiche\",\"chut\",\"ci\",\"cinq\",\"cinquantaine\",\"cinquante\",\"cinquantième\",\"cinquième\",\"clac\",\"clic\",\"combien\",\"comme\",\"comment\",\"compris\",\"concernant\",\"contre\",\"couic\",\"crac\",\"d\",\"da\",\"dans\",\"de\",\"debout\",\"dedans\",\"dehors\",\"delà\",\"depuis\",\"derrière\",\"des\",\"dès\",\"désormais\",\"desquelles\",\"desquels\",\"dessous\",\"dessus\",\"deux\",\"deuxième\",\"deuxièmement\",\"devant\",\"devers\",\"devra\",\"différent\",\"différente\",\"différentes\",\"différents\",\"dire\",\"divers\",\"diverse\",\"diverses\",\"dix\",\"dix-huit\",\"dixième\",\"dix-neuf\",\"dixsept\",\"doit\",\"doivent\",\"donc\",\"dont\",\"douze\",\"douzième\",\"dring\",\"du\",\"duquel\",\"durant\",\"e\",\"effet\",\"eh\",\"elle\",\"elle-même\",\"elles\",\"ellesmêmes\",\"en\",\"encore\",\"entre\",\"envers\",\"environ\",\"es\",\"ès\",\"est\",\"et\",\"etant\",\"étaient\",\"étais\",\"était\",\"étant\",\"etc\",\"été\",\"etre\",\"être\",\"eu\",\"euh\",\"eux\",\"eux-mêmes\",\"excepté\",\"f\",\"façon\",\"fais\",\"faisaient\",\"faisant\",\"fait\",\"feront\",\"fi\",\"flac\",\"floc\",\"font\",\"g\",\"gens\",\"h\",\"ha\",\"hé\",\"hein\",\"hélas\",\"hem\",\"hep\",\"hi\",\"ho\",\"holà\",\"hop\",\"hormis\",\"hors\",\"hou\",\"houp\",\"hue\",\"hui\",\"huit\",\"huitième\",\"hum\",\"hurrah\",\"i\",\"il\",\"ils\",\"importe\",\"j\",\"je\",\"jusqu\",\"jusque\",\"k\",\"l\",\"la\",\"là\",\"laquelle\",\"las\",\"le\",\"lequel\",\"les\",\"lès\",\"lesquelles\",\"lesquels\",\"leur\",\"leurs\",\"longtemps\",\"lorsque\",\"lui\",\"lui-même\",\"m\",\"ma\",\"maint\",\"mais\",\"malgré\",\"me\",\"même\",\"mêmes\",\"merci\",\"mes\",\"mien\",\"mienne\",\"miennes\",\"miens\",\"mille\",\"mince\",\"moi\",\"moi-même\",\"moins\",\"mon\",\"moyennant\",\"n\",\"na\",\"ne\",\"néanmoins\",\"neuf\",\"neuvième\",\"ni\",\"nombreuses\",\"nombreux\",\"non\",\"nos\",\"notre\",\"nôtre\",\"nôtres\",\"nous\",\"nous-mêmes\",\"nul\",\"o\",\"o|\",\"ô\",\"oh\",\"ohé\",\"olé\",\"ollé\",\"on\",\"ont\",\"onze\",\"onzième\",\"ore\",\"ou\",\"où\",\"ouf\",\"ouias\",\"oust\",\"ouste\",\"outre\",\"p\",\"paf\",\"pan\",\"par\",\"parmi\",\"partant\",\"particulier\",\"particulière\",\"particulièrement\",\"pas\",\"passé\",\"pendant\",\"personne\",\"peu\",\"peut\",\"peuvent\",\"peux\",\"pff\",\"pfft\",\"pfut\",\"pif\",\"plein\",\"plouf\",\"plus\",\"plusieurs\",\"plutôt\",\"pouah\",\"pour\",\"pourquoi\",\"premier\",\"première\",\"premièrement\",\"près\",\"proche\",\"psitt\",\"puisque\",\"q\",\"qu\",\"quand\",\"quant\",\"quanta\",\"quant-à-soi\",\"quarante\",\"quatorze\",\"quatre\",\"quatre- vingt\",\"quatrième\",\"quatrièmement\",\"que\",\"quel\",\"quelconque\",\"quelle\",\"quelles\",\"quelque\",\"quelques\",\"quelqu'un\",\"quels\",\"qui\",\"quiconque\",\"quinze\",\"quoi\",\"quoique\",\"r\",\"revoici\",\"revoilà\",\"rien\",\"s\",\"sa\",\"sacrebleu\",\"sans\",\"sapristi\",\"sauf\",\"se\",\"seize\",\"selon\",\"sept\",\"septième\",\"sera\",\"seront\",\"ses\",\"si\",\"sien\",\"sienne\",\"siennes\",\"siens\",\"sinon\",\"six\",\"sixième\",\"soi\",\"soi-même\",\"soit\",\"soixante\",\"son\",\"sont\",\"sous\",\"stop\",\"suis\",\"suivant\",\"sur\",\"surtout\",\"t\",\"ta\",\"tac\",\"tant\",\"te\",\"té\",\"tel\",\"telle\",\"tellement\",\"telles\",\"tels\",\"tenant\",\"tes\",\"tic\",\"tien\",\"tienne\",\"tiennes\",\"tiens\",\"toc\",\"toi\",\"toi-même\",\"ton\",\"touchant\",\"toujours\",\"tous\",\"tout\",\"toute\",\"toutes\",\"treize\",\"trente\",\"très\",\"trois\",\"troisième\",\"troisièmement\",\"trop\",\"tsoin\",\"tsouin\",\"tu\",\"u\",\"un\",\"une\",\"unes\",\"uns\",\"v\",\"va\",\"vais\",\"vas\",\"vé\",\"vers\",\"via\",\"vif\",\"vifs\",\"vingt\",\"vivat\",\"vive\",\"vives\",\"vlan\",\"voici\",\"voilà\",\"vont\",\"vos\",\"votre\",\"vôtre\",\"vôtres\",\"vous\",\"vous-mêmes\",\"vu\",\"w\",\"x\",\"y\",\"z\",\"zut\"]\n",
    "    \n",
    "    return geopoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_html_pages_in_dirs_and_extract_mails_dict(pages_dir):\n",
    "    dict_mails={}\n",
    "    #rootdir=\"./DHFR_sample/\"\n",
    "    count_Cc=0\n",
    "    count_message=0\n",
    "    for subdir, dirs, files in os.walk(pages_dir):\n",
    "#ON VA LIRE TOUS LES FICHIERS DU REP ROOTDIR\n",
    "        \n",
    "        for file in files:\n",
    "    #V1     if file.endswith('.html') == True  and file != (\"index.html\") and not re.match(\"mail\\d+.html\",file) : \n",
    "           if re.match(\"msg\\d+.html\",file) :\n",
    "          \n",
    "            # les index.html sont des recap, on pourrait les compter pour s'assurer du nombre de messages\n",
    "                \n",
    "                #!print \"\\n\",os.path.join(subdir, file), \"\\n\"\n",
    "                champs_mail={}\n",
    "                filename=os.path.join(subdir, file)\n",
    "                champs_mail[\"Geo_Topic\"]=read_html_and_testtextextract(filename)\n",
    "                \n",
    "                \n",
    "                soup=BeautifulSoup(open(filename, 'r').read(),'lxml')#, 'html.parser') \n",
    "#RECUPERER LE SUJET DU MESSAGE\n",
    "                \n",
    "                i=0\n",
    "                for zone_cible in soup.findAll('ul'):\n",
    "                    i+=1\n",
    "                    sujet_messg = zone_cible.find(string=re.compile(\"\\[DH\\]\"))\n",
    "                    zone_de_metadonnees = None\n",
    "                    special_auteur=0\n",
    "##ON ISOLE LE CHAMPS AUTEUR\n",
    "                    if '<li><strong>From</strong>:' in str(zone_cible) and '<li><strong>To</strong>:' in str(zone_cible) :\n",
    "                        #print \"FROM/TO DETECTED, i= \",i\n",
    "                        count_message+=1\n",
    "                        #print  zone_cible\n",
    "                        nom_auteur= re.sub(r'<ul>\\n<li><strong>From</strong>: ', '',str(zone_cible))\n",
    "                        sep = \"&lt;\"\n",
    "                        nom_auteur = nom_auteur.split(sep, 1)[0]\n",
    "                        \n",
    "                        \n",
    "                        mail1= re.sub(r'<ul>\\n<li><strong>From</strong>:.+\\n.+\\n.+\\n document\\.write\\(\\\"', '',str(zone_cible))\n",
    "                        mail2=re.sub(r'<ul>\\n<li><strong>From</strong>:.+\\n.+\\n.+\\n document\\.write\\(\\\".+@\\\" \\+ \\\"', '',str(zone_cible))\n",
    "                        sep1 = \"\\\" + \\\"@\\\" + \"\n",
    "                        sep2 = \"\\\")\"\n",
    "                        mail1= mail1.split(sep1, 1)[0]\n",
    "                        mail2= mail2.split(sep2, 1)[0]\n",
    "                        mail_auteur= mail1+\"@\"+mail2\n",
    "                        #print mail_auteur\n",
    "                        if len(mail_auteur)>60:\n",
    "                            try:\n",
    "                                mail= re.sub(r'<ul>\\n<li><strong>From</strong>:', '',str(zone_cible))\n",
    "                                mail=re.search('&lt;(.*)&gt;',str(mail))\n",
    "                                mail=mail.group(1)\n",
    "                                #print \"MAILBIS\",mail\n",
    "                                mail_auteur=mail\n",
    "                                #special_auteur=1\n",
    "                            except:\n",
    "                                print \"BUG\"\n",
    "                        #CHECK PROPER CAPTURE OF THE NAME    \n",
    "                        if len(nom_auteur)>50:\n",
    "                            #print zone_cible\n",
    "                            nom_auteur=mail_auteur\n",
    "                        if len(nom_auteur)<5:\n",
    "                            nom_auteur=mail_auteur\n",
    "                        if nom_auteur[0]== \"\\\"\":\n",
    "                            nom_auteur=nom_auteur[1:]\n",
    "                            nom_auteur=nom_auteur[:-2]\n",
    "                            print nom_auteur\n",
    "                        if nom_auteur.endswith(' '):\n",
    "                            nom_auteur=nom_auteur[:-1]\n",
    "                        #!print \"nom_auteur \",nom_auteur+\" mail_auteur \"+mail_auteur,\"\\n\"\n",
    "                        champs_mail[\"nom_auteur\"]=  nom_auteur\n",
    "                        champs_mail[\"mail_auteur\"]=  mail_auteur\n",
    "                        champs_mail[\"ref_physique_de_l_article\"]=  os.path.join(subdir, file)\n",
    "                        champs_mail[\"sujet_du_message\"] = sujet_messg\n",
    "##ON ISOLE LE CHAMP DESTINATAIRES\n",
    "                        #if special_auteur == 1:\n",
    "                            #print \"ZCspecAut\",zone_cible\n",
    "                        #else: \n",
    "                        \n",
    "                        destinataires=re.sub(r'<ul>\\n<li><strong>From</strong>:.+\\n.+\\n.+\\n.+\\n.+\\n.+\\n.+\\n', '',str(zone_cible))\n",
    "                        \n",
    "                        sep3=\";</li>\"\n",
    "                        destinataires= destinataires.split(sep3, 1)[0]\n",
    "                        #print destinataires\n",
    "                        dest=re.findall(\"<script type=.*\\n.+\\n.+\\n.+\\n.+/script>\", destinataires)\n",
    "                        #print dest\n",
    "                        liste_destinataires=[]\n",
    "                        for destinataire in dest:\n",
    "                            #print destinataire,\"\\n___________________\\n\"\n",
    "                            mail1= re.sub(r'<script type=\\\"text/javascript\\\">\\n <!-- \\n document\\.write\\(\\\"', '',str(destinataire))\n",
    "                            mail2=re.sub(r'<script type=\\\"text/javascript\\\">\\n <!-- \\n document\\.write\\(\\\".+@\\\" \\+ \\\"', '',str(destinataire))\n",
    "                            sep1 = \"\\\" + \\\"@\\\" + \"\n",
    "                            sep2 = \"\\\")\"\n",
    "                            mail1= mail1.split(sep1, 1)[0]\n",
    "                            mail2= mail2.split(sep2, 1)[0]\n",
    "                            mail_destinataire= mail1+\"@\"+mail2\n",
    "                            #print mail_destinataire,\"\\n___________\\n\"\n",
    "                            if mail_destinataire not in liste_destinataires :\n",
    "                                liste_destinataires.append(mail_destinataire)\n",
    "                        if liste_destinataires==[]:\n",
    "                            #print zone_cible\n",
    "                            liste_destinataires=['dh@groupes.renater.fr']\n",
    "                        #!print \"destinataires: \",liste_destinataires,\"\\n\"\n",
    "                        champs_mail[\"liste_mails_dests\"] = liste_destinataires\n",
    "                       \n",
    "                        if '<li><strong>Cc</strong>:' in str(zone_cible):\n",
    "##ON ISOLE LE CHAMP CC\n",
    "                            #print \"CC DETECTED, i= \",i\n",
    "                            #print zone_cible\n",
    "                            count_Cc+=1\n",
    "                            CCfield= re.search('<li><strong>Cc</strong>:(.*)<li><strong>Subject</strong>',str(zone_cible),flags=re.DOTALL)\n",
    "                            CC= CCfield.group(1)\n",
    "                            CC=str(CC)\n",
    "                            CC=re.sub(r'^.+\\n','',str(CC),flags=0)\n",
    "                            CCs=re.findall(\"<script type=.*\\n.+\\n.+\\n.+\\n.+/script>\", CC)\n",
    "                            #sep1 = \"\\\" + \\\"@\\\" + \"\n",
    "                            #mail1= mail1.split(sep1, 1)[0]\n",
    "                            #print \"CC FIELd\\n\"#,CCs\n",
    "                            liste_CCs=[]\n",
    "                            for CC in CCs:\n",
    "                                #print CC,\"\\n___________________\\n\"\n",
    "                                mail1= re.sub(r'<script type=\\\"text/javascript\\\">\\n <!-- \\n document\\.write\\(\\\"', '',str(CC))\n",
    "                                mail2=re.sub(r'<script type=\\\"text/javascript\\\">\\n <!-- \\n document\\.write\\(\\\".+@\\\" \\+ \\\"', '',str(CC))\n",
    "                                sep1 = \"\\\" + \\\"@\\\" + \"\n",
    "                                sep2 = \"\\\")\"\n",
    "                                mail1= mail1.split(sep1, 1)[0]\n",
    "                                mail2= mail2.split(sep2, 1)[0]\n",
    "                                mail_CC= mail1+\"@\"+mail2\n",
    "                                #print mail_CC,\"\\n___________\\n\"\n",
    "                                if mail_CC not in liste_CCs :\n",
    "                                    liste_CCs.append(mail_CC)\n",
    "                            #!print \"CCs: \",liste_CCs\n",
    "                            champs_mail[\"liste_CCs\"] = liste_CCs\n",
    "##ON ISOLE LE CHAMP DATE                       \n",
    "                        #print \"ZONECIBLE\",zone_cible\n",
    "                        champs_datef=re.search('<li><strong>Date</strong>:(.+?)</li>',str(zone_cible),flags=re.DOTALL)\n",
    "                        #print \"CHAMPSDATEF\",champs_datef\n",
    "                        try :\n",
    "                            champs_date=champs_datef.group(1)\n",
    "                        except :\n",
    "                            #print \"ZCparent\", zone_cible.parent\n",
    "                            champs_date=re.search('<!--X-Date: (.*) -->',str(soup))\n",
    "                            champs_date=champs_date.group(1)\n",
    "                            #print champs_date\n",
    "                            \n",
    "                        #print champs_date\n",
    "                        try : \n",
    "                            champs_date=parser.parse(champs_date)\n",
    "                        except ValueError :\n",
    "                            sep=\" (\"\n",
    "                            champs_date=champs_date.split(sep, 1)[0]\n",
    "                            \n",
    "                        #!print  champs_date\n",
    "                        champs_mail[\"date\"] = champs_date\n",
    "                        \n",
    "                        \n",
    "                        dict_mails[os.path.join(subdir, file)]= champs_mail\n",
    "                        break\n",
    "##LE DICTIONNAIRE DES MAILS SE NOMME dict_mails\n",
    "    #print dict_mails\n",
    "    print count_message,\" messages\"\n",
    "    print count_Cc, \" messages avec Cc\"\n",
    "    ratiocount = float(count_Cc)/float(count_message)\n",
    "    print \"Taux_de_mails_avec_CC:\",ratiocount,\"\\n\"\n",
    "    return dict_mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus_and_extract_topo_network(corpus):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus_and_extract_topo_Geo_network(corpus):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus_and_extract_mails(corpus):\n",
    "    liste_mails=[]\n",
    "    for mesg in corpus:\n",
    "        #print corpus[mesg]\n",
    "        if corpus[mesg]['mail_auteur'] not in liste_mails:\n",
    "            liste_mails.append(corpus[mesg]['mail_auteur'])\n",
    "        for i,key in enumerate(corpus[mesg]['liste_mails_dests']):\n",
    "            #print key\n",
    "            if key not in liste_mails:\n",
    "                liste_mails.append(key)\n",
    "        if 'liste_CCs' in corpus[mesg] :\n",
    "            #print \"TRUVE CC\"\n",
    "            for i,key in enumerate(corpus[mesg]['liste_CCs']):\n",
    "                #print key\n",
    "                if key not in liste_mails:\n",
    "                    liste_mails.append(key)\n",
    "    return liste_mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus_and_extract_localisations(corpus):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus_and_extract_names(corpus):\n",
    "    liste_noms=[]\n",
    "    for mesg in corpus:\n",
    "        try: \n",
    "            if corpus[mesg]['nom_auteur'][0]== \"\\\"\":\n",
    "                print corpus[mesg]['nom_auteur']\n",
    "        except IndexError:\n",
    "            print \"Bug\", corpus[mesg]['nom_auteur']\n",
    "            print corpus[mesg]\n",
    "        if [corpus[mesg]['nom_auteur'],corpus[mesg]['mail_auteur']] not in liste_noms:\n",
    "            for i,key in enumerate(liste_noms):\n",
    "                if SequenceMatcher(None, corpus[mesg]['nom_auteur'], key[0]).ratio() >=0.9:\n",
    "                    print \"POSSIBLE DUPLICATE |\", corpus[mesg]['nom_auteur'],\"|   |\",key[0],\"|\"\n",
    "                    print corpus[mesg]['mail_auteur'], key[1]\n",
    "                \n",
    "            liste_noms.append([corpus[mesg]['nom_auteur'],corpus[mesg]['mail_auteur']])\n",
    "        \n",
    "    return liste_noms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Noiret, Serge\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Noiret, Serge\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Ghislain SILLAUME\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Noiret, Serge\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['spatiale.Cordialement,Elina', 'spatiale', 'spatiale', 'g\\xc3\\xa9omatique,', 'g\\xc3\\xa9ographique,', 'spatiale', 'spatiale', 'spatiales', 'spatiale,'] \n",
      " points :  9\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['urbanisme,', 'g\\xc3\\xa9ographie,'] \n",
      " points :  2\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "aude.da-cruz-lima\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie)'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographiques'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['urbanisation,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographes,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Wandl-Vogt, Eveline\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Wandl-Vogt, Eveline\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['spatiales'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "elisabeth.belmas\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographique\\xc2\\xa0:'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Viera Rebolledo-Dhuin\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographique\\xc2\\xa0:'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographiques'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['spatiales', '&quot;g\\xc3\\xa9omatique&quot;'] \n",
      " points :  2\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['spatial'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Antonio A. Casilli\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Anne Gresillon\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['urbanisation', 'urbaniste'] \n",
      " points :  2\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie.Pour'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['geography,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Antonio A. Casilli\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Epler, Jakob\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographiques'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9omaticiens', 'g\\xc3\\xa9omatique'] \n",
      " points :  2\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Noiret, Serge\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,', 'g\\xc3\\xa9ographique'] \n",
      " points :  2\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Manuel Zacklad\n",
      "294  messages\n",
      "35  messages avec Cc\n",
      "Taux_de_mails_avec_CC: 0.119047619048 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_mails_DH=read_html_pages_in_dirs_and_extract_mails_dict(\"./DHFRsample\")\n",
    "#print corpus_mails_DH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liste_mails=read_corpus_and_extract_mails(corpus_mails_DH)\n",
    "print len(liste_mails)\n",
    "print liste_mails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_noms=read_corpus_and_extract_names(corpus_mails_DH)\n",
    "print len(liste_noms)\n",
    "print liste_noms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html_pages_in_dirs_and_testtextextract(pages_dir):\n",
    "    dict_mails={}\n",
    "    #rootdir=\"./DHFR_sample/\"\n",
    "\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(pages_dir):\n",
    "#ON VA LIRE TOUS LES FICHIERS DU REP ROOTDIR\n",
    "        \n",
    "        for file in files:\n",
    "            dictT={}\n",
    "    #V1     if file.endswith('.html') == True  and file != (\"index.html\") and not re.match(\"mail\\d+.html\",file) : \n",
    "            if re.match(\"msg\\d+.html\",file):\n",
    "                filename=os.path.join(subdir, file)\n",
    "                with open(filename, 'r') as myfile:           \n",
    "                \n",
    "                    data=myfile.read()\n",
    "                    result = re.search('<!--X-Body-of-Message-->(.*)<!--X-Body-of-Message-End-->', data, flags=re.DOTALL)\n",
    "                    ZC= result.group(1)\n",
    "                    ZC=cleanhtml(ZC)\n",
    "                    dictT[\"content\"]=ZC\n",
    "                    dictT[\"geo\"]=topic_analysis_with_manual_detection(ZC)\n",
    "                dict_mails[filename]= dictT\n",
    "            \n",
    "    return dict_mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html_and_testtextextract(filename):\n",
    "                dictT={}\n",
    "                with open(filename, 'r') as myfile:\n",
    "                    data=myfile.read()\n",
    "                    result = re.search('<!--X-Body-of-Message-->(.*)<!--X-Body-of-Message-End-->', data, flags=re.DOTALL)\n",
    "                    ZC= result.group(1)\n",
    "                    ZC=cleanhtml(ZC)\n",
    "                    dictT[\"content\"]=ZC\n",
    "                    dictT[\"geo\"]=topic_analysis_with_manual_detection(ZC)\n",
    "                return dictT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_txt=read_html_pages_in_dirs_and_testtextextract(\"./DHFRsample\")\n",
    "#print corpus_txt\n",
    "#lng= language_detection_with_pyenchant(str(corpus_txt))\n",
    "#print lng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "print enchant.list_languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyse =topic_analysis_with_nltk_gensim(corpus_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Analyse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_analysis_with_nltk_gensim(string_to_read):\n",
    "    string_to_read=unicode(str(string_to_read), 'utf8')\n",
    "    lng = language_detection_with_pyenchant(string_to_read)\n",
    "   \n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    import string\n",
    "    print lng\n",
    "    stop = set(stopwords.words(lng))\n",
    "    exclude = set(string.punctuation)\n",
    "    lemma = WordNetLemmatizer()\n",
    "    def clean(doc):\n",
    "        stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "        \n",
    "        punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "        return normalized\n",
    "\n",
    "    doc_clean = clean(string_to_read).split()\n",
    "    print \"DOC CLEAN\",doc_clean\n",
    "        # Importing Gensim\n",
    "    import gensim\n",
    "    from gensim import corpora\n",
    "\n",
    "        # Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "    dictionary = corpora.Dictionary([doc_clean])\n",
    "\n",
    "        # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = dictionary.doc2bow(doc_clean)\n",
    "    Lda = gensim.models.ldamodel.LdaModel\n",
    "    print Lda\n",
    "\n",
    "        # Running and Trainign LDA model on the document term matrix.\n",
    "    ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=5)\n",
    "    return ldamodel\n",
    "       \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    #return [topic_words,doctopic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
