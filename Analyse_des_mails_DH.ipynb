{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some imports\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser\n",
    "from difflib import SequenceMatcher\n",
    "import re,os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FOR TOPIC MODELLING\n",
    "#https://de.dariah.eu/tatom/topic_model_python.html\n",
    "import numpy as np  # a conventional alias\n",
    "\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "  cleanr = re.compile('<.*?>')\n",
    "  cleantext = re.sub(cleanr, '', raw_html)\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def language_detection_with_pyenchant(string_to_read):\n",
    "    #https://stackoverflow.com/questions/3788870/how-to-check-if-a-word-is-an-english-word-with-python\n",
    "    import enchant\n",
    "    lg_ang=0\n",
    "    us = enchant.Dict(\"en_US\")\n",
    "    #print \"US LOADED\"\n",
    "    fr = enchant.Dict(\"fr_FR\")\n",
    "    #print \"FR LOADED\"\n",
    "    lg_fr=0\n",
    "    lg_ang=0\n",
    "    #print \"string_to_read\",string_to_read\n",
    "    for word in string_to_read.split():\n",
    "        #print fr.check(word)\n",
    "        #print word\n",
    "        if fr.check(word) == True:\n",
    "            lg_fr+=1\n",
    "        if us.check(word) == True:\n",
    "            lg_ang+=1\n",
    "    #print \"THERE I AM\"\n",
    "    if lg_fr >= lg_ang :\n",
    "        return \"french\"\n",
    "    else:\n",
    "        if lg_ang > lg_fr :\n",
    "        \n",
    "            return \"english\"\n",
    "        else: \n",
    "            return \"NEITHER ENGLISH NOR FRENCH\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_analysis_with_mallet(string_to_read):\n",
    "    lng = language_detection_with_pyenchant(string_to_read)\n",
    "    if lng== \"french\" :\n",
    "        lng= [\"a\",\"à\",\"â\",\"abord\",\"afin\",\"ah\",\"ai\",\"aie\",\"ainsi\",\"allaient\",\"allo\",\"allô\",\"allons\",\"après\",\"assez\",\"attendu\",\"au\",\"aucun\",\"aucune\",\"aujourd\",\"aujourd'hui\",\"auquel\",\"aura\",\"auront\",\"aussi\",\"autre\",\"autres\",\"aux\",\"auxquelles\",\"auxquels\",\"avaient\",\"avais\",\"avait\",\"avant\",\"avec\",\"avoir\",\"ayant\",\"b\",\"bah\",\"beaucoup\",\"bien\",\"bigre\",\"boum\",\"bravo\",\"brrr\",\"c\",\"ça\",\"car\",\"ce\",\"ceci\",\"cela\",\"celle\",\"celle-ci\",\"celle-là\",\"celles\",\"celles-ci\",\"celles-là\",\"celui\",\"celui-ci\",\"celui-là\",\"cent\",\"cependant\",\"certain\",\"certaine\",\"certaines\",\"certains\",\"certes\",\"ces\",\"cet\",\"cette\",\"ceux\",\"ceux-ci\",\"ceuxlà\",\"chacun\",\"chaque\",\"cher\",\"chère\",\"chères\",\"chers\",\"chez\",\"chiche\",\"chut\",\"ci\",\"cinq\",\"cinquantaine\",\"cinquante\",\"cinquantième\",\"cinquième\",\"clac\",\"clic\",\"combien\",\"comme\",\"comment\",\"compris\",\"concernant\",\"contre\",\"couic\",\"crac\",\"d\",\"da\",\"dans\",\"de\",\"debout\",\"dedans\",\"dehors\",\"delà\",\"depuis\",\"derrière\",\"des\",\"dès\",\"désormais\",\"desquelles\",\"desquels\",\"dessous\",\"dessus\",\"deux\",\"deuxième\",\"deuxièmement\",\"devant\",\"devers\",\"devra\",\"différent\",\"différente\",\"différentes\",\"différents\",\"dire\",\"divers\",\"diverse\",\"diverses\",\"dix\",\"dix-huit\",\"dixième\",\"dix-neuf\",\"dixsept\",\"doit\",\"doivent\",\"donc\",\"dont\",\"douze\",\"douzième\",\"dring\",\"du\",\"duquel\",\"durant\",\"e\",\"effet\",\"eh\",\"elle\",\"elle-même\",\"elles\",\"ellesmêmes\",\"en\",\"encore\",\"entre\",\"envers\",\"environ\",\"es\",\"ès\",\"est\",\"et\",\"etant\",\"étaient\",\"étais\",\"était\",\"étant\",\"etc\",\"été\",\"etre\",\"être\",\"eu\",\"euh\",\"eux\",\"eux-mêmes\",\"excepté\",\"f\",\"façon\",\"fais\",\"faisaient\",\"faisant\",\"fait\",\"feront\",\"fi\",\"flac\",\"floc\",\"font\",\"g\",\"gens\",\"h\",\"ha\",\"hé\",\"hein\",\"hélas\",\"hem\",\"hep\",\"hi\",\"ho\",\"holà\",\"hop\",\"hormis\",\"hors\",\"hou\",\"houp\",\"hue\",\"hui\",\"huit\",\"huitième\",\"hum\",\"hurrah\",\"i\",\"il\",\"ils\",\"importe\",\"j\",\"je\",\"jusqu\",\"jusque\",\"k\",\"l\",\"la\",\"là\",\"laquelle\",\"las\",\"le\",\"lequel\",\"les\",\"lès\",\"lesquelles\",\"lesquels\",\"leur\",\"leurs\",\"longtemps\",\"lorsque\",\"lui\",\"lui-même\",\"m\",\"ma\",\"maint\",\"mais\",\"malgré\",\"me\",\"même\",\"mêmes\",\"merci\",\"mes\",\"mien\",\"mienne\",\"miennes\",\"miens\",\"mille\",\"mince\",\"moi\",\"moi-même\",\"moins\",\"mon\",\"moyennant\",\"n\",\"na\",\"ne\",\"néanmoins\",\"neuf\",\"neuvième\",\"ni\",\"nombreuses\",\"nombreux\",\"non\",\"nos\",\"notre\",\"nôtre\",\"nôtres\",\"nous\",\"nous-mêmes\",\"nul\",\"o\",\"o|\",\"ô\",\"oh\",\"ohé\",\"olé\",\"ollé\",\"on\",\"ont\",\"onze\",\"onzième\",\"ore\",\"ou\",\"où\",\"ouf\",\"ouias\",\"oust\",\"ouste\",\"outre\",\"p\",\"paf\",\"pan\",\"par\",\"parmi\",\"partant\",\"particulier\",\"particulière\",\"particulièrement\",\"pas\",\"passé\",\"pendant\",\"personne\",\"peu\",\"peut\",\"peuvent\",\"peux\",\"pff\",\"pfft\",\"pfut\",\"pif\",\"plein\",\"plouf\",\"plus\",\"plusieurs\",\"plutôt\",\"pouah\",\"pour\",\"pourquoi\",\"premier\",\"première\",\"premièrement\",\"près\",\"proche\",\"psitt\",\"puisque\",\"q\",\"qu\",\"quand\",\"quant\",\"quanta\",\"quant-à-soi\",\"quarante\",\"quatorze\",\"quatre\",\"quatre- vingt\",\"quatrième\",\"quatrièmement\",\"que\",\"quel\",\"quelconque\",\"quelle\",\"quelles\",\"quelque\",\"quelques\",\"quelqu'un\",\"quels\",\"qui\",\"quiconque\",\"quinze\",\"quoi\",\"quoique\",\"r\",\"revoici\",\"revoilà\",\"rien\",\"s\",\"sa\",\"sacrebleu\",\"sans\",\"sapristi\",\"sauf\",\"se\",\"seize\",\"selon\",\"sept\",\"septième\",\"sera\",\"seront\",\"ses\",\"si\",\"sien\",\"sienne\",\"siennes\",\"siens\",\"sinon\",\"six\",\"sixième\",\"soi\",\"soi-même\",\"soit\",\"soixante\",\"son\",\"sont\",\"sous\",\"stop\",\"suis\",\"suivant\",\"sur\",\"surtout\",\"t\",\"ta\",\"tac\",\"tant\",\"te\",\"té\",\"tel\",\"telle\",\"tellement\",\"telles\",\"tels\",\"tenant\",\"tes\",\"tic\",\"tien\",\"tienne\",\"tiennes\",\"tiens\",\"toc\",\"toi\",\"toi-même\",\"ton\",\"touchant\",\"toujours\",\"tous\",\"tout\",\"toute\",\"toutes\",\"treize\",\"trente\",\"très\",\"trois\",\"troisième\",\"troisièmement\",\"trop\",\"tsoin\",\"tsouin\",\"tu\",\"u\",\"un\",\"une\",\"unes\",\"uns\",\"v\",\"va\",\"vais\",\"vas\",\"vé\",\"vers\",\"via\",\"vif\",\"vifs\",\"vingt\",\"vivat\",\"vive\",\"vives\",\"vlan\",\"voici\",\"voilà\",\"vont\",\"vos\",\"votre\",\"vôtre\",\"vôtres\",\"vous\",\"vous-mêmes\",\"vu\",\"w\",\"x\",\"y\",\"z\",\"zut\"]\n",
    "    \n",
    "    vectorizer = text.CountVectorizer(input=string_to_read, stop_words=lng, min_df=3)\n",
    "    dtm = vectorizer.fit_transform(string_to_read.split()).toarray()\n",
    "    vocab = np.array(vectorizer.get_feature_names())\n",
    "    dtm.shape\n",
    "    len(vocab)\n",
    "    num_topics = 20\n",
    "    num_top_words = 20\n",
    "    clf = decomposition.NMF(n_components=num_topics, random_state=1)\n",
    "    doctopic = clf.fit_transform(dtm)\n",
    "    topic_words = []\n",
    "    for topic in clf.components_:\n",
    "        word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "        topic_words.append([vocab[i] for i in word_idx])\n",
    "    doctopic = doctopic / np.sum(doctopic, axis=1, keepdims=True)\n",
    "    return [topic_words,doctopic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#so far french support\n",
    "def find_geo_topics(string_to_read, lng):\n",
    "    geo=0\n",
    "    geo_words=[]\n",
    "    #print string_to_read\n",
    "    if lng== \"french\":\n",
    "        for word in (string_to_read.split()):\n",
    "        #print word\n",
    "        #LUSSAULT ;) https://www.espacestemps.net/articles/entrees-par-categories/\n",
    "            # if (word in [\"Théorie de l’espace\",\"Accessibilité\",\"Acteur spatial\",\"Action spatiale\",\"Agencement\",\"Agglomération\",\"Ailleurs\",\"Aire\",\"Aire culturelle\",\"Alignement\",\"Ambiance architecturale et urbaine\",\"Aménagement du territoire\",\"Anamorphose\",\"Anthropisation\",\"Archipel mégalopolitain mondial \",\"Armature urbaine\",\"Attraction\",\"Autocorrélation spatiale\",\"Banlieue\",\"Campagne\",\"Capital spatial\",\"Carte\",\"Carte mentale\",\"Centralité\",\"Centre/Périphérie\",\"Centre urbain\",\"Chorème\",\"Chorotype\",\"Circulation\",\"Citadinité\",\"Communication territoriale\",\"Commutateur\",\"Compromis territorial\",\"Concentration\",\"Configuration spatiale\",\"Confins\",\"Connexité\",\"Contact\",\"Contiguïté\",\"Continent\",\"Continuité\",\"Coprésence\",\"Corps\",\"Cospatialité\",\"Cyberespace\",\"Décentralisation\",\"Découpage\",\"Découverte\",\"Défrichement\",\"Densité\",\"Désert\",\"Déterritorialisation\",\"Développement local\",\"Diaspora\",\"Différenciation spatiale -Diffusion\",\"Discontinuité\",\"Dispositif spatial légitime\",\"Distance\",\"Distribution rang/taille\",\"Distribution spatiale\",\"District industriel\",\"Diversité\",\"Dynamique spatiale\",\"Écart\",\"Échelle\",\"Économie-monde\",\"Écoumène\",\"Edge City\",\"Emblème territorial\",\"Emboîtement\",\"Empire\",\"Enclavement -Ensemble géographique\",\"Espace\",\"Espace public \",\"Espace vécu\",\"État\",\"État local\",\"Étendue\",\"Fédéralisme\",\"Finage\",\"Firme transnationale\",\"Fleuve\",\"Flux\",\"Foncier\",\"Forêt\",\"Fractale\",\"Friche\",\"Front\",\"Front pionnier\",\"Frontière\",\"Générique \",\"Gentrification\",\"Géoéconomie\",\"Géogramme\",\"Géographicité\",\"Géographie\",\"Géon\",\"Géopolitique\",\"Géosystème\",\"Géotype\",\"Ghetto\",\"Glacis\",\"Gouvernement urbain\",\"Gradient\",\"Graphe\",\"Graphique\",\"Gravitaire \",\"Guerre\",\"Habitat\",\"Habitat non-réglementaire\",\"Habiter\",\"Haut lieu\",\"Heimat\",\"Hétérotopie\",\"Hinterland\",\"Horizont\",\"Hors-sol\",\"Hub\",\"Identité spatiale\",\"Île\",\"Image\",\"Imaginaire géographique\",\"Immanence/Transcendance \",\"Infra-urbain\",\"Interaction spatiale\",\"Interface\",\"Interspatialité\",\"Irrigation\",\"Isolat\",\"Isotropie\",\"Jardin\",\"Justice spatiale\",\"Lieu\",\"Lieux centraux \",\"Limite\",\"Littoral\",\"Local\",\"Localisation\",\"Logistique\",\"Maillage\",\"Maison\",\"Marchandise\",\"Médiance\",\"Méditerranée\",\"Mer\",\"Métaphore spatiale\",\"Métrique\",\"Métropole/Mégalopole\",\"Métropolisation\",\"Migration\",\"Milieu\",\"Milieu innovateur\",\"Minorité territoriale\",\"Mobilité\",\"Monde\",\"Mondialisation\",\"Montagne\",\"Nation\",\"Network\",\"Nœud\",\"Norme\",\"Oasis\",\"Objet géographique\",\"Parc à thème\",\"Parc naturel\",\"Parcours\",\"Partie du monde\",\"Pavillonnaire \",\"Pays\",\"Paysage\",\"Périurbain\",\"Peuplement\",\"Polarisation\",\"Population \",\"Position\",\"Pratique spatiale\",\"Projet urbain\",\"Prospective territoriale\",\"Proxémie\",\"Reconstruction\",\"Reconversion\",\"Rénovation/Restauration/Réhabilitation\",\"Représentation de l’espace\",\"Réseau\",\"Réseau technique\",\"Réseau urbain\",\"Rhizome\",\"Rue\",\"Rural\",\"Schéma d’aménagement\",\"Ségrégation\",\"Seuil\",\"Site\",\"Situation géographique\",\"Société-Monde\",\"Sol\",\"Spatialité\",\"Stratégie spatiale\",\"Substance\",\"Système d’Information Géographique \",\"Système productif local \",\"Système spatial\",\"Technopôle/Technopole\",\"Télé-communication\",\"Télétravail\",\"Terre\",\"Territoire\",\"Territorial \",\"Territorialité\",\"Terroir\",\"Topogenèse\",\"Topographie\",\"Topologie\",\"Toponymie\",\"Tourisme\",\"Transition démographique\",\"Transports\",\"Ubiquité\",\"Urbain\",\"Urbain \",\"Urbanisation\",\"Urbanité\",\"Valeur spatiale\",\"Vaterland\",\"Végétation\",\"Village\",\"Ville\",\"Ville mondiale\",\"Ville nouvelle\",\"Violence\",\"Visibilité \",\"Voisinage\",\"Zonage\",\"Zone climatique\"]) or  \"géogr\" in word or \"géomat\" in word :\n",
    "            if  \"géogr\" in word or \"géomat\" in word or \"spatial\" in word or \"urbanis\" in word:  #or \"carto\" in word:  \n",
    "                geo+=1\n",
    "                geo_words.append(word)\n",
    "    if lng== \"english\":\n",
    "        for word in (string_to_read.split()):\n",
    "        #print word\n",
    "        #LUSSAULT ;) https://www.espacestemps.net/articles/entrees-par-categories/\n",
    "             if  \"geogr\" in word or \"geomat\" in word or \"spatial\" in word or \"urbanis\" in word: # or \"carto\" in word:\n",
    "                geo+=1\n",
    "                geo_words.append(word)\n",
    "    print \"SUJETS\\n\",geo_words,\"\\n points : \",geo\n",
    "    return [geo, geo_words]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_analysis_with_manual_detection(string_to_read):\n",
    "    lng = language_detection_with_pyenchant(string_to_read)\n",
    "    if lng== \"french\" :\n",
    "        geopoints= find_geo_topics(string_to_read, \"french\")\n",
    "        #print string_to_read\n",
    "    else:\n",
    "        if lng== \"english\" :\n",
    "            geopoints= find_geo_topics(string_to_read, \"english\")\n",
    "        \n",
    "        else:\n",
    "            print \"LANGUAGE NOT YET SUPPORTED\"\n",
    "            geopoints=['',''] \n",
    "    #    lng= [\"a\",\"à\",\"â\",\"abord\",\"afin\",\"ah\",\"ai\",\"aie\",\"ainsi\",\"allaient\",\"allo\",\"allô\",\"allons\",\"après\",\"assez\",\"attendu\",\"au\",\"aucun\",\"aucune\",\"aujourd\",\"aujourd'hui\",\"auquel\",\"aura\",\"auront\",\"aussi\",\"autre\",\"autres\",\"aux\",\"auxquelles\",\"auxquels\",\"avaient\",\"avais\",\"avait\",\"avant\",\"avec\",\"avoir\",\"ayant\",\"b\",\"bah\",\"beaucoup\",\"bien\",\"bigre\",\"boum\",\"bravo\",\"brrr\",\"c\",\"ça\",\"car\",\"ce\",\"ceci\",\"cela\",\"celle\",\"celle-ci\",\"celle-là\",\"celles\",\"celles-ci\",\"celles-là\",\"celui\",\"celui-ci\",\"celui-là\",\"cent\",\"cependant\",\"certain\",\"certaine\",\"certaines\",\"certains\",\"certes\",\"ces\",\"cet\",\"cette\",\"ceux\",\"ceux-ci\",\"ceuxlà\",\"chacun\",\"chaque\",\"cher\",\"chère\",\"chères\",\"chers\",\"chez\",\"chiche\",\"chut\",\"ci\",\"cinq\",\"cinquantaine\",\"cinquante\",\"cinquantième\",\"cinquième\",\"clac\",\"clic\",\"combien\",\"comme\",\"comment\",\"compris\",\"concernant\",\"contre\",\"couic\",\"crac\",\"d\",\"da\",\"dans\",\"de\",\"debout\",\"dedans\",\"dehors\",\"delà\",\"depuis\",\"derrière\",\"des\",\"dès\",\"désormais\",\"desquelles\",\"desquels\",\"dessous\",\"dessus\",\"deux\",\"deuxième\",\"deuxièmement\",\"devant\",\"devers\",\"devra\",\"différent\",\"différente\",\"différentes\",\"différents\",\"dire\",\"divers\",\"diverse\",\"diverses\",\"dix\",\"dix-huit\",\"dixième\",\"dix-neuf\",\"dixsept\",\"doit\",\"doivent\",\"donc\",\"dont\",\"douze\",\"douzième\",\"dring\",\"du\",\"duquel\",\"durant\",\"e\",\"effet\",\"eh\",\"elle\",\"elle-même\",\"elles\",\"ellesmêmes\",\"en\",\"encore\",\"entre\",\"envers\",\"environ\",\"es\",\"ès\",\"est\",\"et\",\"etant\",\"étaient\",\"étais\",\"était\",\"étant\",\"etc\",\"été\",\"etre\",\"être\",\"eu\",\"euh\",\"eux\",\"eux-mêmes\",\"excepté\",\"f\",\"façon\",\"fais\",\"faisaient\",\"faisant\",\"fait\",\"feront\",\"fi\",\"flac\",\"floc\",\"font\",\"g\",\"gens\",\"h\",\"ha\",\"hé\",\"hein\",\"hélas\",\"hem\",\"hep\",\"hi\",\"ho\",\"holà\",\"hop\",\"hormis\",\"hors\",\"hou\",\"houp\",\"hue\",\"hui\",\"huit\",\"huitième\",\"hum\",\"hurrah\",\"i\",\"il\",\"ils\",\"importe\",\"j\",\"je\",\"jusqu\",\"jusque\",\"k\",\"l\",\"la\",\"là\",\"laquelle\",\"las\",\"le\",\"lequel\",\"les\",\"lès\",\"lesquelles\",\"lesquels\",\"leur\",\"leurs\",\"longtemps\",\"lorsque\",\"lui\",\"lui-même\",\"m\",\"ma\",\"maint\",\"mais\",\"malgré\",\"me\",\"même\",\"mêmes\",\"merci\",\"mes\",\"mien\",\"mienne\",\"miennes\",\"miens\",\"mille\",\"mince\",\"moi\",\"moi-même\",\"moins\",\"mon\",\"moyennant\",\"n\",\"na\",\"ne\",\"néanmoins\",\"neuf\",\"neuvième\",\"ni\",\"nombreuses\",\"nombreux\",\"non\",\"nos\",\"notre\",\"nôtre\",\"nôtres\",\"nous\",\"nous-mêmes\",\"nul\",\"o\",\"o|\",\"ô\",\"oh\",\"ohé\",\"olé\",\"ollé\",\"on\",\"ont\",\"onze\",\"onzième\",\"ore\",\"ou\",\"où\",\"ouf\",\"ouias\",\"oust\",\"ouste\",\"outre\",\"p\",\"paf\",\"pan\",\"par\",\"parmi\",\"partant\",\"particulier\",\"particulière\",\"particulièrement\",\"pas\",\"passé\",\"pendant\",\"personne\",\"peu\",\"peut\",\"peuvent\",\"peux\",\"pff\",\"pfft\",\"pfut\",\"pif\",\"plein\",\"plouf\",\"plus\",\"plusieurs\",\"plutôt\",\"pouah\",\"pour\",\"pourquoi\",\"premier\",\"première\",\"premièrement\",\"près\",\"proche\",\"psitt\",\"puisque\",\"q\",\"qu\",\"quand\",\"quant\",\"quanta\",\"quant-à-soi\",\"quarante\",\"quatorze\",\"quatre\",\"quatre- vingt\",\"quatrième\",\"quatrièmement\",\"que\",\"quel\",\"quelconque\",\"quelle\",\"quelles\",\"quelque\",\"quelques\",\"quelqu'un\",\"quels\",\"qui\",\"quiconque\",\"quinze\",\"quoi\",\"quoique\",\"r\",\"revoici\",\"revoilà\",\"rien\",\"s\",\"sa\",\"sacrebleu\",\"sans\",\"sapristi\",\"sauf\",\"se\",\"seize\",\"selon\",\"sept\",\"septième\",\"sera\",\"seront\",\"ses\",\"si\",\"sien\",\"sienne\",\"siennes\",\"siens\",\"sinon\",\"six\",\"sixième\",\"soi\",\"soi-même\",\"soit\",\"soixante\",\"son\",\"sont\",\"sous\",\"stop\",\"suis\",\"suivant\",\"sur\",\"surtout\",\"t\",\"ta\",\"tac\",\"tant\",\"te\",\"té\",\"tel\",\"telle\",\"tellement\",\"telles\",\"tels\",\"tenant\",\"tes\",\"tic\",\"tien\",\"tienne\",\"tiennes\",\"tiens\",\"toc\",\"toi\",\"toi-même\",\"ton\",\"touchant\",\"toujours\",\"tous\",\"tout\",\"toute\",\"toutes\",\"treize\",\"trente\",\"très\",\"trois\",\"troisième\",\"troisièmement\",\"trop\",\"tsoin\",\"tsouin\",\"tu\",\"u\",\"un\",\"une\",\"unes\",\"uns\",\"v\",\"va\",\"vais\",\"vas\",\"vé\",\"vers\",\"via\",\"vif\",\"vifs\",\"vingt\",\"vivat\",\"vive\",\"vives\",\"vlan\",\"voici\",\"voilà\",\"vont\",\"vos\",\"votre\",\"vôtre\",\"vôtres\",\"vous\",\"vous-mêmes\",\"vu\",\"w\",\"x\",\"y\",\"z\",\"zut\"]\n",
    "    \n",
    "    return geopoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_html_pages_in_dirs_and_extract_mails_dict(pages_dir):\n",
    "    dict_mails={}\n",
    "    #rootdir=\"./DHFR_sample/\"\n",
    "    count_Cc=0\n",
    "    count_message=0\n",
    "    for subdir, dirs, files in os.walk(pages_dir):\n",
    "#ON VA LIRE TOUS LES FICHIERS DU REP ROOTDIR\n",
    "        \n",
    "        for file in files:\n",
    "    #V1     if file.endswith('.html') == True  and file != (\"index.html\") and not re.match(\"mail\\d+.html\",file) : \n",
    "           if re.match(\"msg\\d+.html\",file) :\n",
    "          \n",
    "            # les index.html sont des recap, on pourrait les compter pour s'assurer du nombre de messages\n",
    "                \n",
    "                #!print \"\\n\",os.path.join(subdir, file), \"\\n\"\n",
    "                champs_mail={}\n",
    "                filename=os.path.join(subdir, file)\n",
    "                champs_mail[\"Geo_Topic\"]=read_html_and_testtextextract(filename)\n",
    "                \n",
    "                \n",
    "                soup=BeautifulSoup(open(filename, 'r').read(),'lxml')#, 'html.parser') \n",
    "#RECUPERER LE SUJET DU MESSAGE\n",
    "                \n",
    "                i=0\n",
    "                for zone_cible in soup.findAll('ul'):\n",
    "                    i+=1\n",
    "                    sujet_messg = zone_cible.find(string=re.compile(\"\\[DH\\]\"))\n",
    "                    zone_de_metadonnees = None\n",
    "                    special_auteur=0\n",
    "##ON ISOLE LE CHAMPS AUTEUR\n",
    "                    if '<li><strong>From</strong>:' in str(zone_cible) and '<li><strong>To</strong>:' in str(zone_cible) :\n",
    "                        #print \"FROM/TO DETECTED, i= \",i\n",
    "                        count_message+=1\n",
    "                        #print  zone_cible\n",
    "                        nom_auteur= re.sub(r'<ul>\\n<li><strong>From</strong>: ', '',str(zone_cible))\n",
    "                        sep = \"&lt;\"\n",
    "                        nom_auteur = nom_auteur.split(sep, 1)[0]\n",
    "                        \n",
    "                        \n",
    "                        mail1= re.sub(r'<ul>\\n<li><strong>From</strong>:.+\\n.+\\n.+\\n document\\.write\\(\\\"', '',str(zone_cible))\n",
    "                        mail2=re.sub(r'<ul>\\n<li><strong>From</strong>:.+\\n.+\\n.+\\n document\\.write\\(\\\".+@\\\" \\+ \\\"', '',str(zone_cible))\n",
    "                        sep1 = \"\\\" + \\\"@\\\" + \"\n",
    "                        sep2 = \"\\\")\"\n",
    "                        mail1= mail1.split(sep1, 1)[0]\n",
    "                        mail2= mail2.split(sep2, 1)[0]\n",
    "                        mail_auteur= mail1+\"@\"+mail2\n",
    "                        #print mail_auteur\n",
    "                        if len(mail_auteur)>60:\n",
    "                            try:\n",
    "                                mail= re.sub(r'<ul>\\n<li><strong>From</strong>:', '',str(zone_cible))\n",
    "                                mail=re.search('&lt;(.*)&gt;',str(mail))\n",
    "                                mail=mail.group(1)\n",
    "                                #print \"MAILBIS\",mail\n",
    "                                mail_auteur=mail\n",
    "                                #special_auteur=1\n",
    "                            except:\n",
    "                                print \"BUG\"\n",
    "                        #CHECK PROPER CAPTURE OF THE NAME    \n",
    "                        if len(nom_auteur)>50:\n",
    "                            #print zone_cible\n",
    "                            nom_auteur=mail_auteur\n",
    "                        if len(nom_auteur)<5:\n",
    "                            nom_auteur=mail_auteur\n",
    "                        if nom_auteur[0]== \"\\\"\":\n",
    "                            nom_auteur=nom_auteur[1:]\n",
    "                            nom_auteur=nom_auteur[:-2]\n",
    "                            print nom_auteur\n",
    "                        if nom_auteur.endswith(' '):\n",
    "                            nom_auteur=nom_auteur[:-1]\n",
    "                        #!print \"nom_auteur \",nom_auteur+\" mail_auteur \"+mail_auteur,\"\\n\"\n",
    "                        champs_mail[\"nom_auteur\"]=  nom_auteur\n",
    "                        champs_mail[\"mail_auteur\"]=  mail_auteur\n",
    "                        champs_mail[\"ref_physique_de_l_article\"]=  os.path.join(subdir, file)\n",
    "                        champs_mail[\"sujet_du_message\"] = sujet_messg\n",
    "##ON ISOLE LE CHAMP DESTINATAIRES\n",
    "                        #if special_auteur == 1:\n",
    "                            #print \"ZCspecAut\",zone_cible\n",
    "                        #else: \n",
    "                        \n",
    "                        destinataires=re.sub(r'<ul>\\n<li><strong>From</strong>:.+\\n.+\\n.+\\n.+\\n.+\\n.+\\n.+\\n', '',str(zone_cible))\n",
    "                        \n",
    "                        sep3=\";</li>\"\n",
    "                        destinataires= destinataires.split(sep3, 1)[0]\n",
    "                        #print destinataires\n",
    "                        dest=re.findall(\"<script type=.*\\n.+\\n.+\\n.+\\n.+/script>\", destinataires)\n",
    "                        #print dest\n",
    "                        liste_destinataires=[]\n",
    "                        for destinataire in dest:\n",
    "                            #print destinataire,\"\\n___________________\\n\"\n",
    "                            mail1= re.sub(r'<script type=\\\"text/javascript\\\">\\n <!-- \\n document\\.write\\(\\\"', '',str(destinataire))\n",
    "                            mail2=re.sub(r'<script type=\\\"text/javascript\\\">\\n <!-- \\n document\\.write\\(\\\".+@\\\" \\+ \\\"', '',str(destinataire))\n",
    "                            sep1 = \"\\\" + \\\"@\\\" + \"\n",
    "                            sep2 = \"\\\")\"\n",
    "                            mail1= mail1.split(sep1, 1)[0]\n",
    "                            mail2= mail2.split(sep2, 1)[0]\n",
    "                            mail_destinataire= mail1+\"@\"+mail2\n",
    "                            #print mail_destinataire,\"\\n___________\\n\"\n",
    "                            if mail_destinataire not in liste_destinataires :\n",
    "                                liste_destinataires.append(mail_destinataire)\n",
    "                        if liste_destinataires==[]:\n",
    "                            #print zone_cible\n",
    "                            liste_destinataires=['dh@groupes.renater.fr']\n",
    "                        #!print \"destinataires: \",liste_destinataires,\"\\n\"\n",
    "                        champs_mail[\"liste_mails_dests\"] = liste_destinataires\n",
    "                       \n",
    "                        if '<li><strong>Cc</strong>:' in str(zone_cible):\n",
    "##ON ISOLE LE CHAMP CC\n",
    "                            #print \"CC DETECTED, i= \",i\n",
    "                            #print zone_cible\n",
    "                            count_Cc+=1\n",
    "                            CCfield= re.search('<li><strong>Cc</strong>:(.*)<li><strong>Subject</strong>',str(zone_cible),flags=re.DOTALL)\n",
    "                            CC= CCfield.group(1)\n",
    "                            CC=str(CC)\n",
    "                            CC=re.sub(r'^.+\\n','',str(CC),flags=0)\n",
    "                            CCs=re.findall(\"<script type=.*\\n.+\\n.+\\n.+\\n.+/script>\", CC)\n",
    "                            #sep1 = \"\\\" + \\\"@\\\" + \"\n",
    "                            #mail1= mail1.split(sep1, 1)[0]\n",
    "                            #print \"CC FIELd\\n\"#,CCs\n",
    "                            liste_CCs=[]\n",
    "                            for CC in CCs:\n",
    "                                #print CC,\"\\n___________________\\n\"\n",
    "                                mail1= re.sub(r'<script type=\\\"text/javascript\\\">\\n <!-- \\n document\\.write\\(\\\"', '',str(CC))\n",
    "                                mail2=re.sub(r'<script type=\\\"text/javascript\\\">\\n <!-- \\n document\\.write\\(\\\".+@\\\" \\+ \\\"', '',str(CC))\n",
    "                                sep1 = \"\\\" + \\\"@\\\" + \"\n",
    "                                sep2 = \"\\\")\"\n",
    "                                mail1= mail1.split(sep1, 1)[0]\n",
    "                                mail2= mail2.split(sep2, 1)[0]\n",
    "                                mail_CC= mail1+\"@\"+mail2\n",
    "                                #print mail_CC,\"\\n___________\\n\"\n",
    "                                if mail_CC not in liste_CCs :\n",
    "                                    liste_CCs.append(mail_CC)\n",
    "                            #!print \"CCs: \",liste_CCs\n",
    "                            champs_mail[\"liste_CCs\"] = liste_CCs\n",
    "##ON ISOLE LE CHAMP DATE                       \n",
    "                        #print \"ZONECIBLE\",zone_cible\n",
    "                        champs_datef=re.search('<li><strong>Date</strong>:(.+?)</li>',str(zone_cible),flags=re.DOTALL)\n",
    "                        #print \"CHAMPSDATEF\",champs_datef\n",
    "                        try :\n",
    "                            champs_date=champs_datef.group(1)\n",
    "                        except :\n",
    "                            #print \"ZCparent\", zone_cible.parent\n",
    "                            champs_date=re.search('<!--X-Date: (.*) -->',str(soup))\n",
    "                            champs_date=champs_date.group(1)\n",
    "                            #print champs_date\n",
    "                            \n",
    "                        #print champs_date\n",
    "                        try : \n",
    "                            champs_date=parser.parse(champs_date)\n",
    "                        except ValueError :\n",
    "                            sep=\" (\"\n",
    "                            champs_date=champs_date.split(sep, 1)[0]\n",
    "                            \n",
    "                        #!print  champs_date\n",
    "                        champs_mail[\"date\"] = champs_date\n",
    "                        \n",
    "                        \n",
    "                        dict_mails[os.path.join(subdir, file)]= champs_mail\n",
    "                        break\n",
    "##LE DICTIONNAIRE DES MAILS SE NOMME dict_mails\n",
    "    #print dict_mails\n",
    "    print count_message,\" messages\"\n",
    "    print count_Cc, \" messages avec Cc\"\n",
    "    ratiocount = float(count_Cc)/float(count_message)\n",
    "    print \"Taux_de_mails_avec_CC:\",ratiocount,\"\\n\"\n",
    "    return dict_mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus_and_extract_topo_network(corpus):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus_and_extract_topo_Geo_network(corpus):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus_and_extract_mails(corpus):\n",
    "    liste_mails=[]\n",
    "    for mesg in corpus:\n",
    "        #print corpus[mesg]\n",
    "        if corpus[mesg]['mail_auteur'] not in liste_mails:\n",
    "            liste_mails.append(corpus[mesg]['mail_auteur'])\n",
    "        for i,key in enumerate(corpus[mesg]['liste_mails_dests']):\n",
    "            #print key\n",
    "            if key not in liste_mails:\n",
    "                liste_mails.append(key)\n",
    "        if 'liste_CCs' in corpus[mesg] :\n",
    "            #print \"TRUVE CC\"\n",
    "            for i,key in enumerate(corpus[mesg]['liste_CCs']):\n",
    "                #print key\n",
    "                if key not in liste_mails:\n",
    "                    liste_mails.append(key)\n",
    "    return liste_mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus_and_extract_localisations(corpus):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus_and_extract_names(corpus):\n",
    "    liste_noms=[]\n",
    "    for mesg in corpus:\n",
    "        try: \n",
    "            if corpus[mesg]['nom_auteur'][0]== \"\\\"\":\n",
    "                print corpus[mesg]['nom_auteur']\n",
    "                \n",
    "        except IndexError:\n",
    "            print \"Bug\", corpus[mesg]['nom_auteur']\n",
    "            print corpus[mesg]\n",
    "        if [corpus[mesg]['nom_auteur'],corpus[mesg]['mail_auteur']] not in liste_noms:\n",
    "            for i,key in enumerate(liste_noms):\n",
    "                if SequenceMatcher(None, corpus[mesg]['nom_auteur'], key[0]).ratio() >=0.9:\n",
    "                    print \"POSSIBLE DUPLICATE |\", corpus[mesg]['nom_auteur'],\"|   |\",key[0],\"|\"\n",
    "                    print corpus[mesg]['mail_auteur'], key[1]\n",
    "                    \n",
    "                \n",
    "            liste_noms.append([corpus[mesg]['nom_auteur'],corpus[mesg]['mail_auteur']])\n",
    "        \n",
    "    return liste_noms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus_and_extract_names_2(corpus):\n",
    "    dict_noms={}\n",
    "    for mesg in corpus:\n",
    "        try: \n",
    "            if corpus[mesg]['nom_auteur'][0]== \"\\\"\":\n",
    "                print corpus[mesg]['nom_auteur']\n",
    "                \n",
    "        except IndexError:\n",
    "            print \"Bug\", corpus[mesg]['nom_auteur']\n",
    "            print corpus[mesg]\n",
    "            #LE NOM D AUTEUR N EST IL PAS  DANS NOTRE TABLEAU?\n",
    "        if corpus[mesg]['nom_auteur'] not in liste_noms[:,0]:\n",
    "            for i,key in enumerate(liste_noms):\n",
    "            #LE NOM D AUTEUR A T IL LEGEREMENT VARIE?\n",
    "                if SequenceMatcher(None, corpus[mesg]['nom_auteur'], key[0]).ratio() >=0.9:\n",
    "                    print \"POSSIBLE DUPLICATE |\", corpus[mesg]['nom_auteur'],\"|   |\",key[0],\"|\"\n",
    "                    print corpus[mesg]['mail_auteur'], key[1][0]\n",
    "                    print corpus[mesg]['date'], key[1][1]\n",
    "                else:\n",
    "                    liste_noms.append([corpus[mesg]['nom_auteur'],[corpus[mesg]['mail_auteur'],corpus[mesg]['date']])\n",
    "        else: \n",
    "    return liste_noms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Noiret, Serge\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Noiret, Serge\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Ghislain SILLAUME\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Noiret, Serge\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['spatiale.Cordialement,Elina', 'spatiale', 'spatiale', 'g\\xc3\\xa9omatique,', 'g\\xc3\\xa9ographique,', 'spatiale', 'spatiale', 'spatiales', 'spatiale,'] \n",
      " points :  9\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['urbanisme,', 'g\\xc3\\xa9ographie,'] \n",
      " points :  2\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "aude.da-cruz-lima\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie)'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographiques'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['urbanisation,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographes,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Wandl-Vogt, Eveline\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Wandl-Vogt, Eveline\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['spatiales'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "elisabeth.belmas\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographique\\xc2\\xa0:'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Viera Rebolledo-Dhuin\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographique\\xc2\\xa0:'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographiques'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['spatiales', '&quot;g\\xc3\\xa9omatique&quot;'] \n",
      " points :  2\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['spatial'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Antonio A. Casilli\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Anne Gresillon\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['urbanisation', 'urbaniste'] \n",
      " points :  2\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie.Pour'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['geography,'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Antonio A. Casilli\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Epler, Jakob\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographiques'] \n",
      " points :  1\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "['g\\xc3\\xa9omaticiens', 'g\\xc3\\xa9omatique'] \n",
      " points :  2\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Noiret, Serge\n",
      "SUJETS\n",
      "['g\\xc3\\xa9ographie,', 'g\\xc3\\xa9ographique'] \n",
      " points :  2\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "SUJETS\n",
      "[] \n",
      " points :  0\n",
      "Manuel Zacklad\n",
      "294  messages\n",
      "35  messages avec Cc\n",
      "Taux_de_mails_avec_CC: 0.119047619048 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_mails_DH=read_html_pages_in_dirs_and_extract_mails_dict(\"./DHFRsample\")\n",
    "#print corpus_mails_DH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282\n",
      "['jorge.fins@univ-tours.fr', 'dh@groupes.renater.fr', 'dumouchelsuzanne@yahoo.fr', 'legram@yahoogroupes.fr', 'culture.numerique@ml.free.fr', 'listesocius@groups.openedition.org', 'antoine.courtin@mac.com', 'ontologie-patrimoine@services.cnrs.fr', 'laetitia.bontemps@univ-tours.fr', 'DH@cru.fr', 'stephane.pouyllau@huma-num.fr', 'georges-xavier.blary@unilim.fr', 'Eveline.Wandl-Vogt@oeaw.ac.at', 'exploreAT@oeaw.ac.at', 'antonio.casilli@ehess.fr', 'frederic@clavert.net', 'antoine.blanchard@gmail.com', 'Nicole.Dufournaud@laposte.net', 'marjorie.burghart@ehess.fr', 'jean-daniel.fekete@inria.fr', 'pabloruizfabo@gmail.com', 'dh@cru.fr', 'oledeuff@gmail.com', 'pierre.mounier@openedition.org', 'claire.clivaz@unil.ch', 'christine.chadier@univ-lyon3.fr', 'aude.da-cruz-lima@mae.u-paris10.fr', 'archives-son-audiovisuel@groups.openedition.org', 'jerome.valluy@univ-paris1.fr', 'alexandre.hocquet@univ-lorraine.fr', 'annael.le-poullennec@psl.eu', 'elenagonzalezblanco@yahoo.es', 'tei-l@listserv.brown.edu', 'humanist@lists.digitalhumanities.org', 'digitalclassicist@jiscmail.ac.uk', 'dm-l@uleth.ca', 'dhd@mailman.rrz.uni-hamburg.de', 'globaloutlookdh-l@uleth.ca', 'iwetel@listserv.rediris.es', 'aiucd-l@humnet.unipi.it', 'air-l@aoir.org', 'institute@lists.uvic.ca', 'INFOLING@LISTSERV.REDIRIS.ES', 'infoling@listserv.rediris.es', 'humanist-l@uleth.ca', 'redhd@humanidadesdigitales.net', 'centernet@lists.digitalhumanities.org', 'dhcarolina@listserv.unc.edu', 'website@hastac.org', 'humanidadesdigitais@gmail.com', 'svhoolan@ulb.ac.be', 'jbcamps@hotmail.com', 'thibault.clerice@enc-sorbonne.fr', 'iglouvremb@gmail.com', 'adeline.joffres@huma-num.fr', 'jerome.darmont@univ-lyon2.fr', 'madics-adoc@listes.univ-lyon2.fr', 'annonces@madics.fr', 'fil-tmd@groupes.renater.fr', 'liste-egc@polytech.univ-nantes.fr', 'eda-liste@listes.univ-lyon2.fr', 'gazettebd3@imag.fr', 'bull-i3@irit.fr', 'secardinolivier@yahoo.fr', 'adherents-aipu-fr@groupes.renater.fr', 'afec-info@groupes.renater.fr', 'anstia-adherents@groupes.renater.fr', 'debuter-en-dh@groupes.renater.fr', 'emploi-fle@groupes.renater.fr', 'laurens@ehess.fr', 'listes@ahicf.com', 'histoire_eco-request@groupes.renater.fr', 'athena@services.cnrs.fr', 'archives-fr@yahoogroupes.fr', 'nep-his@lists.repec.org', 'medici@listes.huma-num.fr', 'ferinterfrance@googlegroups.com', 'enrico.natale@infoclio.ch', 'pierre.mounier@ehess.fr', 'stephane.pouyllau@cnrs.fr', 'Francoise.Blum@univ-paris1.fr', 'Serge.Noiret@EUI.eu', 'manuel.zacklad@cnam.fr', 'Mathieu.Andro@versailles.inra.fr', 'lisa.chupin@gmail.com', 'marionlame@gmail.com', 'marin.dacos@revues.org', 'eric.kergosien@univ-lille3.fr', 'info-ic@listes.irisa.fr', 'ln@cines.fr', 'magis@imag.fr', 'inforsid@listes.insa-lyon.fr', 'stefanev@club-internet.fr', 'laurence.rageot@univ-tours.fr', 'olivier.marlet@univ-tours.fr', 'delegue.general@adbu.fr', 'helene.coste@univ-lehavre.fr', 'valerie.neouze@parisdescartes.fr', 'beatrice.markhoff@univ-tours.fr', 'clarisse_bardiot@mac.com', 'Eric.Guichard@enssib.fr', 'pierazzo@gmail.com', 'vial.stephane@gmail.com', 'recherche-design@listes.univ-paris1.fr', 'projekt-membres@liste.unimes.fr', 'projekt@unimes.fr', 'info@design-fax.fr', 'theuth@listes.univ-rennes1.fr', 'martaseve@gmail.com', 'elodie.faath@openedition.org', 'martin.grandjean@unil.ch', 'sclerisse@parisnanterre.fr', 'slh@ens-lyon.fr', 'txm-users@groupes.renater.fr', 'laurent.romary@inria.fr', 'ncasanova@mmsh.univ-aix.fr', 'clarisse_bardiot@me.com', 'MKoenig@dhi-paris.fr', 'benoit.epron@enssib.fr', 'jakob.epler@dariah.eu', 'claire.clivaz@sib.swiss', 'gregory.grefenstette@inria.fr', 'frederic.clavert@uni.lu', 'muriel.foulonneau@gmail.com', 'recherche.coordination@bnf.fr', 'dh-request@groupes.renater.fr', 'formation.continue@enc-sorbonne.fr', 'cecile.boulaire@univ-tours.fr', 'tei-fr@groupes.renater.fr', 'christine.benevent@univ-tours.fr', 'laurent.gerbier@univ-tours.fr', 'sandrine.breuil@univ-tours.fr', 'delphine.cavallo@univ-amu.fr', 'francesco.beretta@ish-lyon.cnrs.fr', 'florence.andreacola@univ-grenoble-alpes.fr', 'm.bourgatte@icp.fr', 'johann.holland@campus-condorcet.fr', 'dominique.stutzmann@irht.cnrs.fr', 'Nicolas.Larrousse@huma-num.fr', 'marie.e.lescasse@gmail.com', 'michel.bernard@univ-paris3.fr', 'stamkou@free.fr', 'francoise.paquienseguy@sciencespo-lyon.fr', 'myriam.tazi@sciencespo.fr', 'aurelien.berra@gmail.com', 'anne-laure.brisac@inha.fr', 'spadinielena@gmail.com', 'TEXTUALSCHOLARSHIP@jiscmail.ac.uk', 'marin.dacos@openedition.org', 'sylvain.laube@univ-brest.fr', 'anne.baillot@gmail.com', 'sarah.cadorel@sciencespo.fr', 'cecile.soudan@ehess.fr', 'jea.herzog@gmail.com', 'vanessa.juloux@ephe.sorbonne.fr', 'aurelie.olivesi@wanadoo.fr', 'diffusion@listes.ancmsp.com', 'efigies-info@rezo.net', 'etudesfeministes-l@simone.univ-tlse2.fr', 'aurelie.olivesi@univ-lyon1.fr', 'aamonnz@gmail.com', 'beauguittelaurent@hotmail.com', 'geotamtam@unil.ch', 'irihs@univ-rouen.fr', 'e.salvatori@mediev.unipi.it', 'huyghe.marie@gmail.com', 'laurent.cailly@univ-tours.fr', 'adbs-info@listes.adbs.fr', 'accesouvert@groupes.renater.fr', 'docs-ri@yahoogroupes.fr', 'doccitanist@services.cnrs.fr', 'info-aria@lsis.org', 'marie-laure.massot@ens.fr', 'Marie-Laure.Massot@ens.fr', 'thierry.poibeau@ens.fr', 'Rene.Audet@lit.ulaval.ca', 'corpus-ecrits@cru.fr', 'cahier@groupes.renater.fr', 'cecile.rodrigues@cnrs.fr', 'gerald.kembellec@lecnam.net', 'francois.bavaud@unil.ch', 'llist@unil.ch', 'southasia-dh@lists.globaloutlookdh.org', 'air-l@listserv.aoir.org', 'humanisticadh@gmail.com', 'ahdig@googlegroups.com', 'textualscholarship@jiscmail.ac.uk', 'Guillaume.Blum@design.ulaval.ca', 'liste-proml@lri.fr', 'elinaleblanc3007@gmail.com', 'inatheque@ina.fr', 'mahe.annaig@wanadoo.fr', 'florence.clavaud@free.fr', 'valerie.beaugiraud@ens-lyon.fr', 'jouni.tuominen@aalto.fi', 'antonio.casilli@googlemail.com', 'claire.lemercier@sciencespo.fr', 'jahjah.marc@gmail.com', 'pascal.cristofoli@ehess.fr', 'projet.iglouvre@gmail.com', 'fatihaidmhand@yahoo.es', 'nicolasthelyrennes2@gmail.com', 'litor@listes.univ-paris3.fr', 'numeruniv-quotidien@cines.fr', 'marta.materni@gmail.com', 'isabelle.thiebau@univ-lille2.fr', 'Christine.michel@insa-lyon.fr', 'cerisier@univ-poitiers.fr', 'lisette.calderan@inria.fr', 'remi.jimenes@gmail.com', 'joel.marchand@huma-num.fr', 'arno.zucker@gmail.com', 'stephane.loret@univ-nantes.fr', 'michaelesinatra@gmail.com', 'emmanuelle.duwez@sciencespo.fr', 'amel.fraisse@univ-lille3.fr', 'gpansu@gmail.com', 'francois.theron@uvsq.fr', 'caroline.cance@univ-orleans.fr', 'raphaelle.bour@irit.fr', 'caroline.rossi@univ-grenoble-alpes.fr', 'audrey.baneyx@sciencespo.fr', 'sandrine.clerisse@cnrs.fr', 'pe.barrault@gmail.com', 'digit_hum@ens.fr', 'stephane.lamasse@univ-paris1.fr', 'julia.bonaccorsi@univ-lyon2.fr', 'tei-fr@cru.fr', 'nicolas.legrand@obspm.fr', 'Frederic.Clavert@unil.ch', 'Daniel.Stoekl@ephe.sorbonne.fr', 'mehdi.khamassi@upmc.fr', 'sabine.loudcher@univ-lyon2.fr', 'sarah.cordonnier@gmail.com', 'florent.laroche@ec-nantes.fr', 'maud.ingarao@ens-lyon.fr', 'schassan@hab.de', 'c.schoech@gmail.com', 'gerald.kembellec@cnam.fr', 'elisabeth.belmas@wanadoo.fr', 'gresillon@cmb.hu-berlin.de', 'caroline.muller@univ-reims.fr', 'clemence.jacquot@gmail.com', 'emmanuelle.morlock@gmail.com', 'quanti@groupes.renater.fr', 'rbdd@services.cnrs.fr', 'yosra.ghliss17@gmail.com', 'nadine.dardenne@cnrs.fr', 'emilien.ruiz@ehess.fr', 'camillemonnier33@hotmail.com', 'dbernhard@unistra.fr', 'ottaviano.nancy@gmail.com', 'jouni.tuominen@helsinki.fi', 'cynthia.pedroja@meshs.fr', 'olfa.lamloum@gmail.com', 'paul.girard@sciencespo.fr', 'marie-noelle.polino@ahicf.com', 'colette.cadiou@irstea.fr', 'yamina.bensaadoune@univ-rouen.fr', 'carmen.brando@gmail.com', 'francesca.frontini@univ-montp3.fr', 'ghislain.sillaume@cvce.eu', 'aurelie.olivesi@gmail.com', 'amelie.vairelles@sciencespo.fr', 'alexandre.moatti@mines.org', 'emmanuelguez@yahoo.fr', 'graham.ranger@univ-avignon.fr', 'corpora@uib.no', 'saes@univ-pau.fr', 'alaes_liste@yahoogroupes.fr', 'parislinguists@yahoogroupes.fr', 'agorantic@listes.univ-avignon.fr', 'jf.omhover@histographe.com', 'richard.walter@ens.fr', 'espejosuros.javier@gmail.com', 'viera.rebolledodhuin@free.fr', 'judith.hannoun@univ-amu.fr', 'raphaelle.krummeich@univ-rouen.fr', 'nicolas.thely@univ-rennes2.fr', 'hdefouca@u-paris10.fr', 'humanum-diffusion@listes.huma-num.fr', 'humanum-veille@listes.huma-num.fr']\n"
     ]
    }
   ],
   "source": [
    "liste_mails=read_corpus_and_extract_mails(corpus_mails_DH)\n",
    "print len(liste_mails)\n",
    "print liste_mails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSSIBLE DUPLICATE | Pierre Mounier |   | Pierre Mounier |\n",
      "pierre.mounier@openedition.org pierre.mounier@ehess.fr\n",
      "2013-07-05 10:09:24+02:00\n",
      "POSSIBLE DUPLICATE | Clarisse Bardiot |   | Clarisse Bardiot |\n",
      "clarisse_bardiot@me.com clarisse_bardiot@mac.com\n",
      "2015-07-09 14:46:14+02:00\n",
      "POSSIBLE DUPLICATE | Claire Clivaz |   | Claire Clivaz |\n",
      "claire.clivaz@unil.ch claire.clivaz@sib.swiss\n",
      "2014-07-04 22:10:57+02:00\n",
      "POSSIBLE DUPLICATE | Stéphane Pouyllau |   | Stéphane Pouyllau |\n",
      "stephane.pouyllau@cnrs.fr stephane.pouyllau@huma-num.fr\n",
      "2013-07-18 11:56:40+02:00\n",
      "POSSIBLE DUPLICATE | Marin Dacos |   | Marin Dacos |\n",
      "marin.dacos@openedition.org marin.dacos@revues.org\n",
      "2012-07-25 13:34:30+01:00\n",
      "POSSIBLE DUPLICATE | Antonio Casilli |   | Antonio A. Casilli |\n",
      "antonio.casilli@googlemail.com antonio.casilli@ehess.fr\n",
      "2014-07-07 11:23:08+02:00\n",
      "POSSIBLE DUPLICATE | Sandrine Clérisse |   | Sandrine Clérisse |\n",
      "sandrine.clerisse@cnrs.fr sclerisse@parisnanterre.fr\n",
      "2018-03-21 10:40:36+01:00\n",
      "POSSIBLE DUPLICATE | Frédéric Clavert |   | Frédéric Clavert |\n",
      "Frederic.Clavert@unil.ch frederic@clavert.net\n",
      "2016-07-07 13:11:56+00:00\n",
      "POSSIBLE DUPLICATE | Jouni Tuominen |   | Jouni Tuominen |\n",
      "jouni.tuominen@helsinki.fi jouni.tuominen@aalto.fi\n",
      "2017-06-01 20:00:43+03:00\n",
      "176\n",
      "[['Jorge Fins', 'jorge.fins@univ-tours.fr'], ['dumouchel suzanne', 'dumouchelsuzanne@yahoo.fr'], ['antoine Courtin', 'antoine.courtin@mac.com'], ['Laetitia Bontemps', 'laetitia.bontemps@univ-tours.fr'], ['St\\xc3\\xa9phane Pouyllau', 'stephane.pouyllau@huma-num.fr'], ['Georges-Xavier Blary', 'georges-xavier.blary@unilim.fr'], ['Wandl-Vogt, Eveline', 'Eveline.Wandl-Vogt@oeaw.ac.at'], ['Antonio A. Casilli', 'antonio.casilli@ehess.fr'], ['Fr\\xc3\\xa9d\\xc3\\xa9ric Clavert', 'frederic@clavert.net'], ['Antoine Blanchard', 'antoine.blanchard@gmail.com'], ['Pablo Ruiz', 'pabloruizfabo@gmail.com'], ['Olivier Le Deuff', 'oledeuff@gmail.com'], ['CHADIER Christine', 'christine.chadier@univ-lyon3.fr'], ['aude.da-cruz-lima', 'aude.da-cruz-lima@mae.u-paris10.fr'], ['jerome valluy', 'jerome.valluy@univ-paris1.fr'], ['Annael LE POULLENNEC', 'annael.le-poullennec@psl.eu'], ['Elena Gonz\\xc3\\xa1lez-Blanco', 'elenagonzalezblanco@yahoo.es'], ['Seth van Hooland', 'svhoolan@ulb.ac.be'], ['Jean-Baptiste CAMPS', 'jbcamps@hotmail.com'], ['BRUNET Mich\\xc3\\xa8le', 'iglouvremb@gmail.com'], ['Adeline Joffres', 'adeline.joffres@huma-num.fr'], ['J\\xc3\\xa9r\\xc3\\xb4me Darmont', 'jerome.darmont@univ-lyon2.fr'], ['Olivier secardin', 'secardinolivier@yahoo.fr'], ['Sylvain Laurens', 'laurens@ehess.fr'], ['Rails &amp; Histoire', 'listes@ahicf.com'], ['Enrico Natale', 'enrico.natale@infoclio.ch'], ['Pierre Mounier', 'pierre.mounier@ehess.fr'], ['Francoise Blum', 'Francoise.Blum@univ-paris1.fr'], ['Noiret, Serge', 'Serge.Noiret@EUI.eu'], ['Manuel Zacklad', 'manuel.zacklad@cnam.fr'], ['ekergosien', 'eric.kergosien@univ-lille3.fr'], ['stefanev', 'stefanev@club-internet.fr'], ['Laurence Rageot', 'laurence.rageot@univ-tours.fr'], ['delegue.general@adbu.fr', 'delegue.general@adbu.fr'], ['B\\xc3\\xa9atrice Markhoff', 'beatrice.markhoff@univ-tours.fr'], ['Marin Dacos', 'marin.dacos@revues.org'], ['Clarisse Bardiot', 'clarisse_bardiot@mac.com'], ['Eric Guichard', 'Eric.Guichard@enssib.fr'], ['Elena Pierazzo', 'pierazzo@gmail.com'], ['St\\xc3\\xa9phane Vial', 'vial.stephane@gmail.com'], ['Pierre Mounier', 'pierre.mounier@openedition.org'], ['Marta Severo', 'martaseve@gmail.com'], ['Elodie Faath', 'elodie.faath@openedition.org'], ['Martin Grandjean', 'martin.grandjean@unil.ch'], ['Sandrine Cl\\xc3\\xa9risse', 'sclerisse@parisnanterre.fr'], ['Serge Heiden', 'slh@ens-lyon.fr'], ['Laurent Romary', 'laurent.romary@inria.fr'], ['CASANOVA Nathalie', 'ncasanova@mmsh.univ-aix.fr'], ['Clarisse Bardiot', 'clarisse_bardiot@me.com'], ['Mareike Koenig', 'MKoenig@dhi-paris.fr'], ['Epron Beno\\xc3\\xaet', 'benoit.epron@enssib.fr'], ['Epler, Jakob', 'jakob.epler@dariah.eu'], ['Claire Clivaz', 'claire.clivaz@sib.swiss'], ['Claire Clivaz', 'claire.clivaz@unil.ch'], ['Fr\\xc3\\xa9d\\xc3\\xa9ric CLAVERT', 'frederic.clavert@uni.lu'], ['Muriel Foulonneau', 'muriel.foulonneau@gmail.com'], ['recherche.coordination@bnf.fr', 'recherche.coordination@bnf.fr'], ['Formation continue ENC', 'formation.continue@enc-sorbonne.fr'], ['C\\xc3\\xa9cile BOULAIRE', 'cecile.boulaire@univ-tours.fr'], ['CAVALLO Delphine', 'delphine.cavallo@univ-amu.fr'], ['Marjorie Burghart', 'marjorie.burghart@ehess.fr'], ['Francesco Beretta', 'francesco.beretta@ish-lyon.cnrs.fr'], ['Florence Andreacola', 'florence.andreacola@univ-grenoble-alpes.fr'], ['Michael Bourgatte', 'm.bourgatte@icp.fr'], ['Johann Holland', 'johann.holland@campus-condorcet.fr'], ['St\\xc3\\xa9phane Pouyllau', 'stephane.pouyllau@cnrs.fr'], ['Dominique Stutzmann', 'dominique.stutzmann@irht.cnrs.fr'], ['Nicolas Larrousse', 'Nicolas.Larrousse@huma-num.fr'], ['Marie-Eglantine Lescasse', 'marie.e.lescasse@gmail.com'], ['Michel Bernard', 'michel.bernard@univ-paris3.fr'], ['Sofia Papastamkou', 'stamkou@free.fr'], ['Paquien-S\\xc3\\xa9guy Fran\\xc3\\xa7oise\\t', 'francoise.paquienseguy@sciencespo-lyon.fr'], ['Myriam TAZI', 'myriam.tazi@sciencespo.fr'], ['Aur\\xc3\\xa9lien Berra', 'aurelien.berra@gmail.com'], ['Anne-Laure Brisac-Chra\\xc3\\xafbi', 'anne-laure.brisac@inha.fr'], ['Elena Spadini', 'spadinielena@gmail.com'], ['Marin Dacos', 'marin.dacos@openedition.org'], ['Laube Sylvain', 'sylvain.laube@univ-brest.fr'], ['Anne Baillot', 'anne.baillot@gmail.com'], ['Sarah Cadorel', 'sarah.cadorel@sciencespo.fr'], ['Soudan Cecile', 'cecile.soudan@ehess.fr'], ['Jeanne Herzog', 'jea.herzog@gmail.com'], ['Vanessa Juloux', 'vanessa.juloux@ephe.sorbonne.fr'], ['Aurelie OLIVESI', 'aurelie.olivesi@wanadoo.fr'], ['Alexandre Monnin', 'aamonnz@gmail.com'], ['beauguitte laurent', 'beauguittelaurent@hotmail.com'], ['IRIHS', 'irihs@univ-rouen.fr'], ['Enrica Salvatori', 'e.salvatori@mediev.unipi.it'], ['Marie-laure Massot', 'marie-laure.massot@ens.fr'], ['Thierry Poibeau', 'thierry.poibeau@ens.fr'], ['Ren\\xc3\\xa9 Audet', 'Rene.Audet@lit.ulaval.ca'], ['Cecile Rodrigues', 'cecile.rodrigues@cnrs.fr'], ['Alexandre Hocquet', 'alexandre.hocquet@univ-lorraine.fr'], ['G\\xc3\\xa9rald KEMBELLEC', 'gerald.kembellec@lecnam.net'], ['Fran\\xc3\\xa7ois Bavaud', 'francois.bavaud@unil.ch'], ['Guillaume Blum', 'Guillaume.Blum@design.ulaval.ca'], ['Elina Leblanc', 'elinaleblanc3007@gmail.com'], ['inatheque@ina.fr', 'inatheque@ina.fr'], ['Annaig Mahe', 'mahe.annaig@wanadoo.fr'], ['florence.clavaud@free.fr', 'florence.clavaud@free.fr'], ['Val\\xc3\\xa9rie Beaugiraud', 'valerie.beaugiraud@ens-lyon.fr'], ['Jouni Tuominen', 'jouni.tuominen@aalto.fi'], ['Burghart Marjorie', 'marjorie.burghart@ehess.fr'], ['Antonio Casilli', 'antonio.casilli@googlemail.com'], ['Mathieu Andro', 'Mathieu.Andro@versailles.inra.fr'], ['Claire Lemercier', 'claire.lemercier@sciencespo.fr'], ['marc jahjah', 'jahjah.marc@gmail.com'], ['Cristofoli Pascal', 'pascal.cristofoli@ehess.fr'], ['Equipe projet IGLouvre', 'projet.iglouvre@gmail.com'], ['Fatiha IDMHAND', 'fatihaidmhand@yahoo.es'], ['Nicolas Th\\xc3\\xa9ly', 'nicolasthelyrennes2@gmail.com'], ['marta materni', 'marta.materni@gmail.com'], ['isabelle.thiebau@univ-lille2.fr', 'isabelle.thiebau@univ-lille2.fr'], ['Christine Michel', 'Christine.michel@insa-lyon.fr'], ['Lisette Calderan', 'lisette.calderan@inria.fr'], ['R\\xc3\\xa9mi Jimenes', 'remi.jimenes@gmail.com'], ['Joel Marchand', 'joel.marchand@huma-num.fr'], ['arno.zucker@gmail.com', 'arno.zucker@gmail.com'], ['St\\xc3\\xa9phane Loret', 'stephane.loret@univ-nantes.fr'], ['Michael Sinatra', 'michaelesinatra@gmail.com'], ['Emmanuelle Duwez', 'emmanuelle.duwez@sciencespo.fr'], ['Amel Fraisse', 'amel.fraisse@univ-lille3.fr'], ['Gilles Pansu', 'gpansu@gmail.com'], ['Fran\\xc3\\xa7ois Th\\xc3\\xa9ron', 'francois.theron@uvsq.fr'], ['Caroline Cance', 'caroline.cance@univ-orleans.fr'], ['Rapha\\xc3\\xablle Bour', 'raphaelle.bour@irit.fr'], ['CAROLINE ROSSI', 'caroline.rossi@univ-grenoble-alpes.fr'], ['Audrey Baneyx - Sciences Po', 'audrey.baneyx@sciencespo.fr'], ['Sandrine Cl\\xc3\\xa9risse', 'sandrine.clerisse@cnrs.fr'], ['Pierre-Edouard Barrault', 'pe.barrault@gmail.com'], ['lamasse', 'stephane.lamasse@univ-paris1.fr'], ['Julia Bonaccorsi', 'julia.bonaccorsi@univ-lyon2.fr'], ['Nicolas Legrand', 'nicolas.legrand@obspm.fr'], ['Fr\\xc3\\xa9d\\xc3\\xa9ric Clavert', 'Frederic.Clavert@unil.ch'], ['Daniel Stoekl', 'Daniel.Stoekl@ephe.sorbonne.fr'], ['Mehdi Khamassi', 'mehdi.khamassi@upmc.fr'], ['Florent Laroche', 'florent.laroche@ec-nantes.fr'], ['Maud Ingarao', 'maud.ingarao@ens-lyon.fr'], ['G\\xc3\\xa9rald Kembellec', 'gerald.kembellec@cnam.fr'], ['elisabeth.belmas', 'elisabeth.belmas@wanadoo.fr'], ['Anne Gresillon', 'gresillon@cmb.hu-berlin.de'], ['Caroline Muller', 'caroline.muller@univ-reims.fr'], ['cl\\xc3\\xa9mence jacquot', 'clemence.jacquot@gmail.com'], ['Emmanuelle Morlock', 'emmanuelle.morlock@gmail.com'], ['Yosra Ghliss', 'yosra.ghliss17@gmail.com'], ['Nadine Dardenne', 'nadine.dardenne@cnrs.fr'], ['Emilien RUIZ', 'emilien.ruiz@ehess.fr'], ['camille monnier', 'camillemonnier33@hotmail.com'], ['dbernhard@unistra.fr', 'dbernhard@unistra.fr'], ['Nancy Ottaviano', 'ottaviano.nancy@gmail.com'], ['Jouni Tuominen', 'jouni.tuominen@helsinki.fi'], ['listes@ahicf.com', 'listes@ahicf.com'], ['Cynthia Pedroja', 'cynthia.pedroja@meshs.fr'], ['Olfa Lamloum', 'olfa.lamloum@gmail.com'], ['Paul Girard', 'paul.girard@sciencespo.fr'], ['marie-noelle.polino@ahicf.com', 'marie-noelle.polino@ahicf.com'], ['Cadiou Colette', 'colette.cadiou@irstea.fr'], ['YAMINA BENSAADOUNE', 'yamina.bensaadoune@univ-rouen.fr'], ['Sandrine Breuil', 'sandrine.breuil@univ-tours.fr'], ['Carmen Brando', 'carmen.brando@gmail.com'], ['Ghislain SILLAUME', 'ghislain.sillaume@cvce.eu'], ['Aur\\xc3\\xa9lie Olivesi', 'aurelie.olivesi@gmail.com'], ['Am\\xc3\\xa9lie VAIRELLES', 'amelie.vairelles@sciencespo.fr'], ['Alexandre Moatti', 'alexandre.moatti@mines.org'], ['emmanuel guez', 'emmanuelguez@yahoo.fr'], ['Graham Ranger', 'graham.ranger@univ-avignon.fr'], ['jf.omhover@histographe.com', 'jf.omhover@histographe.com'], ['Richard Walter', 'richard.walter@ens.fr'], ['Javier Espejo Sur\\xc3\\xb3s', 'espejosuros.javier@gmail.com'], ['Viera Rebolledo-Dhuin', 'viera.rebolledodhuin@free.fr'], ['HANNOUN Judith', 'judith.hannoun@univ-amu.fr'], ['Isabelle Thiebau', 'isabelle.thiebau@univ-lille2.fr'], ['RAPHAELLE BRANGIER', 'raphaelle.krummeich@univ-rouen.fr'], ['Th\\xc3\\xa9ly Nicolas', 'nicolas.thely@univ-rennes2.fr'], ['H\\xc3\\xa9l\\xc3\\xa8ne de Foucaud', 'hdefouca@u-paris10.fr'], ['marionlame@gmail.com', 'marionlame@gmail.com']]\n"
     ]
    }
   ],
   "source": [
    "liste_noms=read_corpus_and_extract_names(corpus_mails_DH)\n",
    "print len(liste_noms)\n",
    "print liste_noms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html_pages_in_dirs_and_testtextextract(pages_dir):\n",
    "    dict_mails={}\n",
    "    #rootdir=\"./DHFR_sample/\"\n",
    "\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(pages_dir):\n",
    "#ON VA LIRE TOUS LES FICHIERS DU REP ROOTDIR\n",
    "        \n",
    "        for file in files:\n",
    "            dictT={}\n",
    "    #V1     if file.endswith('.html') == True  and file != (\"index.html\") and not re.match(\"mail\\d+.html\",file) : \n",
    "            if re.match(\"msg\\d+.html\",file):\n",
    "                filename=os.path.join(subdir, file)\n",
    "                with open(filename, 'r') as myfile:           \n",
    "                \n",
    "                    data=myfile.read()\n",
    "                    result = re.search('<!--X-Body-of-Message-->(.*)<!--X-Body-of-Message-End-->', data, flags=re.DOTALL)\n",
    "                    ZC= result.group(1)\n",
    "                    ZC=cleanhtml(ZC)\n",
    "                    dictT[\"content\"]=ZC\n",
    "                    dictT[\"geo\"]=topic_analysis_with_manual_detection(ZC)\n",
    "                dict_mails[filename]= dictT\n",
    "            \n",
    "    return dict_mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html_and_testtextextract(filename):\n",
    "                dictT={}\n",
    "                with open(filename, 'r') as myfile:\n",
    "                    data=myfile.read()\n",
    "                    result = re.search('<!--X-Body-of-Message-->(.*)<!--X-Body-of-Message-End-->', data, flags=re.DOTALL)\n",
    "                    ZC= result.group(1)\n",
    "                    ZC=cleanhtml(ZC)\n",
    "                    dictT[\"content\"]=ZC\n",
    "                    dictT[\"geo\"]=topic_analysis_with_manual_detection(ZC)\n",
    "                return dictT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_txt=read_html_pages_in_dirs_and_testtextextract(\"./DHFRsample\")\n",
    "#print corpus_txt\n",
    "#lng= language_detection_with_pyenchant(str(corpus_txt))\n",
    "#print lng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "print enchant.list_languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyse =topic_analysis_with_nltk_gensim(corpus_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Analyse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_analysis_with_nltk_gensim(string_to_read):\n",
    "    string_to_read=unicode(str(string_to_read), 'utf8')\n",
    "    lng = language_detection_with_pyenchant(string_to_read)\n",
    "   \n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    import string\n",
    "    print lng\n",
    "    stop = set(stopwords.words(lng))\n",
    "    exclude = set(string.punctuation)\n",
    "    lemma = WordNetLemmatizer()\n",
    "    def clean(doc):\n",
    "        stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "        \n",
    "        punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "        return normalized\n",
    "\n",
    "    doc_clean = clean(string_to_read).split()\n",
    "    print \"DOC CLEAN\",doc_clean\n",
    "        # Importing Gensim\n",
    "    import gensim\n",
    "    from gensim import corpora\n",
    "\n",
    "        # Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "    dictionary = corpora.Dictionary([doc_clean])\n",
    "\n",
    "        # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = dictionary.doc2bow(doc_clean)\n",
    "    Lda = gensim.models.ldamodel.LdaModel\n",
    "    print Lda\n",
    "\n",
    "        # Running and Trainign LDA model on the document term matrix.\n",
    "    ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=5)\n",
    "    return ldamodel\n",
    "       \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    #return [topic_words,doctopic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
